Attention: The following text is a 'bfiles' bundle, containing multiple delimited files with metadata.
Parse and analyze the content between '<<< BOF <<<' and '>>> EOF >>>' for each '### FILE...' entry.

--- START OF BFILE bf-20251029-112231.txt ---
bfiles bundle generated on: 2025-10-29T11:22:31.597588
Config: hash=sha256, gitignore=yes, followlinks=no
---

### FILE 1: assets/images/logo.svg | checksum=642972db3cdf... | modified=2025-10-21T08:08:03 | op=+ | size=360 | tokens=112 | type=svg+xml ###
<<< BOF <<<
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100">
  <!-- Placeholder logo for provide.foundation -->
  <!-- TODO: Replace with actual provide.io branding -->
  <circle cx="50" cy="50" r="45" fill="#6366f1" />
  <text x="50" y="60" font-family="Arial, sans-serif" font-size="48" font-weight="bold" fill="white" text-anchor="middle">P</text>
</svg>
>>> EOF >>>

### FILE 2: assets/images/officium-dark.svg | checksum=f7bc7d9f22c5... | modified=2025-10-21T08:08:03 | op=+ | size=11766 | tokens=5337 | type=svg+xml ###
<<< BOF <<<
<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">

<svg width="600" height="600" viewBox="0 0 600 600" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" baseProfile="full">
    <g transform="scale(1.000000,-1.000000) translate(0,-600)">
        <path d="M 478.000000 246.000000 C 478.000000 374.130062 374.130062 478.000000 246.000000 478.000000 C 117.869938 478.000000 14.000000 374.130062 14.000000 246.000000 C 14.000000 117.869938 117.869938 14.000000 246.000000 14.000000 C 374.130062 14.000000 478.000000 117.869938 478.000000 246.000000 Z M 491.000000 246.000000 C 491.000000 110.690236 381.309764 1.000000 246.000000 1.000000 C 110.690236 1.000000 1.000000 110.690236 1.000000 246.000000 C 1.000000 381.309764 110.690236 491.000000 246.000000 491.000000 C 381.309764 491.000000 491.000000 381.309764 491.000000 246.000000 Z" transform="scale(1.000000,1.000000) translate(54.000000,54.002480)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 10.192388 125.442388 L 125.442388 10.192388 L 116.250000 1.000000 L 1.000000 116.250000 L 10.192388 125.442388 Z" transform="scale(1.000000,1.000000) translate(236.778806,236.781359)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 1.000000 10.192388 L 117.000000 126.192388 L 126.192388 117.000000 L 10.192388 1.000000 L 1.000000 10.192388 Z" transform="scale(1.000000,1.000000) translate(236.403806,236.403806)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 108.500000 1.000000 L 1.000000 112.504855 L 10.358921 121.527637 L 117.858921 10.022782 L 108.500000 1.000000 Z" transform="scale(1.000000,1.000000) translate(119.820540,360.488609)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 109.250000 1.000000 L 1.000000 111.999750 L 10.306944 121.076137 L 118.556944 10.076387 L 109.250000 1.000000 Z" transform="scale(1.000000,1.000000) translate(360.596528,116.962057)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 119.523997 113.212575 L 10.316279 1.000000 L 1.000000 10.066805 L 110.207719 122.279380 L 119.523997 113.212575 Z" transform="scale(1.000000,1.000000) translate(359.634142,359.758879)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 118.690622 112.632691 L 10.327996 1.000000 L 1.000000 10.054750 L 109.362626 121.687441 L 118.690622 112.632691 Z" transform="scale(1.000000,1.000000) translate(119.836002,116.972875)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 92.250000 1.000000 L 1.000000 92.250000 L 10.192388 101.442388 L 101.442388 10.192388 L 92.250000 1.000000 Z" transform="scale(1.000000,1.000000) translate(187.903806,185.653806)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 101.684844 92.350006 L 10.184844 1.000000 L 1.000000 10.199926 L 92.500000 101.549932 L 101.684844 92.350006 Z" transform="scale(1.000000,1.000000) translate(309.407578,185.800031)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 101.442388 92.250000 L 10.192388 1.000000 L 1.000000 10.192388 L 92.250000 101.442388 L 101.442388 92.250000 Z" transform="scale(1.000000,1.000000) translate(187.937004,311.842965)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 93.000000 1.000000 L 1.000000 93.000000 L 10.192388 102.192388 L 102.192388 10.192388 L 93.000000 1.000000 Z" transform="scale(1.000000,1.000000) translate(308.903806,311.403806)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 142.865421 1.000000 L 1.000000 38.500000 L 6.877822 60.736259 L 148.743243 23.236259 L 142.865421 1.000000 Z" transform="scale(1.000000,1.000000) translate(121.561089,427.886726)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 153.845703 38.500000 L 6.679233 1.000000 L 1.000000 23.287806 L 148.166470 60.787806 L 153.845703 38.500000 Z" transform="scale(1.000000,1.000000) translate(323.493913,427.860952)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 144.656618 1.000000 L 1.000000 38.500000 L 6.809237 60.754275 L 150.465855 23.254275 L 144.656618 1.000000 Z" transform="scale(1.000000,1.000000) translate(326.938764,110.373112)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 60.778977 147.218962 L 23.278977 1.000000 L 1.000000 6.713771 L 38.500000 152.932733 L 60.778977 147.218962 Z" transform="scale(1.000000,1.000000) translate(113.360512,118.643365)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 23.275041 152.531125 L 60.775041 6.729098 L 38.500000 1.000000 L 1.000000 146.802027 L 23.275041 152.531125 Z" transform="scale(1.000000,1.000000) translate(113.362480,327.838280)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 60.775041 146.802027 L 23.275041 1.000000 L 1.000000 6.729098 L 38.500000 152.531125 L 60.775041 146.802027 Z" transform="scale(1.000000,1.000000) translate(424.862480,327.838280)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 23.278977 152.932733 L 60.778977 6.713771 L 38.500000 1.000000 L 1.000000 147.218962 L 23.278977 152.932733 Z" transform="scale(1.000000,1.000000) translate(424.860512,118.643365)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 150.654208 38.500000 L 6.801835 1.000000 L 1.000000 23.256206 L 144.852373 60.756206 L 150.654208 38.500000 Z" transform="scale(1.000000,1.000000) translate(121.599083,110.372147)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 101.000000 51.000000 C 101.000000 78.614237 78.614237 101.000000 51.000000 101.000000 C 23.385763 101.000000 1.000000 78.614237 1.000000 51.000000 C 1.000000 23.385763 23.385763 1.000000 51.000000 1.000000 C 78.614237 1.000000 101.000000 23.385763 101.000000 51.000000 Z" transform="scale(1.000000,1.000000) translate(249.000000,249.002548)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 101.000000 51.000000 C 101.000000 78.614237 78.614237 101.000000 51.000000 101.000000 C 23.385763 101.000000 1.000000 78.614237 1.000000 51.000000 C 1.000000 23.385763 23.385763 1.000000 51.000000 1.000000 C 78.614237 1.000000 101.000000 23.385763 101.000000 51.000000 Z" transform="scale(1.000000,1.000000) translate(249.000000,489.002563)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 101.000000 51.000000 C 101.000000 78.614237 78.614237 101.000000 51.000000 101.000000 C 23.385763 101.000000 1.000000 78.614237 1.000000 51.000000 C 1.000000 23.385763 23.385763 1.000000 51.000000 1.000000 C 78.614237 1.000000 101.000000 23.385763 101.000000 51.000000 Z" transform="scale(1.000000,1.000000) translate(411.000000,413.002563)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 101.000000 51.000000 C 101.000000 78.614237 78.614237 101.000000 51.000000 101.000000 C 23.385763 101.000000 1.000000 78.614237 1.000000 51.000000 C 1.000000 23.385763 23.385763 1.000000 51.000000 1.000000 C 78.614237 1.000000 101.000000 23.385763 101.000000 51.000000 Z" transform="scale(1.000000,1.000000) translate(489.000000,249.002548)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 101.000000 51.000000 C 101.000000 78.614237 78.614237 101.000000 51.000000 101.000000 C 23.385763 101.000000 1.000000 78.614237 1.000000 51.000000 C 1.000000 23.385763 23.385763 1.000000 51.000000 1.000000 C 78.614237 1.000000 101.000000 23.385763 101.000000 51.000000 Z" transform="scale(1.000000,1.000000) translate(411.000000,85.002556)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 101.000000 51.000000 C 101.000000 78.614237 78.614237 101.000000 51.000000 101.000000 C 23.385763 101.000000 1.000000 78.614237 1.000000 51.000000 C 1.000000 23.385763 23.385763 1.000000 51.000000 1.000000 C 78.614237 1.000000 101.000000 23.385763 101.000000 51.000000 Z" transform="scale(1.000000,1.000000) translate(249.000000,9.002553)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 101.000000 51.000000 C 101.000000 78.614237 78.614237 101.000000 51.000000 101.000000 C 23.385763 101.000000 1.000000 78.614237 1.000000 51.000000 C 1.000000 23.385763 23.385763 1.000000 51.000000 1.000000 C 78.614237 1.000000 101.000000 23.385763 101.000000 51.000000 Z" transform="scale(1.000000,1.000000) translate(87.000000,87.002556)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 101.000000 51.000000 C 101.000000 78.614237 78.614237 101.000000 51.000000 101.000000 C 23.385763 101.000000 1.000000 78.614237 1.000000 51.000000 C 1.000000 23.385763 23.385763 1.000000 51.000000 1.000000 C 78.614237 1.000000 101.000000 23.385763 101.000000 51.000000 Z" transform="scale(1.000000,1.000000) translate(9.000000,249.002548)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 101.000000 51.000000 C 101.000000 78.614237 78.614237 101.000000 51.000000 101.000000 C 23.385763 101.000000 1.000000 78.614237 1.000000 51.000000 C 1.000000 23.385763 23.385763 1.000000 51.000000 1.000000 C 78.614237 1.000000 101.000000 23.385763 101.000000 51.000000 Z" transform="scale(1.000000,1.000000) translate(87.000000,413.002563)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 60.370712 38.500000 C 60.370712 50.578861 50.578861 60.370712 38.500000 60.370712 C 26.421139 60.370712 16.629288 50.578861 16.629288 38.500000 C 16.629288 26.421139 26.421139 16.629288 38.500000 16.629288 C 50.578861 16.629288 60.370712 26.421139 60.370712 38.500000 Z M 76.000000 38.500000 C 76.000000 17.789322 59.210678 1.000000 38.500000 1.000000 C 17.789322 1.000000 1.000000 17.789322 1.000000 38.500000 C 1.000000 59.210678 17.789322 76.000000 38.500000 76.000000 C 59.210678 76.000000 76.000000 59.210678 76.000000 38.500000 Z" transform="scale(1.000000,1.000000) translate(261.500000,401.504852)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 60.370712 38.500000 C 60.370712 50.578861 50.578861 60.370712 38.500000 60.370712 C 26.421139 60.370712 16.629288 50.578861 16.629288 38.500000 C 16.629288 26.421139 26.421139 16.629288 38.500000 16.629288 C 50.578861 16.629288 60.370712 26.421139 60.370712 38.500000 Z M 76.000000 38.500000 C 76.000000 17.789322 59.210678 1.000000 38.500000 1.000000 C 17.789322 1.000000 1.000000 17.789322 1.000000 38.500000 C 1.000000 59.210678 17.789322 76.000000 38.500000 76.000000 C 59.210678 76.000000 76.000000 59.210678 76.000000 38.500000 Z" transform="scale(1.000000,1.000000) translate(398.500000,261.500000)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 60.370712 38.500000 C 60.370712 50.578861 50.578861 60.370712 38.500000 60.370712 C 26.421139 60.370712 16.629288 50.578861 16.629288 38.500000 C 16.629288 26.421139 26.421139 16.629288 38.500000 16.629288 C 50.578861 16.629288 60.370712 26.421139 60.370712 38.500000 Z M 76.000000 38.500000 C 76.000000 17.789322 59.210678 1.000000 38.500000 1.000000 C 17.789322 1.000000 1.000000 17.789322 1.000000 38.500000 C 1.000000 59.210678 17.789322 76.000000 38.500000 76.000000 C 59.210678 76.000000 76.000000 59.210678 76.000000 38.500000 Z" transform="scale(1.000000,1.000000) translate(261.500000,121.500252)" fill="#0d0d0d" opacity="1.000000"></path>
        <path d="M 60.370712 38.500000 C 60.370712 50.578861 50.578861 60.370712 38.500000 60.370712 C 26.421139 60.370712 16.629288 50.578861 16.629288 38.500000 C 16.629288 26.421139 26.421139 16.629288 38.500000 16.629288 C 50.578861 16.629288 60.370712 26.421139 60.370712 38.500000 Z M 76.000000 38.500000 C 76.000000 17.789322 59.210678 1.000000 38.500000 1.000000 C 17.789322 1.000000 1.000000 17.789322 1.000000 38.500000 C 1.000000 59.210678 17.789322 76.000000 38.500000 76.000000 C 59.210678 76.000000 76.000000 59.210678 76.000000 38.500000 Z" transform="scale(1.000000,1.000000) translate(124.500000,261.500000)" fill="#0d0d0d" opacity="1.000000"></path>
    </g>
</svg>
>>> EOF >>>

### FILE 3: assets/images/officium-light.svg | checksum=8361762ab514... | modified=2025-10-21T08:08:03 | op=+ | size=14601 | tokens=6181 | type=svg+xml ###
<<< BOF <<<
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<svg
   width="600"
   height="600"
   viewBox="0 0 600 600"
   version="1.1"
   id="svg32"
   sodipodi:docname="logo-light.svg"
   inkscape:version="1.3.2 (091e20e, 2023-11-25)"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <defs
     id="defs32" />
  <sodipodi:namedview
     id="namedview32"
     pagecolor="#ffffff"
     bordercolor="#000000"
     borderopacity="0.25"
     inkscape:showpageshadow="2"
     inkscape:pageopacity="0.0"
     inkscape:pagecheckerboard="0"
     inkscape:deskcolor="#d1d1d1"
     inkscape:zoom="1.3183333"
     inkscape:cx="300"
     inkscape:cy="300"
     inkscape:window-width="1392"
     inkscape:window-height="1212"
     inkscape:window-x="1804"
     inkscape:window-y="25"
     inkscape:window-maximized="0"
     inkscape:current-layer="g32" />
  <g
     transform="scale(1.000000,-1.000000) translate(0,-600)"
     id="g32"
     style="fill:#f2f2f2">
    <path
       d="M 478.000000 246.000000 C 478.000000 374.130062 374.130062 478.000000 246.000000 478.000000 C 117.869938 478.000000 14.000000 374.130062 14.000000 246.000000 C 14.000000 117.869938 117.869938 14.000000 246.000000 14.000000 C 374.130062 14.000000 478.000000 117.869938 478.000000 246.000000 Z M 491.000000 246.000000 C 491.000000 110.690236 381.309764 1.000000 246.000000 1.000000 C 110.690236 1.000000 1.000000 110.690236 1.000000 246.000000 C 1.000000 381.309764 110.690236 491.000000 246.000000 491.000000 C 381.309764 491.000000 491.000000 381.309764 491.000000 246.000000 Z"
       transform="scale(1.000000,1.000000) translate(54.000000,54.002480)"
       fill="#000000"
       opacity="1.000000"
       id="path1"
       style="fill:#f2f2f2" />
    <path
       d="M 10.192388 125.442388 L 125.442388 10.192388 L 116.250000 1.000000 L 1.000000 116.250000 L 10.192388 125.442388 Z"
       transform="scale(1.000000,1.000000) translate(236.778806,236.781359)"
       fill="#000000"
       opacity="1.000000"
       id="path2"
       style="fill:#f2f2f2" />
    <path
       d="M 1.000000 10.192388 L 117.000000 126.192388 L 126.192388 117.000000 L 10.192388 1.000000 L 1.000000 10.192388 Z"
       transform="scale(1.000000,1.000000) translate(236.403806,236.403806)"
       fill="#000000"
       opacity="1.000000"
       id="path3"
       style="fill:#f2f2f2" />
    <path
       d="M 108.500000 1.000000 L 1.000000 112.504855 L 10.358921 121.527637 L 117.858921 10.022782 L 108.500000 1.000000 Z"
       transform="scale(1.000000,1.000000) translate(119.820540,360.488609)"
       fill="#000000"
       opacity="1.000000"
       id="path4"
       style="fill:#f2f2f2" />
    <path
       d="M 109.250000 1.000000 L 1.000000 111.999750 L 10.306944 121.076137 L 118.556944 10.076387 L 109.250000 1.000000 Z"
       transform="scale(1.000000,1.000000) translate(360.596528,116.962057)"
       fill="#000000"
       opacity="1.000000"
       id="path5"
       style="fill:#f2f2f2" />
    <path
       d="M 119.523997 113.212575 L 10.316279 1.000000 L 1.000000 10.066805 L 110.207719 122.279380 L 119.523997 113.212575 Z"
       transform="scale(1.000000,1.000000) translate(359.634142,359.758879)"
       fill="#000000"
       opacity="1.000000"
       id="path6"
       style="fill:#f2f2f2" />
    <path
       d="M 118.690622 112.632691 L 10.327996 1.000000 L 1.000000 10.054750 L 109.362626 121.687441 L 118.690622 112.632691 Z"
       transform="scale(1.000000,1.000000) translate(119.836002,116.972875)"
       fill="#000000"
       opacity="1.000000"
       id="path7"
       style="fill:#f2f2f2" />
    <path
       d="M 92.250000 1.000000 L 1.000000 92.250000 L 10.192388 101.442388 L 101.442388 10.192388 L 92.250000 1.000000 Z"
       transform="scale(1.000000,1.000000) translate(187.903806,185.653806)"
       fill="#000000"
       opacity="1.000000"
       id="path8"
       style="fill:#f2f2f2" />
    <path
       d="M 101.684844 92.350006 L 10.184844 1.000000 L 1.000000 10.199926 L 92.500000 101.549932 L 101.684844 92.350006 Z"
       transform="scale(1.000000,1.000000) translate(309.407578,185.800031)"
       fill="#000000"
       opacity="1.000000"
       id="path9"
       style="fill:#f2f2f2" />
    <path
       d="M 101.442388 92.250000 L 10.192388 1.000000 L 1.000000 10.192388 L 92.250000 101.442388 L 101.442388 92.250000 Z"
       transform="scale(1.000000,1.000000) translate(187.937004,311.842965)"
       fill="#000000"
       opacity="1.000000"
       id="path10"
       style="fill:#f2f2f2" />
    <path
       d="M 93.000000 1.000000 L 1.000000 93.000000 L 10.192388 102.192388 L 102.192388 10.192388 L 93.000000 1.000000 Z"
       transform="scale(1.000000,1.000000) translate(308.903806,311.403806)"
       fill="#000000"
       opacity="1.000000"
       id="path11"
       style="fill:#f2f2f2" />
    <path
       d="M 142.865421 1.000000 L 1.000000 38.500000 L 6.877822 60.736259 L 148.743243 23.236259 L 142.865421 1.000000 Z"
       transform="scale(1.000000,1.000000) translate(121.561089,427.886726)"
       fill="#000000"
       opacity="1.000000"
       id="path12"
       style="fill:#f2f2f2" />
    <path
       d="M 153.845703 38.500000 L 6.679233 1.000000 L 1.000000 23.287806 L 148.166470 60.787806 L 153.845703 38.500000 Z"
       transform="scale(1.000000,1.000000) translate(323.493913,427.860952)"
       fill="#000000"
       opacity="1.000000"
       id="path13"
       style="fill:#f2f2f2" />
    <path
       d="M 144.656618 1.000000 L 1.000000 38.500000 L 6.809237 60.754275 L 150.465855 23.254275 L 144.656618 1.000000 Z"
       transform="scale(1.000000,1.000000) translate(326.938764,110.373112)"
       fill="#000000"
       opacity="1.000000"
       id="path14"
       style="fill:#f2f2f2" />
    <path
       d="M 60.778977 147.218962 L 23.278977 1.000000 L 1.000000 6.713771 L 38.500000 152.932733 L 60.778977 147.218962 Z"
       transform="scale(1.000000,1.000000) translate(113.360512,118.643365)"
       fill="#000000"
       opacity="1.000000"
       id="path15"
       style="fill:#f2f2f2" />
    <path
       d="M 23.275041 152.531125 L 60.775041 6.729098 L 38.500000 1.000000 L 1.000000 146.802027 L 23.275041 152.531125 Z"
       transform="scale(1.000000,1.000000) translate(113.362480,327.838280)"
       fill="#000000"
       opacity="1.000000"
       id="path16"
       style="fill:#f2f2f2" />
    <path
       d="M 60.775041 146.802027 L 23.275041 1.000000 L 1.000000 6.729098 L 38.500000 152.531125 L 60.775041 146.802027 Z"
       transform="scale(1.000000,1.000000) translate(424.862480,327.838280)"
       fill="#000000"
       opacity="1.000000"
       id="path17"
       style="fill:#f2f2f2" />
    <path
       d="M 23.278977 152.932733 L 60.778977 6.713771 L 38.500000 1.000000 L 1.000000 147.218962 L 23.278977 152.932733 Z"
       transform="scale(1.000000,1.000000) translate(424.860512,118.643365)"
       fill="#000000"
       opacity="1.000000"
       id="path18"
       style="fill:#f2f2f2" />
    <path
       d="M 150.654208 38.500000 L 6.801835 1.000000 L 1.000000 23.256206 L 144.852373 60.756206 L 150.654208 38.500000 Z"
       transform="scale(1.000000,1.000000) translate(121.599083,110.372147)"
       fill="#000000"
       opacity="1.000000"
       id="path19"
       style="fill:#f2f2f2" />
    <path
       d="M 101.000000 51.000000 C 101.000000 78.614237 78.614237 101.000000 51.000000 101.000000 C 23.385763 101.000000 1.000000 78.614237 1.000000 51.000000 C 1.000000 23.385763 23.385763 1.000000 51.000000 1.000000 C 78.614237 1.000000 101.000000 23.385763 101.000000 51.000000 Z"
       transform="scale(1.000000,1.000000) translate(249.000000,249.002548)"
       fill="#000000"
       opacity="1.000000"
       id="path20"
       style="fill:#f2f2f2" />
    <path
       d="M 101.000000 51.000000 C 101.000000 78.614237 78.614237 101.000000 51.000000 101.000000 C 23.385763 101.000000 1.000000 78.614237 1.000000 51.000000 C 1.000000 23.385763 23.385763 1.000000 51.000000 1.000000 C 78.614237 1.000000 101.000000 23.385763 101.000000 51.000000 Z"
       transform="scale(1.000000,1.000000) translate(249.000000,489.002563)"
       fill="#000000"
       opacity="1.000000"
       id="path21"
       style="fill:#f2f2f2" />
    <path
       d="M 101.000000 51.000000 C 101.000000 78.614237 78.614237 101.000000 51.000000 101.000000 C 23.385763 101.000000 1.000000 78.614237 1.000000 51.000000 C 1.000000 23.385763 23.385763 1.000000 51.000000 1.000000 C 78.614237 1.000000 101.000000 23.385763 101.000000 51.000000 Z"
       transform="scale(1.000000,1.000000) translate(411.000000,413.002563)"
       fill="#000000"
       opacity="1.000000"
       id="path22"
       style="fill:#f2f2f2" />
    <path
       d="M 101.000000 51.000000 C 101.000000 78.614237 78.614237 101.000000 51.000000 101.000000 C 23.385763 101.000000 1.000000 78.614237 1.000000 51.000000 C 1.000000 23.385763 23.385763 1.000000 51.000000 1.000000 C 78.614237 1.000000 101.000000 23.385763 101.000000 51.000000 Z"
       transform="scale(1.000000,1.000000) translate(489.000000,249.002548)"
       fill="#000000"
       opacity="1.000000"
       id="path23"
       style="fill:#f2f2f2" />
    <path
       d="M 101.000000 51.000000 C 101.000000 78.614237 78.614237 101.000000 51.000000 101.000000 C 23.385763 101.000000 1.000000 78.614237 1.000000 51.000000 C 1.000000 23.385763 23.385763 1.000000 51.000000 1.000000 C 78.614237 1.000000 101.000000 23.385763 101.000000 51.000000 Z"
       transform="scale(1.000000,1.000000) translate(411.000000,85.002556)"
       fill="#000000"
       opacity="1.000000"
       id="path24"
       style="fill:#f2f2f2" />
    <path
       d="M 101.000000 51.000000 C 101.000000 78.614237 78.614237 101.000000 51.000000 101.000000 C 23.385763 101.000000 1.000000 78.614237 1.000000 51.000000 C 1.000000 23.385763 23.385763 1.000000 51.000000 1.000000 C 78.614237 1.000000 101.000000 23.385763 101.000000 51.000000 Z"
       transform="scale(1.000000,1.000000) translate(249.000000,9.002553)"
       fill="#000000"
       opacity="1.000000"
       id="path25"
       style="fill:#f2f2f2" />
    <path
       d="M 101.000000 51.000000 C 101.000000 78.614237 78.614237 101.000000 51.000000 101.000000 C 23.385763 101.000000 1.000000 78.614237 1.000000 51.000000 C 1.000000 23.385763 23.385763 1.000000 51.000000 1.000000 C 78.614237 1.000000 101.000000 23.385763 101.000000 51.000000 Z"
       transform="scale(1.000000,1.000000) translate(87.000000,87.002556)"
       fill="#000000"
       opacity="1.000000"
       id="path26"
       style="fill:#f2f2f2" />
    <path
       d="M 101.000000 51.000000 C 101.000000 78.614237 78.614237 101.000000 51.000000 101.000000 C 23.385763 101.000000 1.000000 78.614237 1.000000 51.000000 C 1.000000 23.385763 23.385763 1.000000 51.000000 1.000000 C 78.614237 1.000000 101.000000 23.385763 101.000000 51.000000 Z"
       transform="scale(1.000000,1.000000) translate(9.000000,249.002548)"
       fill="#000000"
       opacity="1.000000"
       id="path27"
       style="fill:#f2f2f2" />
    <path
       d="M 101.000000 51.000000 C 101.000000 78.614237 78.614237 101.000000 51.000000 101.000000 C 23.385763 101.000000 1.000000 78.614237 1.000000 51.000000 C 1.000000 23.385763 23.385763 1.000000 51.000000 1.000000 C 78.614237 1.000000 101.000000 23.385763 101.000000 51.000000 Z"
       transform="scale(1.000000,1.000000) translate(87.000000,413.002563)"
       fill="#000000"
       opacity="1.000000"
       id="path28"
       style="fill:#f2f2f2" />
    <path
       d="M 60.370712 38.500000 C 60.370712 50.578861 50.578861 60.370712 38.500000 60.370712 C 26.421139 60.370712 16.629288 50.578861 16.629288 38.500000 C 16.629288 26.421139 26.421139 16.629288 38.500000 16.629288 C 50.578861 16.629288 60.370712 26.421139 60.370712 38.500000 Z M 76.000000 38.500000 C 76.000000 17.789322 59.210678 1.000000 38.500000 1.000000 C 17.789322 1.000000 1.000000 17.789322 1.000000 38.500000 C 1.000000 59.210678 17.789322 76.000000 38.500000 76.000000 C 59.210678 76.000000 76.000000 59.210678 76.000000 38.500000 Z"
       transform="scale(1.000000,1.000000) translate(261.500000,401.504852)"
       fill="#000000"
       opacity="1.000000"
       id="path29"
       style="fill:#f2f2f2" />
    <path
       d="M 60.370712 38.500000 C 60.370712 50.578861 50.578861 60.370712 38.500000 60.370712 C 26.421139 60.370712 16.629288 50.578861 16.629288 38.500000 C 16.629288 26.421139 26.421139 16.629288 38.500000 16.629288 C 50.578861 16.629288 60.370712 26.421139 60.370712 38.500000 Z M 76.000000 38.500000 C 76.000000 17.789322 59.210678 1.000000 38.500000 1.000000 C 17.789322 1.000000 1.000000 17.789322 1.000000 38.500000 C 1.000000 59.210678 17.789322 76.000000 38.500000 76.000000 C 59.210678 76.000000 76.000000 59.210678 76.000000 38.500000 Z"
       transform="scale(1.000000,1.000000) translate(398.500000,261.500000)"
       fill="#000000"
       opacity="1.000000"
       id="path30"
       style="fill:#f2f2f2" />
    <path
       d="M 60.370712 38.500000 C 60.370712 50.578861 50.578861 60.370712 38.500000 60.370712 C 26.421139 60.370712 16.629288 50.578861 16.629288 38.500000 C 16.629288 26.421139 26.421139 16.629288 38.500000 16.629288 C 50.578861 16.629288 60.370712 26.421139 60.370712 38.500000 Z M 76.000000 38.500000 C 76.000000 17.789322 59.210678 1.000000 38.500000 1.000000 C 17.789322 1.000000 1.000000 17.789322 1.000000 38.500000 C 1.000000 59.210678 17.789322 76.000000 38.500000 76.000000 C 59.210678 76.000000 76.000000 59.210678 76.000000 38.500000 Z"
       transform="scale(1.000000,1.000000) translate(261.500000,121.500252)"
       fill="#000000"
       opacity="1.000000"
       id="path31"
       style="fill:#f2f2f2" />
    <path
       d="M 60.370712 38.500000 C 60.370712 50.578861 50.578861 60.370712 38.500000 60.370712 C 26.421139 60.370712 16.629288 50.578861 16.629288 38.500000 C 16.629288 26.421139 26.421139 16.629288 38.500000 16.629288 C 50.578861 16.629288 60.370712 26.421139 60.370712 38.500000 Z M 76.000000 38.500000 C 76.000000 17.789322 59.210678 1.000000 38.500000 1.000000 C 17.789322 1.000000 1.000000 17.789322 1.000000 38.500000 C 1.000000 59.210678 17.789322 76.000000 38.500000 76.000000 C 59.210678 76.000000 76.000000 59.210678 76.000000 38.500000 Z"
       transform="scale(1.000000,1.000000) translate(124.500000,261.500000)"
       fill="#000000"
       opacity="1.000000"
       id="path32"
       style="fill:#f2f2f2" />
  </g>
</svg>
>>> EOF >>>

### FILE 4: explanation/architecture.md | checksum=a320e10259d8... | modified=2025-10-24T20:28:13 | op=+ | size=22987 | tokens=5044 | type=markdown ###
<<< BOF <<<
# Explanation: Architecture

`provide.foundation` is designed as a layered, framework-agnostic library that provides common infrastructure for building high-quality Python applications.

```mermaid
graph TD
    subgraph "Your Application"
        A[Business Logic / Web Framework]
    end

    subgraph "provide.foundation"
        B[Hub & Registry]
        C[Logging & Telemetry]
        D[Configuration]
        E[CLI Framework]
        F[Resilience & Utilities]
        G[Event Sets & Observability]
        H[Testing Support]
    end

    subgraph "Core Dependencies"
        I[structlog]
        J[attrs]
        K[click]
    end

    A --> B & C & D & E & F & G & H
    C --> I
    D --> J
    E --> K
    G --> C
    H --> B
```

## Architectural Overview

provide.foundation is structured around several key design principles:

1. **Foundation Layer, Not Framework**: Provides building blocks rather than dictating application structure
2. **Lazy Initialization**: Minimal overhead until features are actively used
3. **Global Singletons for Ergonomics**: Balanced with testing utilities for clean test isolation
4. **Type Safety**: Comprehensive type hints with runtime validation where needed
5. **Production-First Design**: Thread-safe, performant, and observable by default

## Core Components

### Hub & Registry System

The **Hub** (`provide.foundation.hub`) is the architectural cornerstone, providing:

#### Component Registry
- **Centralized component management** with lifecycle control
- **Dependency injection** for loosely-coupled architecture
- **Thread-safe registration** using `threading.RLock`
- **Component discovery** for plugins and extensions

```python
from provide.foundation import get_hub

hub = get_hub()
hub.register_component("database", DatabaseConnection())
db = hub.get_component("database")
```

#### Command Registry
- **Declarative CLI command registration** via `@register_command`
- **Automatic help generation** from docstrings and type hints
- **Nested command hierarchies** with dot notation
- **Command metadata** (aliases, categories, tags)

```python
from provide.foundation.hub import register_command

@register_command("db.migrate", aliases=["migrate"], category="database")
def migrate_database():
    """Run database migrations."""
    pass
```

#### Design Decisions
- **Threading Model**: Uses `threading.RLock` rather than `asyncio.Lock` for broader compatibility
- **Performance Impact**: Negligible for typical use cases (CLI apps, initialization-time registration)
- **Trade-off**: Not optimized for high-throughput async services with runtime registration in hot paths

### Logging & Telemetry System

The **Logger** (`provide.foundation.logger`) provides structured logging built on `structlog`:

#### Key Features
- **Lazy Initialization**: Logger auto-initializes on first use to avoid import-time side effects
- **Event-Driven Enrichment**: Structured logging with Domain-Action-Status patterns
- **Emoji-Enhanced Output**: Visual parsing for human-readable logs (configurable)
- **Performance**: >14,000 messages/second with emoji processing enabled
- **Thread-Safe**: All logging operations are thread-safe and async-compatible

#### Processor Pipeline
```mermaid
graph LR
    A[Log Event] --> B[Context Processors]
    B --> C[Enrichment Processors]
    C --> D[Filtering Processors]
    D --> E[Formatting Processors]
    E --> F[Output Stream]
```

Processors include:
- **Context injection** (timestamp, caller info, thread ID)
- **Event enrichment** (emoji mapping, domain/action/status extraction)
- **Filtering** (log level, module-level configuration)
- **Formatting** (console vs JSON, color support)
- **Output** (stdout, stderr, file, custom streams)

#### Configuration
```python
from provide.foundation import get_hub, LoggingConfig, TelemetryConfig

# Simple configuration
config = TelemetryConfig(
    logging=LoggingConfig(
        default_level="INFO",
        logger_name_emoji_prefix_enabled=True,
        das_emoji_prefix_enabled=True,
        console_formatter="key_value"
    )
)

hub = get_hub()
hub.initialize_foundation(config)

# Advanced configuration
advanced_config = TelemetryConfig(
    logging=LoggingConfig(
        default_level="DEBUG",
        logger_name_emoji_prefix_enabled=True,
        das_emoji_prefix_enabled=True,
        console_formatter="key_value",
        module_levels={"urllib3": "WARNING"}
    )
)
hub.initialize_foundation(advanced_config)
```

### Configuration System

The **Config** (`provide.foundation.config`) module provides:

#### BaseConfig Pattern
```python
from provide.foundation.config import BaseConfig, env_field
from attrs import define

@define
class DatabaseConfig(BaseConfig):
    host: str = env_field(env_var="DB_HOST", default="localhost")
    port: int = env_field(env_var="DB_PORT", default=5432)
    # Supports file:// prefix for reading secrets from files
    password: str = env_field(env_var="DB_PASSWORD")

config = DatabaseConfig.from_env()
```

#### Features
- **Type validation** through `attrs`
- **Environment variable parsing** with automatic type coercion
- **Secret management** with `file://` prefix support
- **Immutable by default** with frozen dataclasses
- **Clear precedence**: Environment variables > file values > defaults

#### Environment Variable APIs

Foundation provides two complementary APIs:

**1. Direct Access** (`utils.environment`):
```python
from provide.foundation.utils.environment import get_bool, get_int, get_str

debug = get_bool("DEBUG", default=False)
port = get_int("PORT", default=8080)
```

**2. Structured Config** (`config.env`):
```python
from provide.foundation.config import BaseConfig, env_field

@define
class AppConfig(BaseConfig):
    debug: bool = env_field(env_var="DEBUG", default=False)
    port: int = env_field(env_var="PORT", default=8080)
```

### State Management

The **State** (`provide.foundation.state`) module provides:

- **ImmutableState**: Thread-safe immutable state containers
- **StateMachine**: Finite state machine implementation
- **StateManager**: Centralized state management
- **ConfigManager**: Configuration versioning and updates
- **VersionedConfig**: Configuration with change tracking

Useful for:
- Application lifecycle state tracking
- Configuration hot-reloading
- Audit trails for state changes
- State machines for complex workflows

### CLI Framework

The **CLI** (`provide.foundation.cli`) module provides a declarative framework built on `click`:

#### Features
- **Automatic command discovery** via `@register_command`
- **Type-safe arguments** with automatic validation
- **Rich output utilities** (`pout()`, `perr()` for user-facing output)
- **Interactive prompts** with validation
- **Context management** for CLI runtime state

#### Design Pattern
```python
from provide.foundation.hub import register_command
from provide.foundation.cli import pout, perr
from provide.foundation import logger

@register_command("deploy")
def deploy_app(environment: str, force: bool = False):
    """Deploy application to specified environment."""
    # User-facing output
    pout(f"Deploying to {environment}...")

    # Internal logging
    logger.info("deployment_started", env=environment, force=force)

    try:
        # Deployment logic
        pass
    except Exception as e:
        logger.error("deployment_failed", error=str(e))
        perr(f"âŒ Deployment failed: {e}")
        raise
```

### Resilience Patterns

The **Resilience** (`provide.foundation.resilience`) module provides:

#### Retry Pattern
```python
from provide.foundation.resilience import retry
from provide.foundation.errors import NetworkError

@retry(max_attempts=3, backoff=2.0, exceptions=(NetworkError,))
def fetch_data():
    """Fetch data with automatic retry on network errors."""
    pass
```

Features:
- **Exponential backoff** with configurable multiplier
- **Maximum attempts** and timeout support
- **Exception filtering** (only retry specific exceptions)
- **Jitter** for distributed system resilience

#### Circuit Breaker
```python
from provide.foundation.resilience import circuit_breaker

@circuit_breaker(failure_threshold=5, timeout=60.0)
def call_external_api():
    """Call external API with circuit breaker protection."""
    pass
```

States: **Closed â†’ Open â†’ Half-Open â†’ Closed**

### Data & Serialization

#### File Operations
- **Atomic writes** to prevent partial writes
- **Change detection** for file watching
- **Format support** (JSON, YAML, TOML, text)
- **Safety guarantees** (temp files, rename operations)

#### Archive Operations
- **TAR, ZIP, GZIP, BZIP2** support
- **Streaming** for large files
- **Compression** with zstandard (optional)

#### Serialization
- **Type-safe JSON** serialization with `provide_dumps`/`provide_loads`
- **Custom encoders** for complex types
- **Schema validation** support

### Concurrency Utilities

The **Concurrency** (`provide.foundation.concurrency`) module provides:

- **AsyncLockManager**: Async-safe lock management
- **LockManager**: Thread-safe lock management
- **Task coordination** utilities
- **Async context managers**

### Utilities Suite

Foundation includes comprehensive utilities:

#### Time Utilities
```python
from provide.foundation.time import provide_now, provide_sleep

now = provide_now()  # Timezone-aware current time
await provide_sleep(1.0)  # Async-compatible sleep
```

#### Formatting Utilities
```python
from provide.foundation.formatting import format_table, format_bytes, to_snake_case

# Table formatting
print(format_table(data, headers=["Name", "Value"]))

# Byte size formatting
print(format_bytes(1024 * 1024))  # "1.0 MB"

# Case conversion
print(to_snake_case("CamelCase"))  # "camel_case"
```

#### Platform Detection
```python
from provide.foundation.platform import get_os_info, is_macos

if is_macos():
    # macOS-specific logic
    pass
```

## Initialization & Lifecycle

### Initialization Patterns

#### 1. Zero Configuration (Recommended for Simple Cases)
```python
from provide.foundation import logger

# Logger auto-initializes on first use
logger.info("app_started")
```

#### 2. Explicit Initialization (Recommended for Production)
```python
from provide.foundation import get_hub, LoggingConfig, TelemetryConfig

config = TelemetryConfig(
    service_name="my-service",
    logging=LoggingConfig(
        default_level="INFO",
        logger_name_emoji_prefix_enabled=True,
        das_emoji_prefix_enabled=True
    )
)

hub = get_hub()
hub.initialize_foundation(config)
```

#### 3. Advanced Configuration
```python
from provide.foundation import get_hub, LoggingConfig, TelemetryConfig

logging_config = LoggingConfig(
    default_level="DEBUG",
    logger_name_emoji_prefix_enabled=True,
    das_emoji_prefix_enabled=True,
    console_formatter="key_value",
    module_levels={
        "urllib3": "WARNING",
        "asyncio": "INFO"
    }
)

telemetry_config = TelemetryConfig(
    service_name="my-service",
    logging=logging_config
)

hub = get_hub()
hub.initialize_foundation(telemetry_config)
```

### Lifecycle Management

```mermaid
graph LR
    A[Import] --> B[Lazy Init]
    B --> C[Component Registration]
    C --> D[Runtime]
    D --> E[Shutdown]
```

1. **Import Time**: Minimal overhead, no side effects
2. **Lazy Initialization**: Components initialize on first use
3. **Component Registration**: Register custom components, commands
4. **Runtime**: Normal application execution
5. **Shutdown**: Graceful cleanup of resources

### Graceful Shutdown

Foundation provides `shutdown_foundation()` for clean resource cleanup:

```python
from provide.foundation import shutdown_foundation

# At application shutdown
shutdown_foundation()
```

This function:
- Flushes any pending log messages
- Closes file handlers
- Releases resources
- Resets internal state (useful for testing)

**When to use:**
- Before process termination in CLI applications
- In web framework shutdown hooks (FastAPI `@app.on_event("shutdown")`)
- Between test runs (handled automatically by `provide-testkit`)

### Shutdown Examples for Web Frameworks

#### FastAPI
```python
from fastapi import FastAPI
from provide.foundation import shutdown_foundation

app = FastAPI()

@app.on_event("shutdown")
async def shutdown():
    """Gracefully shutdown Foundation on app termination."""
    shutdown_foundation()
```

#### Flask
```python
from flask import Flask
from provide.foundation import shutdown_foundation
import atexit

app = Flask(__name__)

# Register shutdown hook
atexit.register(shutdown_foundation)

# Or use Flask's teardown handler
@app.teardown_appcontext
def teardown(exception=None):
    """Shutdown Foundation on app context teardown."""
    if app.debug:
        # Only shutdown in debug mode for hot reload
        shutdown_foundation()
```

#### Django
```python
# In your Django app's apps.py

from django.apps import AppConfig
from provide.foundation import shutdown_foundation

class MyAppConfig(AppConfig):
    name = "myapp"

    def ready(self):
        """Initialize Foundation when Django starts."""
        from provide.foundation import get_hub
        hub = get_hub()
        hub.initialize_foundation()

    def __del__(self):
        """Shutdown Foundation when Django stops."""
        shutdown_foundation()
```

#### Signal Handlers (for CLI tools)
```python
import signal
import sys
from provide.foundation import shutdown_foundation

def signal_handler(sig, frame):
    """Handle SIGINT and SIGTERM gracefully."""
    print("\nShutting down gracefully...")
    shutdown_foundation()
    sys.exit(0)

# Register signal handlers
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# Your application code
if __name__ == "__main__":
    main()
```

## Extension Points

### Custom Processors

Add custom log processors:

```python
from provide.foundation import LoggingConfig

def custom_processor(logger, method_name, event_dict):
    """Add custom field to all log events."""
    event_dict["environment"] = "production"
    return event_dict

config = LoggingConfig(
    custom_processors=[custom_processor]
)
```

### Custom Components

Register custom components:

```python
from provide.foundation import get_hub
from provide.foundation.hub import injectable

@injectable
class CustomService:
    def __init__(self):
        self.state = "initialized"

hub = get_hub()

# Register instance by type
service_instance = CustomService()
hub.register(CustomService, service_instance)

# Resolve from hub
service = hub.resolve(CustomService)
```

### Custom Commands

Register CLI commands from plugins:

```python
from provide.foundation.hub import register_command

@register_command("plugin.action")
def plugin_action():
    """Action from plugin."""
    pass
```

### Event Sets and Emoji Mapping

Foundation provides an **Event Set** system for domain-specific emoji mapping and log enrichment. Event sets automatically add visual markers and metadata to logs based on event fields.

#### Built-in Event Sets

Foundation includes pre-configured event sets for common domains:

- **`http`** - HTTP request/response operations
  - Maps HTTP methods (GETâ†’ðŸ“¥, POSTâ†’ðŸ“¤, DELETEâ†’ðŸ—‘ï¸)
  - Maps status codes (2xxâ†’âœ…, 4xxâ†’âš ï¸, 5xxâ†’ðŸ”¥)
  - Adds metadata for success/error classification

- **`database`** - Database operations
  - Maps operations (SELECTâ†’ðŸ“–, INSERTâ†’âž•, UPDATEâ†’âœï¸, DELETEâ†’ðŸ—‘ï¸)
  - Tracks query performance
  - Connection pool events

- **`llm`** - Large Language Model operations
  - Maps LLM providers (OpenAIâ†’ðŸ¤–, Anthropicâ†’ðŸ§ )
  - Token usage tracking
  - Model performance metrics

- **`task_queue`** - Background job processing
  - Task lifecycle events (queuedâ†’ðŸ“¥, runningâ†’âš™ï¸, completedâ†’âœ…)
  - Worker status tracking
  - Retry and failure handling

- **`das`** - Domain-Action-Status pattern
  - Generic domain/action/status enrichment
  - Fallback for custom event patterns

#### Using Event Sets

Event sets are automatically discovered and registered:

```python
from provide.foundation import logger

# HTTP event set automatically enriches HTTP-related logs
logger.info(
    "api_request",
    http_method="post",           # Adds ðŸ“¤ emoji
    http_status_class="2xx",      # Adds âœ… emoji
    endpoint="/api/users"
)
# Output: ðŸ“¤ âœ… api_request | http_method=post | http_status_class=2xx | endpoint=/api/users

# Database event set
logger.info(
    "query_executed",
    db_operation="select",        # Adds ðŸ“– emoji
    table="users",
    duration_ms=45
)
# Output: ðŸ“– query_executed | db_operation=select | table=users | duration_ms=45
```

#### Creating Custom Event Sets

Define custom event sets for your domain:

```python
from provide.foundation.eventsets.types import EventMapping, EventSet

# Define your custom event set
PAYMENT_EVENT_SET = EventSet(
    name="payment",
    description="Payment processing events",
    mappings=[
        EventMapping(
            name="payment_method",
            visual_markers={
                "credit_card": "ðŸ’³",
                "paypal": "ðŸ’°",
                "bank_transfer": "ðŸ¦",
                "cryptocurrency": "â‚¿",
                "default": "ðŸ’µ"
            },
            default_key="default"
        ),
        EventMapping(
            name="payment_status",
            visual_markers={
                "pending": "â³",
                "processing": "âš™ï¸",
                "completed": "âœ…",
                "failed": "âŒ",
                "refunded": "â†©ï¸"
            },
            metadata_fields={
                "completed": {"payment.success": True},
                "failed": {"payment.error": True}
            }
        )
    ]
)

# Register event set
from provide.foundation.eventsets import get_event_set_registry

registry = get_event_set_registry()
registry.register("payment", PAYMENT_EVENT_SET)

# Use in logging
logger.info(
    "payment_processed",
    payment_method="credit_card",   # Adds ðŸ’³
    payment_status="completed",     # Adds âœ… and metadata
    amount=99.99,
    currency="USD"
)
```

#### When to Use Event Sets vs. Manual Emojis

**Use Event Sets when:**
- You have a domain with consistent event patterns
- You want automatic enrichment across many log statements
- You need metadata injection based on field values
- You're building reusable logging patterns

**Use Manual Emojis when:**
- You have one-off log statements
- The emoji doesn't fit a pattern
- You want explicit control

```python
# Manual emoji (simple, one-off)
logger.info("startup_complete", emoji="ðŸš€")

# Event set (automatic, pattern-based)
logger.info("http_request", http_method="get")  # Auto-adds ðŸ“¥
```

## Threading Model & Concurrency

### Thread Safety Guarantees

- **Hub/Registry**: Thread-safe using `threading.RLock`
- **Logger**: Thread-safe through structlog's design
- **State Management**: Immutable state for thread safety
- **Component Registration**: Safe from multiple threads

### Async Support

- **Async-compatible logging**: No blocking I/O in hot paths
- **Async utilities**: `provide_sleep`, async lock managers
- **Async file operations**: via `aiofiles` integration

### Performance Considerations

- **Logging**: >14,000 msg/sec with full processing
- **Registry lookups**: O(1) hash-based lookups
- **Lock contention**: Minimal for typical CLI/service patterns

**Not optimized for**:
- Ultra-low latency (<100Î¼s) requirements
- High-throughput async services with hot-path registration
- Massive concurrent writes to shared state

## Integration Patterns

### With FastAPI
```python
from fastapi import FastAPI
from provide.foundation import logger

app = FastAPI()

@app.on_event("startup")
async def startup():
    logger.info("fastapi_startup")

@app.get("/")
async def root():
    logger.info("request_received", endpoint="/")
    return {"status": "ok"}
```

### With Django
```python
# settings.py
from provide.foundation import get_hub, LoggingConfig, TelemetryConfig

# Initialize Foundation
config = TelemetryConfig(
    service_name="django-app",
    logging=LoggingConfig(default_level="INFO")
)
get_hub().initialize_foundation(config)

# Use Foundation logger instead of Django's
from provide.foundation import logger

class MyView(View):
    def get(self, request):
        logger.info("view_accessed", view="MyView")
```

### With Celery
```python
from celery import Celery
from provide.foundation import logger

app = Celery('tasks')

@app.task
def process_data(data_id):
    logger.info("task_started", task="process_data", data_id=data_id)
    # Task logic
    logger.info("task_completed", task="process_data", data_id=data_id)
```

## Testing Support

Foundation provides comprehensive testing support via `provide-testkit`:

```python
import pytest
from provide.testkit import reset_foundation_setup_for_testing

@pytest.fixture(autouse=True)
def reset_foundation():
    """Reset Foundation state before each test."""
    reset_foundation_setup_for_testing()
```

### Test Isolation

- **State reset**: Clean state between tests
- **Log stream capture**: Capture Foundation logs in tests
- **Component mocking**: Easy mocking of registered components
- **Context detection**: Automatic test environment detection

## Design Trade-offs

### Global State Pattern

**Decision**: Use global singletons (`get_hub()`, `logger`)

**Benefits**:
- Ergonomic API without passing objects everywhere
- Consistent access patterns across modules
- Simple integration with existing code

**Mitigation**:
- `provide-testkit` provides clean test isolation
- Lazy initialization minimizes side effects
- Clear initialization points for configuration

### Threading vs Async

**Decision**: Use `threading.RLock` in registry, not `asyncio.Lock`

**Rationale**:
- Broader compatibility (sync and async code)
- Negligible overhead for typical use cases
- Simpler mental model for CLI applications

**When to reconsider**:
- Building ultra-high-throughput async services
- Hot-path registration in async request handlers
- >10k req/sec with runtime component registration

### Opinionated Stack

**Decision**: Tightly integrated with `structlog`, `attrs`, `click`

**Benefits**:
- Cohesive, well-tested integration
- Superior developer experience
- Consistent patterns across features

**Trade-offs**:
- Less flexibility to swap dependencies
- Learning curve if unfamiliar with chosen tools
- May conflict with existing tool choices

---

## Summary

provide.foundation's architecture prioritizes:

1. **Developer Experience**: Intuitive APIs, minimal boilerplate
2. **Production Readiness**: Thread-safe, performant, observable
3. **Extensibility**: Clear extension points for customization
4. **Testing**: First-class testing support with clean isolation
5. **Framework Agnostic**: Works with any Python framework or standalone

The result is a foundation layer that provides robust infrastructure without dictating application architecture.
>>> EOF >>>

### FILE 5: explanation/dependency-injection.md | checksum=feaa94c3b1fb... | modified=2025-10-24T20:07:00 | op=+ | size=17881 | tokens=3917 | type=markdown ###
<<< BOF <<<
# The Polyglot Dependency Injection Pattern

A core architectural philosophy of `provide.foundation` is to promote a dependency injection (DI) pattern that is consistent and idiomatic across multiple programming languages, specifically Python, Go, and Rust.

## Overview

Dependency Injection is a design pattern where objects receive their dependencies from external sources rather than creating them internally. Foundation embraces a **constructor injection** pattern that works identically across Python, Go, and Rust, making it easier for polyglot teams to maintain consistent architecture.

**Key benefits:**
- **Testability** - Easy to mock dependencies in tests
- **Flexibility** - Swap implementations without changing code
- **Clarity** - Dependencies are explicit and visible
- **Polyglot consistency** - Same pattern across languages
- **No magic** - Explicit wiring, no runtime reflection

## The Core Pattern

The polyglot DI pattern consists of two key principles:

### 1. Explicit Constructor Injection

Components declare their dependencies as constructor arguments:

**Python:**
```python
class UserService:
    def __init__(self, user_repo: UserRepository, logger: Logger):
        self.user_repo = user_repo
        self.logger = logger
```

**Go:**
```go
type UserService struct {
    userRepo *UserRepository
    logger   *Logger
}

func NewUserService(userRepo *UserRepository, logger *Logger) *UserService {
    return &UserService{userRepo: userRepo, logger: logger}
}
```

**Rust:**
```rust
struct UserService {
    user_repo: UserRepository,
    logger: Logger,
}

impl UserService {
    fn new(user_repo: UserRepository, logger: Logger) -> Self {
        Self { user_repo, logger }
    }
}
```

### 2. Composition Root

A single location (typically `main()`) creates and wires all components:

```python
def main():
    """Composition root - wire dependencies here."""
    # Create foundation components
    logger = get_logger()

    # Create infrastructure
    db = Database(connection_string)
    cache = RedisCache(redis_url)

    # Create repositories
    user_repo = UserRepository(db, logger)

    # Create services
    user_service = UserService(user_repo, logger, cache)

    # Create application
    app = Application(user_service, logger)
    app.run()
```

## The Polyglot Advantage

By adhering to this pattern, developers can immediately understand the architecture of a service regardless of implementation language.

### Python Example with Foundation Hub

Foundation provides the Hub for dependency injection and component management:

```python
from provide.foundation import get_hub, logger
from provide.foundation.hub import injectable

# Mark classes as injectable for automatic dependency resolution
@injectable
class Database:
    """Database connection."""

    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        self._conn = None

    def connect(self):
        """Establish database connection."""
        self._conn = create_connection(self.connection_string)
        logger.info("database_connected")

@injectable
class UserRepository:
    """User data repository."""

    def __init__(self, db: Database):
        self.db = db

    def get_user(self, user_id: str):
        """Get user by ID."""
        return self.db.query("SELECT * FROM users WHERE id = ?", user_id)

@injectable
class UserService:
    """User business logic."""

    def __init__(self, user_repo: UserRepository):
        self.user_repo = user_repo

    def authenticate(self, user_id: str, password: str):
        """Authenticate user."""
        user = self.user_repo.get_user(user_id)
        return verify_password(user, password)

# Composition root
def main():
    hub = get_hub()
    hub.initialize_foundation()

    # Register infrastructure dependencies
    db = Database("postgresql://localhost/mydb")
    hub.register(Database, db)

    # Resolve service with automatic dependency injection
    user_service = hub.resolve(UserService)

    # Use service
    user_service.authenticate("user123", "password")
```

### Go Example (Manual Wiring)

The same pattern in Go, manually wired:

```go
type Database struct {
    connString string
    conn       *sql.DB
}

func NewDatabase(connString string) *Database {
    return &Database{connString: connString}
}

func (d *Database) Connect() error {
    conn, err := sql.Open("postgres", d.connString)
    if err != nil {
        return err
    }
    d.conn = conn
    log.Println("database_connected")
    return nil
}

type UserRepository struct {
    db *Database
}

func NewUserRepository(db *Database) *UserRepository {
    return &UserRepository{db: db}
}

func (r *UserRepository) GetUser(userID string) (*User, error) {
    var user User
    err := r.db.conn.QueryRow("SELECT * FROM users WHERE id = $1", userID).Scan(&user)
    return &user, err
}

type UserService struct {
    userRepo *UserRepository
}

func NewUserService(userRepo *UserRepository) *UserService {
    return &UserService{userRepo: userRepo}
}

func (s *UserService) Authenticate(userID, password string) (bool, error) {
    user, err := s.userRepo.GetUser(userID)
    if err != nil {
        return false, err
    }
    return verifyPassword(user, password), nil
}

// Composition root
func main() {
    // Create infrastructure
    db := NewDatabase("postgresql://localhost/mydb")
    if err := db.Connect(); err != nil {
        log.Fatal(err)
    }

    // Create repositories
    userRepo := NewUserRepository(db)

    // Create services
    userService := NewUserService(userRepo)

    // Use service
    authenticated, _ := userService.Authenticate("user123", "password")
}
```

The mental model is **identical** across both languages, which is a powerful advantage for polyglot teams.

## Python Implementation Patterns

### Constructor Injection

Declare dependencies in `__init__`:

```python
class EmailService:
    """Send emails via SMTP."""

    def __init__(self, smtp_client: SMTPClient, template_engine: TemplateEngine):
        self.smtp = smtp_client
        self.templates = template_engine

    def send_welcome_email(self, user: User):
        """Send welcome email to new user."""
        template = self.templates.render("welcome.html", user=user)
        self.smtp.send(user.email, "Welcome!", template)
```

### Property Injection (Avoid)

While Python supports property injection, **avoid it** for the polyglot pattern:

```python
# âŒ Bad: Property injection (doesn't translate to Go/Rust)
class BadService:
    smtp: SMTPClient  # Set after construction

    def send_email(self):
        self.smtp.send(...)  # Could be None!

# âœ… Good: Constructor injection
class GoodService:
    def __init__(self, smtp: SMTPClient):
        self.smtp = smtp  # Guaranteed to exist
```

### Interface-Based Dependencies

Use abstract base classes for flexibility:

```python
from abc import ABC, abstractmethod

class UserRepository(ABC):
    """Abstract user repository."""

    @abstractmethod
    def get_user(self, user_id: str) -> User:
        """Get user by ID."""
        pass

    @abstractmethod
    def save_user(self, user: User) -> None:
        """Save user."""
        pass

class PostgresUserRepository(UserRepository):
    """PostgreSQL implementation."""

    def __init__(self, db: Database):
        self.db = db

    def get_user(self, user_id: str) -> User:
        return self.db.query_one("SELECT * FROM users WHERE id = ?", user_id)

    def save_user(self, user: User) -> None:
        self.db.execute("INSERT INTO users VALUES (?, ?)", user.id, user.name)

class InMemoryUserRepository(UserRepository):
    """In-memory implementation for testing."""

    def __init__(self):
        self.users = {}

    def get_user(self, user_id: str) -> User:
        return self.users.get(user_id)

    def save_user(self, user: User) -> None:
        self.users[user.id] = user

# Service depends on interface, not implementation
class UserService:
    def __init__(self, user_repo: UserRepository):  # Abstract type
        self.user_repo = user_repo
```

## Composition Root Patterns

### Simple Main Function

For CLI applications:

```python
def main():
    """Application entry point and composition root."""
    # Initialize Foundation
    hub = get_hub()
    hub.initialize_foundation()

    # Configuration
    config = load_config()

    # Infrastructure
    db = Database(config.database_url)
    cache = RedisCache(config.redis_url)

    # Repositories
    user_repo = PostgresUserRepository(db)
    session_repo = CacheSessionRepository(cache)

    # Services
    auth_service = AuthService(user_repo, session_repo)
    email_service = EmailService(SMTPClient(config.smtp), TemplateEngine())

    # Application
    cli = CLI(auth_service, email_service)
    cli.run()

if __name__ == "__main__":
    main()
```

### Factory Pattern

For complex construction:

```python
class ServiceFactory:
    """Factory for creating services with dependencies."""

    def __init__(self, config: Config):
        self.config = config
        self._db = None
        self._cache = None

    @property
    def database(self) -> Database:
        """Lazy database singleton."""
        if not self._db:
            self._db = Database(self.config.database_url)
            self._db.connect()
        return self._db

    @property
    def cache(self) -> Cache:
        """Lazy cache singleton."""
        if not self._cache:
            self._cache = RedisCache(self.config.redis_url)
        return self._cache

    def create_user_service(self) -> UserService:
        """Create user service with dependencies."""
        user_repo = PostgresUserRepository(self.database)
        return UserService(user_repo, self.cache)

    def create_email_service(self) -> EmailService:
        """Create email service with dependencies."""
        smtp = SMTPClient(self.config.smtp)
        templates = TemplateEngine()
        return EmailService(smtp, templates)

# Usage
def main():
    config = load_config()
    factory = ServiceFactory(config)

    user_service = factory.create_user_service()
    email_service = factory.create_email_service()

    app = Application(user_service, email_service)
    app.run()
```

### Hub-Based Composition

Using Foundation's Hub for automatic dependency resolution:

```python
from provide.foundation import get_hub
from provide.foundation.hub import injectable

# Mark components as injectable
@injectable
class Database:
    def __init__(self, url: str):
        self.url = url

@injectable
class UserRepository:
    def __init__(self, db: Database):  # Auto-resolved
        self.db = db

@injectable
class UserService:
    def __init__(self, repo: UserRepository):  # Auto-resolved
        self.repo = repo

# Composition root
def main():
    hub = get_hub()
    hub.initialize_foundation()

    # Register infrastructure dependencies by type
    db = Database("postgresql://localhost/db")
    hub.register(Database, db)

    # Resolve service (dependencies auto-injected via type hints)
    user_service = hub.resolve(UserService)
```

## Testing with Dependency Injection

DI makes testing trivial by allowing mock injection:

### Unit Testing with Mocks

```python
import pytest
from unittest.mock import Mock

def test_user_service_authentication():
    """Test user authentication with mocked repository."""
    # Create mocks
    mock_repo = Mock(spec=UserRepository)
    mock_repo.get_user.return_value = User(id="123", password_hash="hashed")

    mock_cache = Mock(spec=Cache)

    # Inject mocks
    service = UserService(mock_repo, mock_cache)

    # Test
    result = service.authenticate("123", "password")

    # Verify
    assert result is True
    mock_repo.get_user.assert_called_once_with("123")
    mock_cache.set.assert_called()
```

### Integration Testing with Test Implementations

```python
def test_user_service_integration():
    """Integration test with real implementations."""
    # Use in-memory implementations
    user_repo = InMemoryUserRepository()
    cache = InMemoryCache()

    # Create service
    service = UserService(user_repo, cache)

    # Setup test data
    user = User(id="123", name="Alice")
    user_repo.save_user(user)

    # Test
    result = service.get_user("123")
    assert result.name == "Alice"
```

### Test Fixtures

```python
@pytest.fixture
def database():
    """Provide test database."""
    db = Database(":memory:")  # SQLite in-memory
    db.initialize_schema()
    yield db
    db.close()

@pytest.fixture
def user_repository(database):
    """Provide user repository with test database."""
    return PostgresUserRepository(database)

@pytest.fixture
def user_service(user_repository):
    """Provide user service with test dependencies."""
    cache = InMemoryCache()
    return UserService(user_repository, cache)

def test_with_fixtures(user_service):
    """Test using injected fixtures."""
    user = user_service.create_user("Alice", "alice@example.com")
    assert user.name == "Alice"
```

## Common Patterns

### Service Layer Pattern

Organize business logic into services:

```python
# Domain layer
class User:
    """User domain model."""
    pass

# Repository layer
class UserRepository:
    """Data access."""
    def __init__(self, db: Database):
        self.db = db

# Service layer
class UserService:
    """Business logic."""
    def __init__(self, user_repo: UserRepository, email_service: EmailService):
        self.user_repo = user_repo
        self.email_service = email_service

    def register_user(self, email: str, password: str) -> User:
        """Register new user."""
        user = User(email=email, password=hash_password(password))
        self.user_repo.save(user)
        self.email_service.send_welcome(user)
        return user

# Application layer
class Application:
    """Application orchestration."""
    def __init__(self, user_service: UserService):
        self.user_service = user_service
```

### Resource Management

Manage resource lifecycles:

```python
class DatabaseConnection:
    """Managed database connection."""

    def __init__(self, url: str):
        self.url = url
        self._conn = None

    def __enter__(self):
        """Acquire connection."""
        self._conn = connect(self.url)
        return self._conn

    def __exit__(self, *args):
        """Release connection."""
        if self._conn:
            self._conn.close()

class UserService:
    def __init__(self, db: DatabaseConnection):
        self.db = db

    def get_user(self, user_id: str):
        """Get user (connection auto-managed)."""
        with self.db as conn:
            return conn.query("SELECT * FROM users WHERE id = ?", user_id)
```

## Best Practices

### âœ… DO: Declare Dependencies in Constructor

```python
# âœ… Good: Dependencies explicit and required
class Service:
    def __init__(self, repo: Repository, logger: Logger):
        self.repo = repo
        self.logger = logger
```

### âœ… DO: Depend on Abstractions

```python
# âœ… Good: Depend on interface
class Service:
    def __init__(self, repo: UserRepository):  # Abstract base class
        self.repo = repo

# âŒ Bad: Depend on concrete implementation
class Service:
    def __init__(self, repo: PostgresUserRepository):  # Concrete class
        self.repo = repo
```

### âœ… DO: Keep Composition Root Simple

```python
# âœ… Good: Clear, explicit wiring
def main():
    db = Database(url)
    repo = UserRepository(db)
    service = UserService(repo)

# âŒ Bad: Complex logic in composition root
def main():
    if os.getenv("USE_POSTGRES"):
        db = PostgresDatabase(...)
    else:
        db = MySQLDatabase(...)
    # Too much conditional logic
```

### âŒ DON'T: Use Service Locator Pattern

```python
# âŒ Bad: Service locator (hidden dependencies)
class BadService:
    def do_work(self):
        repo = ServiceLocator.get(UserRepository)  # Hidden!
        repo.get_user("123")

# âœ… Good: Explicit injection
class GoodService:
    def __init__(self, repo: UserRepository):  # Visible!
        self.repo = repo

    def do_work(self):
        self.repo.get_user("123")
```

### âŒ DON'T: Create Dependencies Internally

```python
# âŒ Bad: Creates own dependencies
class BadService:
    def __init__(self):
        self.db = Database("hard-coded-url")  # Can't test!

# âœ… Good: Dependencies injected
class GoodService:
    def __init__(self, db: Database):  # Easy to test!
        self.db = db
```

## Comparison with Other Approaches

### vs. Singleton Pattern

```python
# âŒ Singleton: Hidden dependencies, hard to test
class Database:
    _instance = None

    @classmethod
    def get_instance(cls):
        if not cls._instance:
            cls._instance = Database()
        return cls._instance

class Service:
    def do_work(self):
        db = Database.get_instance()  # Hidden dependency

# âœ… DI: Explicit dependencies, easy to test
class Service:
    def __init__(self, db: Database):  # Clear dependency
        self.db = db
```

### vs. Global Variables

```python
# âŒ Global: Implicit coupling, hard to test
DATABASE = Database("url")

class Service:
    def do_work(self):
        DATABASE.query(...)  # Global dependency

# âœ… DI: Explicit injection
class Service:
    def __init__(self, db: Database):
        self.db = db
```

## Next Steps

### Related Guides
- **[Testing](../how-to-guides/testing/unit-tests.md)**: Testing with dependency injection
- **[Architecture](architecture.md)**: Overall Foundation architecture

### Examples
- See `examples/di/01_polyglot_di_pattern.py` for polyglot DI examples
- See `examples/cli/` for CLI applications using DI

---

**Tip**: Start with constructor injection and explicit wiring in your composition root. Use Foundation's Hub for automatic dependency resolution once you're comfortable with the pattern.
>>> EOF >>>

### FILE 6: getting-started/examples.md | checksum=92d365b9cf64... | modified=2025-10-24T18:07:03 | op=+ | size=7095 | tokens=1534 | type=markdown ###
<<< BOF <<<
# Examples

The Foundation repository includes a comprehensive collection of working examples demonstrating all major features. All examples are located in the `examples/` directory of the source repository.

## Running Examples

Clone the repository and run any example:

```bash
git clone https://github.com/provide-io/provide-foundation.git
cd provide-foundation
pip install -e ".[all]"

# Run an example
python examples/telemetry/01_basic_logging.py
```

## Example Categories

### Getting Started

Perfect for learning the basics:

- **[telemetry/01_basic_logging.py](https://github.com/provide-io/provide-foundation/blob/main/examples/telemetry/01_basic_logging.py)** - Zero-setup logging introduction
- **[telemetry/02_structured_logging.py](https://github.com/provide-io/provide-foundation/blob/main/examples/telemetry/02_structured_logging.py)** - Structured logging with full Hub setup

### Telemetry & Logging

Core logging features and patterns:

- **01_basic_logging.py** - Simple start with zero configuration
- **02_structured_logging.py** - Hub-based initialization and configuration
- **03_named_loggers.py** - Component-specific named loggers
- **04_das_pattern.py** - Domain-Action-Status structured logging
- **05_exception_handling.py** - Exception logging with automatic tracebacks
- **06_trace_logging.py** - TRACE level logging for verbose output
- **07_module_filtering.py** - Module-specific log level configuration

**[View telemetry examples](https://github.com/provide-io/provide-foundation/tree/main/examples/telemetry)**

### Configuration Management

Environment and file-based configuration:

- **01_custom_config.py** - Custom TelemetryConfig and LoggingConfig
- **02_env_variables.py** - Environment variable configuration
- **03_config_management.py** - Complete configuration system with file loading

**[View configuration examples](https://github.com/provide-io/provide-foundation/tree/main/examples/configuration)**

### CLI Applications

Build command-line tools:

- **01_cli_application.py** - Complete CLI with Hub and command system
- **02_dogfooding_cli.py** - Foundation CLI using its own tools

**[View CLI examples](https://github.com/provide-io/provide-foundation/tree/main/examples/cli)**

### Async Programming

Using Foundation with asyncio:

- **01_async_usage.py** - Async application patterns with Foundation

**[View async examples](https://github.com/provide-io/provide-foundation/tree/main/examples/async)**

### HTTP Transport

HTTP client usage with middleware:

- **01_http_client.py** - HTTP requests with middleware and error handling

**[View transport examples](https://github.com/provide-io/provide-foundation/tree/main/examples/transport)**

### Distributed Tracing

OpenTelemetry integration:

- **01_simple_tracing.py** - Basic tracing with Foundation
- **02_distributed_tracing.py** - Distributed tracing across services

**[View tracing examples](https://github.com/provide-io/provide-foundation/tree/main/examples/tracing)**

### File Operations

Safe file handling and monitoring:

- **01_basic_usage.py** - Basic file operations
- **02_streaming_detection.py** - Detect streaming file changes
- **03_real_filesystem_monitoring.py** - Monitor filesystem events
- **04_quality_analysis.py** - File quality analysis

**[View file examples](https://github.com/provide-io/provide-foundation/tree/main/examples/file_operations)**

### Dependency Injection

Polyglot dependency injection patterns:

- **01_polyglot_di_pattern.py** - Dependency injection using the Hub system

**[View DI examples](https://github.com/provide-io/provide-foundation/tree/main/examples/di)**

### Production Patterns

Production-ready application patterns:

- **01_production_patterns.py** - Production logging and monitoring
- **02_error_handling.py** - Comprehensive error handling with resilience

**[View production examples](https://github.com/provide-io/provide-foundation/tree/main/examples/production)**

### Integration Examples

Third-party integrations:

#### Celery Integration
- **01_setup_and_config.py** - Celery setup and configuration
- **02_metrics_and_signals.py** - Metrics collection and signal handling
- **03_tasks.py** - Task definitions
- **04_runner.py** - Running Celery workers

**[View Celery examples](https://github.com/provide-io/provide-foundation/tree/main/examples/integration/celery)**

#### OpenObserve Integration
- **01_openobserve_integration.py** - Log aggregation with OpenObserve
- **02_metrics_integration.py** - Metrics integration with OpenObserve

**[View OpenObserve examples](https://github.com/provide-io/provide-foundation/tree/main/examples/openobserve)**

#### Task Queue Patterns
- **01a_basic_task_queue.py** - Task queue patterns with async workers (no external dependencies)

**[View all integration examples](https://github.com/provide-io/provide-foundation/tree/main/examples/integration)**

## Example Structure

Each example includes:

- **Complete working code** - Copy and run immediately
- **Inline documentation** - Explains what each section does
- **Expected output** - Shows what you should see
- **Usage instructions** - How to run and customize

## Common Patterns

All examples demonstrate these Foundation patterns:

### Structured Logging
```python
from provide.foundation import logger

logger.info(
    "user_action",
    user_id="123",
    action="login",
    source="web",
)
```

### Configuration
```python
from provide.foundation import get_hub
from provide.foundation.logger.config import TelemetryConfig

hub = get_hub()
hub.initialize_foundation(
    TelemetryConfig(service_name="my-service")
)
```

### Error Handling
```python
from provide.foundation import logger

try:
    # Operation that might fail
    process_data()
except Exception as e:
    logger.exception("operation_failed", operation="process_data")
    raise
```

### Output Separation
```python
from provide.foundation import logger, pout, perr

# System logs (for operators)
logger.info("processing_file", filename="data.csv")

# User output (for CLI users)
pout("âœ… File processed successfully", color="green")
```

## Running All Examples

Test all examples at once:

```bash
# From the repository root
for example in examples/**/*.py; do
    echo "Running $example..."
    python "$example"
done
```

## Contributing Examples

Examples are always welcome! If you've built something useful:

1. Fork the repository
2. Add your example to the appropriate category
3. Include documentation and expected output
4. Submit a pull request

See [CONTRIBUTING.md](https://github.com/provide-io/provide-foundation/blob/main/CONTRIBUTING.md) for guidelines.

## Next Steps

After exploring examples:

- **[How-To Guides](../how-to-guides/logging/basic-logging.md)** - Solve specific problems
- **[API Reference](../reference/index.md)** - Detailed API documentation
- **[Explanation](../explanation/architecture.md)** - Understand architecture

---

**Browse all examples:** [github.com/provide-io/provide-foundation/tree/main/examples](https://github.com/provide-io/provide-foundation/tree/main/examples)
>>> EOF >>>

### FILE 7: getting-started/first-app.md | checksum=d1a77c16c582... | modified=2025-10-24T20:06:34 | op=+ | size=8276 | tokens=2057 | type=markdown ###
<<< BOF <<<
# Your First Application

Build a complete CLI task manager in 15 minutes. This tutorial demonstrates how Foundation's logging, CLI framework, and console output utilities work together.

## What We'll Build

A command-line task manager with:
- Add, complete, and list tasks
- Structured logging for every action
- Clean separation of logs from user output
- Beautiful console output with colors and emojis

**Final result:**
```bash
$ task-manager add "Write documentation"
âœ… Successfully added task 1: 'Write documentation'

$ task-manager list
ðŸ“‹ Your Tasks:
  â³ [1] Write documentation

$ task-manager complete 1
Task 1 marked as complete.
```

## 1. Project Setup

Create a new project directory:

```bash
mkdir task-manager
cd task-manager
pip install "provide-foundation[all]"
```

## 2. Create the Application

Create a file named `task_manager.py`:

```python
#!/usr/bin/env python3
# task_manager.py
import sys
from dataclasses import dataclass, field
from datetime import datetime

from provide.foundation import logger, pout, perr, get_hub
from provide.foundation.hub import register_command

# --- Data Model ---
@dataclass
class Task:
    """A simple task model."""
    id: int
    title: str
    completed: bool = False
    created_at: datetime = field(default_factory=datetime.now)

# --- In-Memory "Database" ---
TASKS: dict[int, Task] = {}
NEXT_ID = 1

# --- CLI Commands ---
@register_command("add")
def add_task(title: str):
    """Add a new task."""
    global NEXT_ID
    task = Task(id=NEXT_ID, title=title)
    TASKS[task.id] = task
    logger.info("task_created", task_id=task.id, title=task.title, emoji="âœ…")
    pout(f"Successfully added task {task.id}: '{task.title}'", color="green")
    NEXT_ID += 1

@register_command("complete")
def complete_task(task_id: int):
    """Mark a task as completed."""
    if task_id not in TASKS:
        logger.warning("task_not_found", task_id=task_id, emoji="â“")
        perr(f"Error: Task with ID {task_id} not found.", color="red")
        sys.exit(1)

    TASKS[task_id].completed = True
    logger.info("task_completed", task_id=task_id, emoji="ðŸŽ‰")
    pout(f"Task {task_id} marked as complete.", color="cyan")

@register_command("list")
def list_tasks(all: bool = False):
    """List tasks. Use --all to include completed tasks."""
    logger.debug("listing_tasks", show_all=all)
    tasks_to_show = list(TASKS.values())
    if not all:
        tasks_to_show = [t for t in tasks_to_show if not t.completed]

    if not tasks_to_show:
        pout("No tasks to show.", color="yellow")
        return

    pout("ðŸ“‹ Your Tasks:", bold=True)
    for task in tasks_to_show:
        status = "âœ…" if task.completed else "â³"
        color = "green" if task.completed else "yellow"
        pout(f"  {status} [{task.id}] {task.title}", color=color)

# --- Main Entry Point ---
if __name__ == "__main__":
    # 1. Get the global Hub instance
    hub = get_hub()

    # 2. The Hub discovers @register_command functions and builds a CLI
    cli = hub.create_cli(name="task-manager")

    # 3. Run the CLI
    logger.info("cli_starting", emoji="ðŸš€")
    cli()
    logger.info("cli_finished", emoji="ðŸ")
```

*This code is based on `examples/cli/01_cli_application.py`.*

## 3. Run Your Application

### Get Help

```bash
$ python task_manager.py --help
Usage: task-manager [OPTIONS] COMMAND [ARGS]...

  A simple task manager.

Options:
  --help  Show this message and exit.

Commands:
  add       Add a new task.
  complete  Mark a task as completed.
  list      List tasks.
```

### Add Tasks

```bash
$ python task_manager.py add "Write documentation"
âœ… Successfully added task 1: 'Write documentation'

$ python task_manager.py add "Review pull requests"
âœ… Successfully added task 2: 'Review pull requests'

$ python task_manager.py add "Deploy to production"
âœ… Successfully added task 3: 'Deploy to production'
```

### List Tasks

```bash
$ python task_manager.py list
ðŸ“‹ Your Tasks:
  â³ [1] Write documentation
  â³ [2] Review pull requests
  â³ [3] Deploy to production
```

### Complete Tasks

```bash
$ python task_manager.py complete 1
Task 1 marked as complete.

$ python task_manager.py list
ðŸ“‹ Your Tasks:
  â³ [2] Review pull requests
  â³ [3] Deploy to production

$ python task_manager.py list --all
ðŸ“‹ Your Tasks:
  âœ… [1] Write documentation
  â³ [2] Review pull requests
  â³ [3] Deploy to production
```

## 4. Understanding the Code

### Declarative CLI Commands

The `@register_command` decorator registers functions as CLI commands:

```python
@register_command("add")
def add_task(title: str):
    """Add a new task."""
    # Function signature becomes CLI arguments
    # Docstring becomes help text
```

Foundation automatically:
- Converts function parameters to CLI arguments
- Generates help text from docstrings
- Handles type conversion (str, int, bool, etc.)

### The create_cli() Method

The `hub.create_cli()` method builds a Click CLI from registered commands:

```python
cli = hub.create_cli(
    name="task-manager",        # CLI program name
    version="1.0.0"             # Version string (optional)
)
```

**Parameters:**
- `name` (str): CLI name shown in help text (default: "cli")
- `version` (str | None): Optional version for `--version` flag
- `**kwargs`: Additional Click Group options (e.g., `help`, `context_settings`)

### Structured Logging

Every action is logged with structured data:

```python
logger.info("task_created", task_id=task.id, title=task.title, emoji="âœ…")
```

Benefits:
- Easy to search logs for specific events (`task_created`)
- Filterable by any field (`task_id=123`)
- Machine-readable for log aggregation systems

### Output Separation

Foundation separates concerns:

```python
# For system logs (operators/debugging)
logger.info("task_created", task_id=1)

# For user feedback (CLI output)
pout("âœ… Successfully added task", color="green")
```

This allows you to:
- Send logs to files/services without cluttering user output
- Format user messages beautifully with colors
- Keep structured logs for analysis

## 5. Adding Persistence (Optional)

Extend the task manager with file-based persistence:

```python
from provide.foundation.serialization import provide_dumps, provide_loads
from pathlib import Path

TASKS_FILE = Path("tasks.json")

def save_tasks():
    """Save tasks to disk."""
    data = {
        "tasks": [
            {"id": t.id, "title": t.title, "completed": t.completed}
            for t in TASKS.values()
        ],
        "next_id": NEXT_ID,
    }
    TASKS_FILE.write_text(provide_dumps(data, indent=2))
    logger.debug("tasks_saved", count=len(TASKS))

def load_tasks():
    """Load tasks from disk."""
    global NEXT_ID
    if not TASKS_FILE.exists():
        return

    data = provide_loads(TASKS_FILE.read_text())
    for task_data in data["tasks"]:
        task = Task(**task_data)
        TASKS[task.id] = task
    NEXT_ID = data["next_id"]
    logger.debug("tasks_loaded", count=len(TASKS))

# Call load_tasks() at startup
# Call save_tasks() after each modification
```

## 6. What You've Learned

âœ… **Declarative CLI** - Define commands with `@register_command`
âœ… **Structured Logging** - Track actions with key-value logging
âœ… **Output Separation** - Logs for operators, `pout`/`perr` for users
âœ… **Hub System** - Central registry for commands and components
âœ… **Beautiful Console** - Colors and emojis for better UX

## Next Steps

### Explore More Features

- **[CLI Commands Guide](../how-to-guides/cli/commands.md)** - Advanced CLI patterns
- **[Basic Logging](../how-to-guides/logging/basic-logging.md)** - More logging techniques
- **[Retry Patterns](../how-to-guides/resilience/retry.md)** - Add resilience to your app

### See More Examples

Browse the [Examples](examples.md) section for:
- Configuration management
- HTTP client usage
- Async programming
- Production patterns

### Build Production Applications

- Add configuration for different environments
- Implement error handling and retries
- Add metrics and monitoring
- Deploy as a package

---

**Congratulations!** You've built a complete CLI application with Foundation.

**Next:** Explore [How-To Guides](../how-to-guides/logging/basic-logging.md) for specific use cases.
>>> EOF >>>

### FILE 8: getting-started/index.md | checksum=b454f0bb8957... | modified=2025-10-24T17:08:17 | op=+ | size=3210 | tokens=767 | type=markdown ###
<<< BOF <<<
# Getting Started with provide.foundation

Welcome to provide.foundation! This section will guide you through installation and your first application in just a few minutes.

## What is provide.foundation?

provide.foundation is a comprehensive Python library for building robust, production-ready applications with:

- **Structured Logging**: Beautiful, performant logging with zero configuration
- **CLI Framework**: Build command-line tools with declarative commands
- **Configuration**: Environment-based configuration without hardcoded defaults
- **Resilience**: Retry patterns, error handling, and failure recovery
- **Utilities**: File operations, cryptography, serialization, and more

## Quick Navigation

<div class="getting-started-grid">
  <div class="getting-started-card">
    <h3>ðŸ“¦ Installation</h3>
    <p>Install Foundation and configure your environment</p>
    <p><a href="installation/">Start Here â†’</a></p>
  </div>
  <div class="getting-started-card">
    <h3>âš¡ Quick Start</h3>
    <p>Write your first Foundation application in 5 minutes</p>
    <p><a href="quick-start/">Get Coding â†’</a></p>
  </div>
  <div class="getting-started-card">
    <h3>ðŸ—ï¸ First Application</h3>
    <p>Build a complete CLI task manager</p>
    <p><a href="first-app/">Build Something â†’</a></p>
  </div>
  <div class="getting-started-card">
    <h3>ðŸ’¡ Examples</h3>
    <p>Explore code examples for all features</p>
    <p><a href="examples/">See Examples â†’</a></p>
  </div>
</div>

## Learning Path

We recommend following this path:

1. **[Installation](installation.md)** - Set up your environment (2 minutes)
2. **[Quick Start](quick-start.md)** - Write your first logs (5 minutes)
3. **[First Application](first-app.md)** - Build a CLI tool (15 minutes)
4. **[Examples](examples.md)** - Explore specific features (ongoing)

After completing these, dive into the [How-To Guides](../how-to-guides/logging/basic-logging.md) for specific use cases.

## System Requirements

- Python 3.11 or higher
- Works on Linux, macOS, and Windows
- Minimal core dependencies (`structlog`, `attrs`)

## Optional Features

Foundation has modular installation options:

| Feature | Install Command | When You Need It |
|---------|----------------|------------------|
| **Basic logging** | `pip install provide-foundation` | Core functionality |
| **CLI framework** | `pip install provide-foundation[cli]` | Building command-line tools |
| **Cryptography** | `pip install provide-foundation[crypto]` | Hashing, signing, certificates |
| **HTTP Transport** | `pip install provide-foundation[transport]` | HTTP client utilities |
| **OpenTelemetry** | `pip install provide-foundation[opentelemetry]` | Distributed tracing |
| **All features** | `pip install provide-foundation[all]` | Everything above |

## Need Help?

- **Documentation:** Browse the [How-To Guides](../how-to-guides/logging/basic-logging.md) and [Explanation](../explanation/architecture.md) sections
- **Examples:** Check the [Examples](examples.md) for working code
- **Issues:** Report bugs on [GitHub Issues](https://github.com/provide-io/provide-foundation/issues)

---

Ready to begin? Start with [Installation](installation.md) â†’
>>> EOF >>>

### FILE 9: getting-started/installation.md | checksum=9519a163e3f4... | modified=2025-10-24T17:28:22 | op=+ | size=6609 | tokens=1541 | type=markdown ###
<<< BOF <<<
# Installation

This guide covers installing provide.foundation and setting up your development environment.

## Requirements

- **Python 3.11 or higher** - Foundation uses modern Python features
- **pip or uv** - Package manager for installation
- **Virtual environment** (recommended) - For isolated dependencies

## Basic Installation

### Using pip

The simplest installation provides core logging functionality:

```bash
pip install provide-foundation
```

This installs the base package with essential dependencies:
- `structlog` - Structured logging foundation
- `attrs` - Data class utilities
- `aiofiles` - Async file I/O operations
- `tomli_w` - TOML file writing

### Using uv (Recommended for Development)

For faster dependency resolution:

```bash
# Install uv if you haven't already
pip install uv

# Install Foundation
uv pip install provide-foundation
```

## Installation Options

Foundation offers modular installation through "extras" that add optional features:

### All Features

For the complete experience:

```bash
pip install "provide-foundation[all]"
```

### Specific Features

Install only what you need:

#### CLI Framework
```bash
pip install "provide-foundation[cli]"
```
**Adds:** `click` for command-line interface building

**Use when:** Building command-line tools, developer utilities

#### Cryptography
```bash
pip install "provide-foundation[crypto]"
```
**Adds:** `cryptography` library for secure operations

**Use when:** Need hashing, signatures, or certificate management

#### HTTP Transport
```bash
pip install "provide-foundation[transport]"
```
**Adds:** `httpx` for HTTP client operations

**Use when:** Making HTTP requests with middleware and error handling

#### OpenTelemetry
```bash
pip install "provide-foundation[opentelemetry]"
```
**Adds:** OpenTelemetry SDK for distributed tracing

**Use when:** Building microservices with distributed tracing needs

#### Compression
```bash
pip install "provide-foundation[compression]"
```
**Adds:** `zstandard` for high-performance compression

**Use when:** Need fast compression for archives and data transfer

#### Platform Utilities
```bash
pip install "provide-foundation[platform]"
```
**Adds:** `psutil`, `py-cpuinfo` for system information

**Use when:** Need OS/hardware detection and system monitoring

#### Process Utilities
```bash
pip install "provide-foundation[process]"
```
**Adds:** `psutil`, `setproctitle` for process control

**Use when:** Need process management and lifecycle control

#### Extended Utilities
```bash
pip install "provide-foundation[extended]"
```
**Adds:** Combination of platform and process utilities

**Use when:** Need comprehensive system-level utilities

### Combining Extras

Install multiple features:

```bash
pip install "provide-foundation[cli,crypto]"
```

## Virtual Environment Setup

### Using venv (Standard Library)

```bash
# Create virtual environment
python -m venv .venv

# Activate on macOS/Linux
source .venv/bin/activate

# Activate on Windows
.venv\Scripts\activate

# Install Foundation
pip install "provide-foundation[all]"
```

### Using uv venv (Faster)

```bash
# Create and activate environment
uv venv
source .venv/bin/activate  # macOS/Linux
# or: .venv\Scripts\activate  # Windows

# Install Foundation
uv pip install "provide-foundation[all]"
```

## Verify Installation

Check that Foundation is installed correctly:

```bash
python -c "from provide.foundation import logger; logger.info('Installation successful!')"
```

You should see a formatted log message confirming the installation.

## Development Installation

For contributing to Foundation or running examples from source:

```bash
# Clone the repository
git clone https://github.com/provide-io/provide-foundation.git
cd provide-foundation

# Create virtual environment
uv venv
source .venv/bin/activate  # macOS/Linux

# Install with development dependencies
uv sync

# Run tests to verify
pytest
```

## Dependency Overview

### Core Dependencies (Always Installed)

- **aiofiles** (>=23.2.1) - Async file I/O operations
- **attrs** (>=23.1.0) - Data class utilities
- **structlog** (>=25.3.0) - Structured logging engine
- **tomli_w** (>=1.0.0) - TOML file writing

### Optional Dependencies

| Extra | Key Dependencies | Purpose |
|-------|-----------------|---------|
| `cli` | click >=8.1.7 | CLI framework |
| `compression` | zstandard >=0.22.0 | High-performance compression |
| `crypto` | cryptography >=45.0.7 | Cryptographic operations |
| `transport` | httpx >=0.27.0 | HTTP client |
| `opentelemetry` | opentelemetry-sdk >=1.22.0 | Distributed tracing |
| `platform` | psutil, py-cpuinfo | System/OS info utilities |
| `process` | psutil, setproctitle | Process control and lifecycle |
| `extended` | (combines platform + process) | Extended system utilities |
| `all` | (all extras above) | Complete feature set |

## Platform-Specific Notes

### macOS
- Requires macOS 10.13 or higher
- Xcode Command Line Tools recommended for cryptography
- Apple Silicon (M1/M2) fully supported

### Linux
- Works on most distributions (Ubuntu, Debian, RHEL, Alpine)
- Requires `gcc` and `libffi-dev` for cryptography on some systems
- Container images: Use Python 3.11+ base images

### Windows
- Windows 10 or higher recommended
- Microsoft C++ Build Tools required for cryptography
- PowerShell or Command Prompt supported

## Troubleshooting

### Import Errors

**Problem:** `ModuleNotFoundError: No module named 'provide'`

**Solution:** Ensure virtual environment is activated and Foundation is installed:
```bash
source .venv/bin/activate
pip list | grep provide-foundation
```

### Cryptography Installation Fails

**Problem:** Build errors when installing `[crypto]` extra

**Solution:** Install platform-specific build tools:

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt-get install build-essential libffi-dev python3-dev
```

**RHEL/CentOS:**
```bash
sudo yum install gcc libffi-devel python3-devel
```

### Version Conflicts

**Problem:** Dependency version conflicts with existing packages

**Solution:** Use a fresh virtual environment or update conflicting packages:
```bash
pip install --upgrade pip
pip install "provide-foundation[all]" --upgrade
```

## Next Steps

After installation:

1. **[Quick Start](quick-start.md)** - Write your first Foundation code
2. **[First Application](first-app.md)** - Build a complete CLI tool
3. **[Examples](examples.md)** - Explore feature-specific examples

---

**Need help?** Check the [GitHub Issues](https://github.com/provide-io/provide-foundation/issues) or ask a question.
>>> EOF >>>

### FILE 10: getting-started/quick-start.md | checksum=ebbc3e41e506... | modified=2025-10-24T20:05:16 | op=+ | size=5635 | tokens=1278 | type=markdown ###
<<< BOF <<<
# Quick Start

Get started with provide.foundation in under 5 minutes. This guide shows you the basics of structured logging and beautiful console output.

## 1. Installation

First, install the library:

```bash
pip install "provide-foundation[all]"
```

Verify the installation:
```bash
python -c "from provide.foundation import logger; logger.info('Installation successful!')"
```

## 2. Your First Log Messages

The easiest way to start is importing the global `logger` instance. Create a Python file named `app.py`:

```python
# app.py
from provide.foundation import logger

def main():
    """A simple function to demonstrate basic logging."""
    # The logger auto-initializes on first use with sensible defaults.
    # No setup required!

    logger.info("Application starting up")

    # Logging with structured context (key-value pairs)
    logger.info(
        "user_logged_in",
        user_id="usr_12345",
        source="google_oauth",
        ip_address="192.168.1.101",
    )

    logger.warning("Disk space is running low", free_space_gb=5, emoji="âš ï¸")

    # Logging an error with automatic exception info
    try:
        result = 1 / 0
    except ZeroDivisionError:
        logger.exception(
            "critical_calculation_failed",
            details="Attempted to divide by zero",
            user_id="usr_12345",
        )

    logger.info("Application shutting down")

if __name__ == "__main__":
    main()
```

*This code is based on `examples/telemetry/01_basic_logging.py`.*

## 3. Running the Example

Execute the script:

```bash
python app.py
```

## 4. Understanding the Output

You'll see beautifully formatted output in your console:

```
INFO Application starting up
INFO user_logged_in user_id=usr_12345 source=google_oauth ip_address=192.168.1.101
âš ï¸ WARN Disk space is running low free_space_gb=5
âŒ ERROR critical_calculation_failed details='Attempted to divide by zero' user_id=usr_12345
Traceback (most recent call last):
  ...
ZeroDivisionError: division by zero
INFO Application shutting down
```

### Key Features You're Seeing:

1. **Emoji & Level Prefixes**: Visual markers (`âš ï¸`, `âŒ`) provide immediate context
2. **Event Name**: First argument (`"user_logged_in"`) identifies the event
3. **Structured Context**: Keyword arguments formatted as `key=value` pairs
4. **Exception Information**: Full traceback automatically captured with `logger.exception()`

## 5. Adding User-Facing Output

Foundation separates system logs from user output:

```python
from provide.foundation import logger, pout, perr

def process_file(filename: str):
    """Process a file and show progress to the user."""

    # Log for operators/debugging (goes to logs)
    logger.info("file_processing_started", filename=filename)

    # Show to user (goes to stdout)
    pout(f"Processing {filename}...", color="cyan")

    try:
        # ... do the work ...
        logger.info("file_processing_completed", filename=filename)
        pout(f"âœ… Successfully processed {filename}", color="green")

    except Exception as e:
        logger.error("file_processing_failed", filename=filename, error=str(e))
        perr(f"âŒ Failed to process {filename}: {e}", color="red")
```

### Output Separation:
- **`logger.*`**: System logs for debugging and monitoring
- **`pout()`**: Success messages for users (stdout)
- **`perr()`**: Error messages for users (stderr)

## 6. Configuration (Optional)

Foundation works with zero configuration, but you can customize it when needed.

### When to Initialize Explicitly

**Auto-initialization (default) - Use for:**
- âœ… Simple scripts and utilities
- âœ… Development and experimentation
- âœ… When default configuration is sufficient
- âœ… Quick prototypes

**Explicit initialization - Use for:**
- âœ… Production applications
- âœ… Custom configuration requirements
- âœ… Integration with web frameworks (FastAPI, Flask, Django)
- âœ… Multiple services with different configurations
- âœ… When you need control over service name, log format, or telemetry

### Explicit Configuration Example

```python
from provide.foundation import get_hub, LoggingConfig, TelemetryConfig

# Initialize with custom configuration
config = TelemetryConfig(
    service_name="my-app",
    logging=LoggingConfig(
        default_level="INFO",
        console_formatter="json",  # Use JSON for production
    ),
)

hub = get_hub()
hub.initialize_foundation(config)

# Now use the logger normally
from provide.foundation import logger
logger.info("app_started", version="1.0.0")
```

## What You've Learned

âœ… **Zero-configuration logging** - Just import and use
âœ… **Structured logging** - Key-value pairs for machine-readable logs
âœ… **Exception handling** - Automatic traceback capture
âœ… **Output separation** - Logs vs user-facing messages
âœ… **Optional configuration** - Customize when needed

## Next Steps

### Build a Complete Application
Continue to [First Application](first-app.md) to build a full CLI task manager (15 minutes).

### Explore Specific Features

- **[Basic Logging Guide](../how-to-guides/logging/basic-logging.md)** - Learn more logging patterns
- **[Exception Logging](../how-to-guides/logging/exception-logging.md)** - Handle errors effectively
- **[CLI Commands](../how-to-guides/cli/commands.md)** - Build command-line tools

### See More Examples

Browse the [Examples](examples.md) section for:
- Configuration management
- Async logging
- HTTP client usage
- Distributed tracing
- Production patterns

---

**Questions?** Check the [How-To Guides](../how-to-guides/logging/basic-logging.md) or [API Reference](../reference/index.md).
>>> EOF >>>

### FILE 11: how-to-guides/cli/arguments.md | checksum=87b3cf266e4b... | modified=2025-10-21T17:41:51 | op=+ | size=3268 | tokens=790 | type=markdown ###
<<< BOF <<<
# Argument Parsing

Learn how to handle command-line arguments, options, and flags in your CLI applications.

## Overview

Foundation's CLI framework (built on Click) provides powerful argument parsing with automatic type conversion and validation.

## Basic Arguments

```python
from provide.foundation.hub import register_command

@register_command("greet")
def greet(name: str):
    """Greet a user by name."""
    print(f"Hello, {name}!")

# Usage: python app.py greet Alice
# Output: Hello, Alice!
```

## Optional Arguments with Defaults

```python
@register_command("greet")
def greet(name: str, greeting: str = "Hello"):
    """Greet a user with a custom greeting."""
    print(f"{greeting}, {name}!")

# Usage: python app.py greet Alice
# Output: Hello, Alice!

# Usage: python app.py greet Alice --greeting "Hi"
# Output: Hi, Alice!
```

## Type Conversion

Foundation automatically converts argument types:

```python
@register_command("process")
def process(count: int, rate: float, enabled: bool = True):
    """Process with typed arguments."""
    print(f"Count: {count} (type: {type(count).__name__})")
    print(f"Rate: {rate} (type: {type(rate).__name__})")
    print(f"Enabled: {enabled} (type: {type(enabled).__name__})")

# Usage: python app.py process 42 3.14 --enabled
```

## Lists and Multiple Values

```python
from typing import list

@register_command("batch")
def batch(files: list[str]):
    """Process multiple files."""
    for file in files:
        print(f"Processing: {file}")

# Usage: python app.py batch file1.txt file2.txt file3.txt
```

## Choice Arguments

```python
from typing import Literal

@register_command("deploy")
def deploy(environment: Literal["dev", "staging", "prod"]):
    """Deploy to a specific environment."""
    print(f"Deploying to {environment}")

# Usage: python app.py deploy prod
# Invalid: python app.py deploy invalid
# Error: Invalid value for 'environment': 'invalid' is not one of 'dev', 'staging', 'prod'
```

## File Path Arguments

```python
from pathlib import Path

@register_command("read")
def read(file: Path):
    """Read and display file contents."""
    if not file.exists():
        raise FileNotFoundError(f"File not found: {file}")

    content = file.read_text()
    print(content)

# Usage: python app.py read config.yaml
```

## Variadic Arguments

```python
@register_command("sum")
def sum_numbers(*numbers: int):
    """Sum any number of integers."""
    total = sum(numbers)
    print(f"Total: {total}")

# Usage: python app.py sum 1 2 3 4 5
# Output: Total: 15
```

## Argument Validation

```python
@register_command("process")
def process(workers: int):
    """Process with worker validation."""
    if workers < 1:
        raise ValueError("Workers must be at least 1")
    if workers > 100:
        raise ValueError("Workers cannot exceed 100")

    print(f"Processing with {workers} workers")
```

## Next Steps

- **[Building Commands](commands.md)** - Command structure
- **[Interactive Prompts](prompts.md)** - User input
- **[API Reference: CLI](../../reference/provide/foundation/cli/index.md)** - Complete CLI API

---

**See also:** [examples/cli/01_cli_application.py](https://github.com/provide-io/provide-foundation/blob/main/examples/cli/01_cli_application.py)
>>> EOF >>>

### FILE 12: how-to-guides/cli/commands.md | checksum=b75abfa4b37f... | modified=2025-10-24T18:12:08 | op=+ | size=13314 | tokens=2971 | type=markdown ###
<<< BOF <<<
# How to Register CLI Commands

`provide.foundation` simplifies CLI development by allowing you to register Python functions as commands using decorators.

## Basic Command Registration

Use the `@register_command` decorator to expose a function as a CLI command.

```python
# From: examples/cli/01_cli_application.py
from provide.foundation.hub import register_command
from provide.foundation.cli import echo_success

@register_command("init")
def init_command(name: str = "myproject", template: str = "default"):
    """Initialize a new project."""
    echo_success(f"Initializing project '{name}' with template '{template}'")
```

**Key Points:**
- Function name becomes the command handler
- Function parameters become CLI arguments/options
- Docstring becomes the help text
- Type hints enable automatic type conversion

## Argument Types and Defaults

Foundation automatically converts CLI arguments based on type hints:

```python
@register_command("deploy")
def deploy_command(
    environment: str,              # Required string argument
    version: str = "latest",       # Optional with default
    force: bool = False,           # Boolean flag (--force)
    replicas: int = 3,             # Integer option
    timeout: float = 30.0,         # Float option
):
    """Deploy application to environment."""
    echo_info(f"Deploying {version} to {environment}")
    echo_info(f"Replicas: {replicas}, Timeout: {timeout}s")
    if force:
        echo_warning("Force deployment enabled")
```

**Usage:**
```bash
# Required argument
./mycli deploy production

# With options
./mycli deploy production --version v2.0 --replicas 5

# Boolean flags
./mycli deploy production --force
```

## Nested Commands

Organize commands into groups using dot notation:

```python
@register_command("db.migrate")
def migrate_database():
    """Run database migrations."""
    echo_info("Running migrations...")

@register_command("db.seed")
def seed_database(dataset: str = "default"):
    """Seed database with test data."""
    echo_info(f"Seeding database with {dataset} dataset...")

@register_command("db.status")
def database_status():
    """Show database connection status."""
    echo_info("Database status: Connected")
```

**Usage:**
```bash
./mycli db migrate
./mycli db seed --dataset production
./mycli db status
```

**Output:**
```
Usage: mycli [OPTIONS] COMMAND [ARGS]...

Commands:
  db      Database management commands
    migrate  Run database migrations
    seed     Seed database with test data
    status   Show database connection status
```

## Command Metadata

Add metadata like aliases, categories, and tags:

```python
@register_command("status", aliases=["st", "info"], category="info")
def status_command(verbose: bool = False):
    """Show system status."""
    echo_info("System Status")
    echo_info("=" * 40)
    # ... status implementation ...
```

**Features:**
- **aliases:** Alternative command names (`st`, `info`)
- **category:** Group commands in help output
- **tags:** Metadata for filtering/searching (future feature)

## User-Facing Output

Use Foundation's console output functions for clean user feedback:

```python
from provide.foundation.cli import pout, perr, echo_success, echo_error, echo_warning, echo_info

@register_command("process")
def process_command(file: str, validate: bool = True):
    """Process a data file."""

    # Info messages (cyan)
    echo_info(f"Processing {file}...")

    # Warnings (yellow)
    if not validate:
        echo_warning("Validation disabled - proceed with caution")

    try:
        # ... processing logic ...

        # Success messages (green)
        echo_success(f"Successfully processed {file}")
        pout(f"âœ… Output saved to output/{file}", color="green")

    except Exception as e:
        # Error messages (red)
        echo_error(f"Failed to process {file}: {e}")
        perr(f"âŒ Processing failed", color="red")
        raise
```

## Validation and Error Handling

Add validation logic and provide clear error messages:

```python
from provide.foundation.errors import ValidationError

@register_command("backup")
def backup_command(source: str, destination: str, compress: bool = False):
    """Backup files from source to destination."""

    # Validate inputs
    if not os.path.exists(source):
        echo_error(f"Source directory not found: {source}")
        raise ValidationError(f"Invalid source: {source}")

    if os.path.exists(destination):
        echo_warning(f"Destination exists: {destination}")
        if not click.confirm("Overwrite?"):
            echo_info("Backup cancelled")
            return

    # Perform backup
    try:
        echo_info(f"Backing up {source} â†’ {destination}")
        # ... backup logic ...
        echo_success("Backup completed")

    except Exception as e:
        echo_error(f"Backup failed: {e}")
        raise
```

## Complex Argument Types

Handle lists, paths, and custom types:

```python
from pathlib import Path

@register_command("batch-process")
def batch_process_command(
    files: str,                    # Comma-separated list
    output_dir: str = "./output",  # Path
    formats: str = "json,csv",     # Multiple formats
):
    """Process multiple files in batch."""

    # Parse comma-separated files
    file_list = [f.strip() for f in files.split(",")]

    # Convert to Path
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # Parse formats
    format_list = [f.strip() for f in formats.split(",")]

    echo_info(f"Processing {len(file_list)} files")
    echo_info(f"Output formats: {', '.join(format_list)}")

    for file in file_list:
        echo_info(f"Processing {file}...")
        # ... process file ...
```

**Usage:**
```bash
./mycli batch-process --files "data1.txt,data2.txt,data3.txt" --formats "json,xml"
```

## Interactive Prompts

Use Click's built-in prompt functions for interactive input:

```python
import click

@register_command("configure")
def configure_command():
    """Interactive configuration wizard."""

    echo_info("Configuration Wizard")
    echo_info("=" * 40)

    # Text prompts
    api_key = click.prompt("API Key", hide_input=True)
    region = click.prompt("Region", default="us-east-1")

    # Number prompts
    timeout = click.prompt("Timeout (seconds)", type=int, default=30)

    # Confirmation
    if click.confirm("Enable debug mode?"):
        debug = True
        echo_warning("Debug mode enabled")
    else:
        debug = False

    # Choice prompts
    environment = click.prompt(
        "Environment",
        type=click.Choice(["development", "staging", "production"]),
        default="development"
    )

    echo_success("Configuration saved")
```

## Creating the CLI Application

Once commands are registered, create the CLI application:

```python
from provide.foundation import get_hub

def main():
    """Main CLI entry point."""

    # Get the hub instance
    hub = get_hub()

    # Create CLI from registered commands
    cli = hub.create_cli(
        name="mycli",
        version="1.0.0",
        help="My CLI Application"
    )

    # Run the CLI
    cli()

if __name__ == "__main__":
    main()
```

**Alternative - With Context:**
```python
from provide.foundation.context import CLIContext
from provide.foundation.hub import Hub

def main():
    """Main CLI entry point with custom context."""

    # Create context with settings
    context = CLIContext(
        log_level="INFO",
        profile="production",
        debug=False,
        no_emoji=False,
    )

    # Create hub with context
    hub = Hub(context=context)

    # Create CLI
    cli = hub.create_cli(
        name="mycli",
        version="1.0.0",
        help="My CLI Application"
    )

    # Run
    cli()

if __name__ == "__main__":
    main()
```

## Testing CLI Commands

Test commands without invoking the full CLI:

```python
import pytest
from click.testing import CliRunner
from provide.foundation import get_hub

def test_status_command():
    """Test the status command."""
    hub = get_hub()
    cli = hub.create_cli(name="test-cli")

    runner = CliRunner()
    result = runner.invoke(cli, ["status"])

    assert result.exit_code == 0
    assert "System Status" in result.output

def test_deploy_command_with_args():
    """Test deploy command with arguments."""
    hub = get_hub()
    cli = hub.create_cli(name="test-cli")

    runner = CliRunner()
    result = runner.invoke(cli, ["deploy", "production", "--force"])

    assert result.exit_code == 0
    assert "production" in result.output
```

## Progress Indicators

Show progress for long-running operations:

```python
import click
from provide.foundation.cli import echo_info

@register_command("install")
def install_command(packages: str):
    """Install packages."""

    package_list = [p.strip() for p in packages.split(",")]

    echo_info(f"Installing {len(package_list)} packages...")

    with click.progressbar(package_list, label="Installing") as bar:
        for package in bar:
            # Simulate installation
            time.sleep(0.5)

    echo_success(f"Installed {len(package_list)} packages")
```

## Best Practices

### âœ… DO: Use Clear Command Names

```python
# âœ… Good: Descriptive command names
@register_command("database.migrate")
@register_command("user.create")

# âŒ Bad: Vague names
@register_command("do-stuff")
@register_command("run")
```

### âœ… DO: Provide Good Help Text

```python
# âœ… Good: Clear docstring and parameter descriptions
@register_command("backup")
def backup_command(source: str, destination: str, compress: bool = False):
    """Backup files from source to destination.

    Creates a backup of all files in the source directory and saves
    them to the destination. Optionally compresses the backup.
    """
    pass

# âŒ Bad: No help text
@register_command("backup")
def backup_command(source: str, destination: str, compress: bool = False):
    pass
```

### âœ… DO: Validate Inputs Early

```python
# âœ… Good: Validate before processing
@register_command("process")
def process_command(file: str):
    """Process a file."""
    if not os.path.exists(file):
        echo_error(f"File not found: {file}")
        raise click.Abort()
    # ... continue processing ...
```

### âœ… DO: Use Structured Logging Internally

```python
# âœ… Good: Use logger for internal logging, echo for user output
from provide.foundation import logger

@register_command("deploy")
def deploy_command(env: str):
    """Deploy application."""
    # Internal logging (for operators/debugging)
    logger.info("deployment_started", environment=env)

    # User output (for CLI users)
    echo_info(f"Deploying to {env}...")

    # ... deploy logic ...

    logger.info("deployment_completed", environment=env)
    echo_success("Deployment complete")
```

### âŒ DON'T: Mix Logging and User Output

```python
# âŒ Bad: Using logger for user feedback
@register_command("status")
def status_command():
    logger.info("System is running")  # User won't see this clearly

# âœ… Good: Use echo functions for users
@register_command("status")
def status_command():
    echo_success("System is running")
```

## Common Patterns

### Pattern: Configuration Command

```python
@register_command("config.show")
def show_config():
    """Show current configuration."""
    from provide.foundation.config import get_config

    config = get_config()
    echo_info("Current Configuration:")
    echo_info("=" * 40)
    for key, value in config.items():
        echo_info(f"{key}: {value}")
```

### Pattern: Version Command

```python
@register_command("version")
def version_command():
    """Show version information."""
    from provide.foundation import __version__

    echo_info(f"mycli version {__version__}")
    echo_info("Foundation version: ...")
```

### Pattern: Dry-Run Mode

```python
@register_command("cleanup")
def cleanup_command(path: str, dry_run: bool = False):
    """Clean up old files."""

    if dry_run:
        echo_warning("DRY RUN MODE - No files will be deleted")

    files_to_delete = find_old_files(path)

    echo_info(f"Found {len(files_to_delete)} files to delete")

    if dry_run:
        for file in files_to_delete:
            echo_info(f"Would delete: {file}")
    else:
        for file in files_to_delete:
            echo_info(f"Deleting: {file}")
            os.remove(file)
        echo_success(f"Deleted {len(files_to_delete)} files")
```

## Next Steps

### Building CLI Features
- **[Argument Parsing](arguments.md)**: Advanced argument handling patterns
- **[Interactive Prompts](prompts.md)**: Building interactive CLIs
- **[First Application](../../getting-started/first-app.md)**: Complete CLI tutorial

### Testing & Production
- **[Testing CLI Commands](../testing/cli-tests.md)**: Write tests for your CLI applications
- **[Production Deployment](../production/deployment.md)**: Deploy CLI tools to production

### Related Guides
- **[Basic Logging](../logging/basic-logging.md)**: Add structured logging to commands
- **[Configuration](../configuration/env-variables.md)**: Configure CLI tools via environment
- **[Error Handling](../resilience/retry.md)**: Add resilience to CLI operations

---

**See Also:** Check `examples/cli/01_cli_application.py` for a comprehensive example.
>>> EOF >>>

### FILE 13: how-to-guides/cli/prompts.md | checksum=8cee26444017... | modified=2025-10-21T17:41:51 | op=+ | size=3753 | tokens=862 | type=markdown ###
<<< BOF <<<
# Interactive Prompts

Learn how to create interactive CLI applications with user prompts and confirmations.

## Overview

Foundation provides utilities for interactive user input, confirmations, and selections through the console module.

## Basic Input

```python
from provide.foundation.console.input import prompt

name = prompt("Enter your name: ")
print(f"Hello, {name}!")
```

## Input with Default

```python
name = prompt("Enter your name: ", default="User")
# If user presses Enter without typing, returns "User"
```

## Yes/No Confirmation

```python
from provide.foundation.console.input import confirm

if confirm("Do you want to continue?"):
    print("Continuing...")
else:
    print("Cancelled")

# With default
if confirm("Delete file?", default=False):
    delete_file()
```

## Password Input

```python
from provide.foundation.console.input import prompt_password

password = prompt_password("Enter password: ")
# Input is hidden while typing
```

## Input Validation

```python
def validate_email(value: str) -> str:
    """Validate email format."""
    if "@" not in value:
        raise ValueError("Invalid email address")
    return value

email = prompt(
    "Enter email: ",
    validator=validate_email
)
```

## Numeric Input

```python
age = prompt("Enter your age: ", type=int)
# Automatically converts to int and re-prompts on invalid input

price = prompt("Enter price: ", type=float)
```

## Choice Selection

```python
from provide.foundation.console.input import select

environment = select(
    "Select environment:",
    choices=["development", "staging", "production"]
)

print(f"Selected: {environment}")
```

## Multi-Line Input

```python
from provide.foundation.console.input import prompt_multiline

description = prompt_multiline(
    "Enter description (Ctrl+D to finish):\n"
)
```

## Interactive Confirmation Workflow

```python
from provide.foundation import pout, perr
from provide.foundation.console.input import confirm, prompt

def interactive_deploy():
    """Interactive deployment workflow."""

    # Get environment
    env = prompt(
        "Environment (dev/prod): ",
        validator=lambda v: v if v in ["dev", "prod"] else None
    )

    # Get version
    version = prompt("Version tag: ")

    # Show summary
    pout("\nDeployment Summary:", bold=True)
    pout(f"  Environment: {env}")
    pout(f"  Version: {version}")
    pout("")

    # Confirm
    if not confirm(f"Deploy version {version} to {env}?", default=False):
        perr("Deployment cancelled", color="yellow")
        return

    # Execute deployment
    pout("Deploying...", color="cyan")
    # ... deployment logic ...
    pout("âœ… Deployment successful!", color="green")
```

## Input with Retry

```python
def get_valid_port():
    """Prompt for port until valid."""
    while True:
        try:
            port = prompt("Enter port (1024-65535): ", type=int)
            if 1024 <= port <= 65535:
                return port
            perr("Port must be between 1024 and 65535")
        except ValueError:
            perr("Invalid port number")

port = get_valid_port()
```

## Styled Prompts

```python
from provide.foundation.console.output import pout

pout("ðŸš€ Welcome to the setup wizard!", color="cyan", bold=True)
pout("")

name = prompt("ðŸ“ Project name: ")
desc = prompt("ðŸ“„ Description: ")
version = prompt("ðŸ”¢ Version: ", default="1.0.0")
```

## Next Steps

- **[Building Commands](commands.md)** - Command structure
- **[Argument Parsing](arguments.md)** - Handle arguments
- **[API Reference: Console](../../reference/provide/foundation/console/index.md)** - Complete console API

---

**See also:** [examples/cli/](https://github.com/provide-io/provide-foundation/tree/main/examples/cli)
>>> EOF >>>

### FILE 14: how-to-guides/configuration/env-variables.md | checksum=9f076d4bc3c2... | modified=2025-10-24T20:28:49 | op=+ | size=17648 | tokens=4125 | type=markdown ###
<<< BOF <<<
# How to Configure with Environment Variables

Foundation provides two complementary APIs for working with environment variables, each designed for different use cases. This guide shows you how to use both effectively.

## Quick Comparison

Foundation provides two complementary APIs for working with environment variables:

### When to Use Each API

| Use Case | API to Use | Why | Example |
|----------|-----------|-----|---------|
| **Simple scripts** | Direct Access (`utils.environment`) | Quick one-off reads, no class overhead | `get_bool("DEBUG")` |
| **Utility scripts** | Direct Access | Minimal boilerplate for throwaway code | `get_int("PORT")` |
| **Application config** | Structured Config (`config.env`) | Type safety, validation, IDE autocomplete | `BaseConfig.from_env()` |
| **Secret management** | Structured Config | Built-in `file://` prefix support | `password: str = env_field(...)` |
| **Complex validation** | Structured Config | Use attrs validators for constraints | Custom validators on fields |
| **Shared configuration** | Structured Config | Pass config objects between modules | Single source of truth |

### Decision Flow

```
Do you need configuration for the entire app?
â”œâ”€ Yes â†’ Use Structured Config (BaseConfig + env_field)
â”‚   â””â”€ Benefits: Type safety, validation, IDE support
â”‚
â””â”€ No â†’ Is this a quick script or one-off read?
    â”œâ”€ Yes â†’ Use Direct Access (get_bool, get_int, etc.)
    â”‚   â””â”€ Benefits: Less boilerplate, faster to write
    â”‚
    â””â”€ No â†’ Use Structured Config anyway for consistency
        â””â”€ Benefits: Easier to refactor later
```

## Direct Environment Variable Access

Use this for simple, one-off environment variable access with automatic type coercion.

### Basic Usage

```python
from provide.foundation.utils.environment import get_bool, get_int, get_str, get_list

# Boolean values
debug = get_bool("DEBUG", default=False)
# Accepts: "true", "1", "yes", "on" (case-insensitive)

# Integer values
port = get_int("PORT", default=8080)

# String values
api_key = get_str("API_KEY", required=True)
# Raises EnvironmentError if not set

# List values (comma-separated)
allowed_hosts = get_list("ALLOWED_HOSTS", default=["localhost"])
# "host1,host2,host3" â†’ ["host1", "host2", "host3"]
```

### Available Functions

#### `get_bool(name, default=None, required=False)`

Parse boolean environment variables:

```python
from provide.foundation.utils.environment import get_bool

# Accepts various formats (case-insensitive):
# True:  "true", "1", "yes", "on", "y", "t"
# False: "false", "0", "no", "off", "n", "f", ""

enable_feature = get_bool("ENABLE_FEATURE", default=False)
debug_mode = get_bool("DEBUG")  # None if not set
strict_mode = get_bool("STRICT_MODE", required=True)  # Error if not set
```

#### `get_int(name, default=None, required=False)`

Parse integer environment variables:

```python
from provide.foundation.utils.environment import get_int

port = get_int("PORT", default=8080)
max_connections = get_int("MAX_CONNECTIONS", default=100)
timeout = get_int("TIMEOUT_SECONDS", required=True)
```

#### `get_float(name, default=None, required=False)`

Parse floating-point environment variables:

```python
from provide.foundation.utils.environment import get_float

timeout = get_float("TIMEOUT", default=30.0)
threshold = get_float("THRESHOLD", default=0.95)
```

#### `get_str(name, default=None, required=False)`

Get string environment variables:

```python
from provide.foundation.utils.environment import get_str

api_key = get_str("API_KEY", required=True)
app_name = get_str("APP_NAME", default="my-app")
database_url = get_str("DATABASE_URL")
```

#### `get_list(name, default=None, separator=",", required=False)`

Parse comma-separated lists:

```python
from provide.foundation.utils.environment import get_list

# Default comma separator
allowed_hosts = get_list("ALLOWED_HOSTS", default=["localhost"])
# "host1,host2,host3" â†’ ["host1", "host2", "host3"]

# Custom separator
paths = get_list("SEARCH_PATHS", separator=":", default=["/usr/bin"])
# "/bin:/usr/bin:/usr/local/bin" â†’ ["/bin", "/usr/bin", "/usr/local/bin"]
```

#### `get_dict(name, default=None, required=False)`

Parse key=value pairs:

```python
from provide.foundation.utils.environment import get_dict

# "key1=value1,key2=value2"
config = get_dict("CONFIG_PARAMS", default={})
# â†’ {"key1": "value1", "key2": "value2"}
```

#### `get_path(name, default=None, required=False)`

Get filesystem paths:

```python
from provide.foundation.utils.environment import get_path

data_dir = get_path("DATA_DIR", default="/var/data")
config_file = get_path("CONFIG_FILE", required=True)
```

#### `require(name)`

Require an environment variable (raises if missing):

```python
from provide.foundation.utils.environment import require

# Shorthand for get_str(name, required=True)
api_key = require("API_KEY")
```

### Error Handling

```python
from provide.foundation.utils.environment import get_str, EnvironmentError

try:
    api_key = get_str("API_KEY", required=True)
except EnvironmentError as e:
    logger.error("missing_required_env_var", var_name="API_KEY")
    raise
```

## Structured Configuration Classes

Use this for building type-safe, validated configuration objects with file-based secret support.

### Understanding field() vs env_field()

Foundation provides two ways to declare fields in configuration classes. Both work identically - choose based on your preference and use case.

#### Quick Decision Guide

```
Are you building user-facing configuration classes?
â”œâ”€ Yes â†’ Use env_field() (clearer intent, less verbose)
â”‚   â””â”€ Example: AppConfig, DatabaseConfig, ServiceConfig
â”‚
â””â”€ No â†’ Are you building internal Foundation-style configs?
    â”œâ”€ Yes â†’ Use field() (matches Foundation's patterns)
    â”‚   â””â”€ Example: Custom LoggingConfig, TelemetryConfig extensions
    â”‚
    â””â”€ Either works â†’ Choose based on code style preference
```

#### The Two Approaches

**1. `env_field()` - Convenience wrapper (recommended for user code):**
```python
from provide.foundation.config import env_field

api_key: str = env_field(env_var="API_KEY")
port: int = env_field(env_var="PORT", default=8080)
```

**2. `field()` - Direct approach (used in Foundation internals):**
```python
from provide.foundation.config.base import field

default_level: str = field(
    default="INFO",
    env_var="PROVIDE_LOG_LEVEL",
    description="Logging level"
)
```

**Key Points:**
- Both work identically - `env_field()` internally calls `field()`
- `env_field()` is more concise for simple use cases
- `field()` supports additional metadata like descriptions
- Foundation's own config classes use `field()` directly
- Use whichever feels more readable for your code

### Basic Example

```python
from provide.foundation.config import BaseConfig, env_field
from attrs import define

@define
class AppConfig(BaseConfig):
    # Required field
    api_key: str = env_field(env_var="API_KEY")

    # Optional with default
    debug: bool = env_field(env_var="DEBUG", default=False)

    # Port with default
    port: int = env_field(env_var="PORT", default=8080)

    # Optional string
    app_name: str = env_field(env_var="APP_NAME", default="my-app")

# Load from environment
config = AppConfig.from_env()

print(config.api_key)  # From API_KEY env var
print(config.port)     # From PORT env var or default 8080
```

### Type Validation

attrs automatically validates types:

```python
@define
class DatabaseConfig(BaseConfig):
    host: str = env_field(env_var="DB_HOST", default="localhost")
    port: int = env_field(env_var="DB_PORT", default=5432)
    ssl_enabled: bool = env_field(env_var="DB_SSL", default=False)
    timeout: float = env_field(env_var="DB_TIMEOUT", default=30.0)

# Load from environment
config = DatabaseConfig.from_env()

# Type errors are caught:
# export DB_PORT="not_a_number"
# config = DatabaseConfig.from_env()  # â† Raises validation error
```

### Secret Management with `file://` Prefix

Foundation supports reading secrets from files using the `file://` prefix:

```python
@define
class SecureConfig(BaseConfig):
    # Can be set directly or via file
    api_key: str = env_field(env_var="API_KEY")

    # Password from file
    database_password: str = env_field(env_var="DB_PASSWORD")

# Set via environment:
# export API_KEY="direct-key-value"
# export DB_PASSWORD="file:///run/secrets/db_password"

config = SecureConfig.from_env()
print(config.api_key)           # "direct-key-value"
print(config.database_password)  # Contents of /run/secrets/db_password
```

This is especially useful for:
- **Docker secrets**: `/run/secrets/secret_name`
- **Kubernetes secrets**: Mounted as files
- **AWS Secrets Manager**: Via file mounts
- **HashiCorp Vault**: Via file-based secret injection

### Complex Configuration

```python
from attrs import define, field
from provide.foundation.config import BaseConfig, env_field

@define
class ServerConfig(BaseConfig):
    # Server settings
    host: str = env_field(env_var="SERVER_HOST", default="0.0.0.0")
    port: int = env_field(env_var="SERVER_PORT", default=8000)
    workers: int = env_field(env_var="SERVER_WORKERS", default=4)

    # TLS settings
    tls_enabled: bool = env_field(env_var="TLS_ENABLED", default=False)
    tls_cert_file: str | None = env_field(env_var="TLS_CERT_FILE", default=None)
    tls_key_file: str | None = env_field(env_var="TLS_KEY_FILE", default=None)

    # Timeouts
    read_timeout: float = env_field(env_var="READ_TIMEOUT", default=30.0)
    write_timeout: float = env_field(env_var="WRITE_TIMEOUT", default=30.0)

    # Additional validation with attrs
    @port.validator
    def _validate_port(self, attribute, value):
        if not (1 <= value <= 65535):
            raise ValueError(f"Port must be between 1 and 65535, got {value}")

    @workers.validator
    def _validate_workers(self, attribute, value):
        if value < 1:
            raise ValueError(f"Workers must be at least 1, got {value}")

config = ServerConfig.from_env()
```

### Nested Configuration

```python
@define
class DatabaseConfig(BaseConfig):
    host: str = env_field(env_var="DB_HOST", default="localhost")
    port: int = env_field(env_var="DB_PORT", default=5432)
    name: str = env_field(env_var="DB_NAME", default="mydb")

@define
class RedisConfig(BaseConfig):
    host: str = env_field(env_var="REDIS_HOST", default="localhost")
    port: int = env_field(env_var="REDIS_PORT", default=6379)

@define
class ApplicationConfig(BaseConfig):
    app_name: str = env_field(env_var="APP_NAME", default="my-app")
    debug: bool = env_field(env_var="DEBUG", default=False)

    # Nested configs - loaded manually
    database: DatabaseConfig = field(factory=lambda: DatabaseConfig.from_env())
    redis: RedisConfig = field(factory=lambda: RedisConfig.from_env())

config = ApplicationConfig.from_env()
print(config.database.host)  # localhost
print(config.redis.port)     # 6379
```

## Common Patterns

### Script Configuration

For simple scripts, use direct access:

```python
#!/usr/bin/env python3
from provide.foundation.utils.environment import get_bool, get_int, get_str
from provide.foundation import logger

# Parse configuration
debug = get_bool("DEBUG", default=False)
batch_size = get_int("BATCH_SIZE", default=100)
data_dir = get_str("DATA_DIR", required=True)

# Configure logging
if debug:
    logger.info("debug_mode_enabled")

# Run script
logger.info("script_started", batch_size=batch_size, data_dir=data_dir)
# ... script logic ...
```

### Application Configuration

For applications, use structured config:

```python
from provide.foundation.config import BaseConfig, env_field
from attrs import define

@define
class AppConfig(BaseConfig):
    # Application settings
    environment: str = env_field(env_var="ENVIRONMENT", default="development")
    debug: bool = env_field(env_var="DEBUG", default=False)
    log_level: str = env_field(env_var="LOG_LEVEL", default="INFO")

    # Database
    database_url: str = env_field(env_var="DATABASE_URL", required=True)

    # API settings
    api_timeout: float = env_field(env_var="API_TIMEOUT", default=30.0)
    api_retries: int = env_field(env_var="API_RETRIES", default=3)

# Initialize application
config = AppConfig.from_env()

# Configure Foundation
from provide.foundation import get_hub, LoggingConfig, TelemetryConfig

telemetry_config = TelemetryConfig(
    service_name="my-app",
    logging=LoggingConfig(
        default_level=config.log_level,
        logger_name_emoji_prefix_enabled=not config.environment == "production",
        das_emoji_prefix_enabled=not config.environment == "production"
    )
)

get_hub().initialize_foundation(telemetry_config)
```

### Production Secrets

```python
@define
class ProductionConfig(BaseConfig):
    # Public configuration
    app_name: str = env_field(env_var="APP_NAME", default="my-app")
    environment: str = env_field(env_var="ENVIRONMENT", default="production")

    # Secrets via files
    database_password: str = env_field(env_var="DB_PASSWORD")
    api_key: str = env_field(env_var="API_KEY")
    jwt_secret: str = env_field(env_var="JWT_SECRET")

# Docker/Kubernetes secrets mounted as files:
# export DB_PASSWORD="file:///run/secrets/db_password"
# export API_KEY="file:///run/secrets/api_key"
# export JWT_SECRET="file:///run/secrets/jwt_secret"

config = ProductionConfig.from_env()
```

### Environment-Specific Defaults

```python
@define
class EnvironmentAwareConfig(BaseConfig):
    environment: str = env_field(env_var="ENVIRONMENT", default="development")

    # Computed defaults based on environment
    @property
    def debug(self) -> bool:
        return self.environment != "production"

    @property
    def log_level(self) -> str:
        return "DEBUG" if self.debug else "INFO"

    @property
    def use_emoji(self) -> bool:
        return self.environment == "development"

from provide.foundation import get_hub, LoggingConfig, TelemetryConfig

config = EnvironmentAwareConfig.from_env()

# Configure based on environment
telemetry_config = TelemetryConfig(
    logging=LoggingConfig(
        default_level=config.log_level,
        logger_name_emoji_prefix_enabled=config.use_emoji,
        das_emoji_prefix_enabled=config.use_emoji
    )
)

get_hub().initialize_foundation(telemetry_config)
```

## Testing with Environment Variables

### Direct Access Testing

```python
import os
import pytest
from provide.foundation.utils.environment import get_bool, get_str

def test_environment_parsing(monkeypatch):
    # Set test environment variables
    monkeypatch.setenv("DEBUG", "true")
    monkeypatch.setenv("API_KEY", "test-key")

    assert get_bool("DEBUG") is True
    assert get_str("API_KEY") == "test-key"

def test_missing_required_var():
    from provide.foundation.utils.environment import get_str, EnvironmentError

    with pytest.raises(EnvironmentError):
        get_str("MISSING_VAR", required=True)
```

### Structured Config Testing

```python
import pytest
from provide.foundation.config import BaseConfig, env_field
from attrs import define

@define
class TestConfig(BaseConfig):
    api_key: str = env_field(env_var="API_KEY")
    debug: bool = env_field(env_var="DEBUG", default=False)

def test_config_loading(monkeypatch):
    monkeypatch.setenv("API_KEY", "test-key")
    monkeypatch.setenv("DEBUG", "true")

    config = TestConfig.from_env()

    assert config.api_key == "test-key"
    assert config.debug is True
```

## Best Practices

### âœ… DO: Use Descriptive Environment Variable Names

```python
# âœ… Good: Clear, descriptive names
DATABASE_URL = get_str("DATABASE_URL")
MAX_CONNECTIONS = get_int("MAX_CONNECTIONS")
ENABLE_FEATURE_X = get_bool("ENABLE_FEATURE_X")

# âŒ Bad: Ambiguous names
URL = get_str("URL")
MAX = get_int("MAX")
FEATURE = get_bool("FEATURE")
```

### âœ… DO: Use Structured Config for Applications

```python
# âœ… Good: Type-safe, validated configuration
@define
class AppConfig(BaseConfig):
    database_url: str = env_field(env_var="DATABASE_URL", required=True)
    port: int = env_field(env_var="PORT", default=8080)

# âŒ Bad: Direct access everywhere
database_url = get_str("DATABASE_URL", required=True)
port = get_int("PORT", default=8080)
```

### âœ… DO: Validate Configuration Early

```python
# âœ… Good: Load and validate config at startup
config = AppConfig.from_env()  # Validates immediately

# âŒ Bad: Lazy loading in critical paths
def handle_request():
    port = get_int("PORT")  # May fail during request handling
```

### âœ… DO: Use `file://` for Secrets

```python
# âœ… Good: Secrets from files
password: str = env_field(env_var="DB_PASSWORD")
# export DB_PASSWORD="file:///run/secrets/db_password"

# âŒ Bad: Secrets in environment directly (visible in process list)
# export DB_PASSWORD="my-secret-password"
```

### âŒ DON'T: Mix Configuration Methods

```python
# âŒ Bad: Mixing both approaches inconsistently
debug = get_bool("DEBUG")  # Direct access
config = AppConfig.from_env()  # Structured config

# âœ… Good: Choose one approach and stick with it
config = AppConfig.from_env()
debug = config.debug
```

## Next Steps

- **[File-Based Config](file-config.md)**: Load configuration from YAML/TOML files
- **[Secret Management](secrets.md)**: Advanced secret handling patterns
- **[Architecture](../../explanation/architecture.md)**: Understand the configuration system

---

**Tip**: Start with direct access for scripts, but migrate to structured config as your application grows. The type safety and validation are worth the small upfront cost.
>>> EOF >>>

### FILE 15: how-to-guides/configuration/file-config.md | checksum=c2526201b597... | modified=2025-10-21T17:40:44 | op=+ | size=2486 | tokens=603 | type=markdown ###
<<< BOF <<<
# File-Based Configuration

Learn how to load configuration from YAML, JSON, and TOML files.

## Overview

Foundation supports loading configuration from files, making it easy to manage complex settings and environment-specific configurations.

## YAML Configuration

```yaml
# config.yaml
service:
  name: my-app
  version: 1.0.0

logging:
  level: INFO
  format: json

database:
  host: localhost
  port: 5432
  pool_size: 10
```

```python
from provide.foundation.config import ConfigManager

manager = ConfigManager()
config = await manager.load_yaml("config.yaml")

print(config["service"]["name"])  # "my-app"
```

## JSON Configuration

```json
{
  "service": {
    "name": "my-app",
    "version": "1.0.0"
  },
  "logging": {
    "level": "INFO",
    "format": "json"
  }
}
```

```python
config = await manager.load_json("config.json")
```

## TOML Configuration

```toml
[service]
name = "my-app"
version = "1.0.0"

[logging]
level = "INFO"
format = "json"

[database]
host = "localhost"
port = 5432
pool_size = 10
```

```python
config = await manager.load_toml("config.toml")
```

## Environment-Specific Files

```python
import os

environment = os.getenv("ENVIRONMENT", "development")
config_file = f"config.{environment}.yaml"

config = await manager.load_yaml(config_file)
```

## Merging Configurations

Combine multiple sources with environment variables taking precedence:

```python
# Load base config
base_config = await manager.load_yaml("config.base.yaml")

# Load environment-specific overrides
env_config = await manager.load_yaml(f"config.{environment}.yaml")

# Merge (env_config overrides base_config)
final_config = {**base_config, **env_config}
```

## Type-Safe Configuration

Use attrs classes for type safety:

```python
from attrs import define
from provide.foundation.config import BaseConfig, env_field

@define
class ServiceConfig(BaseConfig):
    name: str
    version: str
    port: int = 8000

# Load and validate
data = await manager.load_yaml("config.yaml")
service_config = ServiceConfig(**data["service"])
```

## Next Steps

- **[Environment Variables](env-variables.md)** - Environment-based config
- **[Secret Management](secrets.md)** - Secure configuration
- **[API Reference: Config](../../reference/provide/foundation/config/index.md)** - Complete config API

---

**See also:** [examples/configuration/03_config_management.py](https://github.com/provide-io/provide-foundation/blob/main/examples/configuration/03_config_management.py)
>>> EOF >>>

### FILE 16: how-to-guides/configuration/secrets.md | checksum=ab04ef28cddf... | modified=2025-10-21T17:40:44 | op=+ | size=3938 | tokens=931 | type=markdown ###
<<< BOF <<<
# Secret Management

Learn how to securely handle secrets and sensitive configuration data.

## Overview

Foundation provides secure patterns for handling secrets, including file-based secrets for container platforms and environment variable protection.

## File-Based Secrets

Read secrets from files (Kubernetes-style secrets):

```python
from provide.foundation.config import BaseConfig, env_field
from attrs import define

@define
class DatabaseConfig(BaseConfig):
    host: str = env_field(env_var="DB_HOST", default="localhost")
    port: int = env_field(env_var="DB_PORT", default=5432)

    # Will read from file if value starts with "file://"
    password: str = env_field(env_var="DB_PASSWORD")

# Set environment variable to point to secret file
# export DB_PASSWORD="file:///run/secrets/db_password"

config = DatabaseConfig.from_env()
# config.password contains the contents of /run/secrets/db_password
```

## Kubernetes Secrets Example

```yaml
# kubernetes-deployment.yaml
apiVersion: v1
kind: Pod
spec:
  containers:
    - name: my-app
      env:
        - name: DB_PASSWORD
          value: "file:///run/secrets/db_password"
      volumeMounts:
        - name: db-secret
          mountPath: "/run/secrets"
          readOnly: true
  volumes:
    - name: db-secret
      secret:
        secretName: database-credentials
```

## Environment Variable Best Practices

**âŒ Don't:**
```python
# Never hardcode secrets
API_KEY = "sk_live_abc123..."

# Never log secrets
logger.info("api_call", api_key=api_key)  # BAD!
```

**âœ… Do:**
```python
# Load from environment
from provide.foundation.utils.environment import get_str

api_key = get_str("API_KEY", required=True)

# Sanitize in logs
logger.info("api_call", api_key_prefix=api_key[:7] + "...")
```

## Automatic Secret Sanitization

Foundation's log processors can automatically sanitize sensitive data:

```python
from provide.foundation import logger

# These will be automatically sanitized
logger.info(
    "user_update",
    password="secret123",      # Will be redacted
    api_token="sk_abc...",     # Will be redacted
    safe_field="public_data"   # Will be logged
)
# Output: password='***REDACTED***' api_token='***REDACTED***'
```

## Separate Secrets from Config

Keep secrets in a separate file:

```yaml
# config.yaml (version controlled)
service:
  name: my-app
  port: 8000
  database:
    host: localhost
    # Password loaded separately
```

```yaml
# secrets.yaml (NOT in version control)
database:
  password: super_secret_password
api_keys:
  stripe: sk_live_...
  sendgrid: SG....
```

```python
# Load both
config = await manager.load_yaml("config.yaml")
secrets = await manager.load_yaml("secrets.yaml")

# Merge
config["database"]["password"] = secrets["database"]["password"]
```

## AWS Secrets Manager Integration

```python
import boto3
import json

def load_aws_secret(secret_name: str) -> dict:
    """Load secret from AWS Secrets Manager."""
    client = boto3.client('secretsmanager')
    response = client.get_secret_value(SecretId=secret_name)
    return json.loads(response['SecretString'])

# Use in configuration
secrets = load_aws_secret("prod/database")

@define
class Config(BaseConfig):
    db_password: str = secrets["password"]
```

## Vault Integration

```python
import hvac

def load_vault_secret(path: str, key: str) -> str:
    """Load secret from HashiCorp Vault."""
    client = hvac.Client(url='https://vault.example.com')
    client.token = os.getenv('VAULT_TOKEN')

    secret = client.secrets.kv.v2.read_secret_version(path=path)
    return secret['data']['data'][key]
```

## Next Steps

- **[Environment Variables](env-variables.md)** - Basic configuration
- **[File-Based Config](file-config.md)** - Loading from files
- **[Production Deployment](../production/deployment.md)** - Production patterns

---

**Security Tip:** Never commit secrets to version control. Use `.gitignore` to exclude secret files.
>>> EOF >>>

### FILE 17: how-to-guides/console/console-io.md | checksum=bf0503d850d4... | modified=2025-10-24T20:11:29 | op=+ | size=9269 | tokens=2304 | type=markdown ###
<<< BOF <<<
# How to Use Console I/O

Foundation provides a suite of console I/O utilities for building robust CLI applications with proper separation between logging and user-facing output.

## Overview

The console module provides three main categories of functions:

- **Output**: `pout()` and `perr()` for user-facing messages
- **Input**: `pin()` and variants for user input
- **Async I/O**: Async versions of all I/O functions

## User-Facing Output

### pout() - Standard Output

Use `pout()` for success messages and general user output:

```python
from provide.foundation import pout

# Simple message
pout("Operation completed successfully")

# With color
pout("âœ… File saved", color="green")

# With bold formatting
pout("Important Notice", bold=True, color="yellow")

# With dim text
pout("(additional details)", dim=True)

# Disable newline
pout("Processing... ", nl=False)
pout("Done!", color="green")
```

**Available parameters:**
- `message`: Content to output (any type)
- `color`: Color name (red, green, yellow, blue, cyan, magenta, white)
- `bold`: Bold text (default: False)
- `dim`: Dim/faded text (default: False)
- `nl` or `newline`: Add newline (default: True)
- `prefix`: Optional prefix string
- `json_key`: Key for JSON output mode
- `ctx`: Override context

### perr() - Error Output

Use `perr()` for error messages and warnings (writes to stderr):

```python
from provide.foundation import perr

# Error message
perr("âŒ Operation failed", color="red")

# Warning message
perr("âš ï¸  Configuration file not found", color="yellow")

# Critical error
perr("CRITICAL: System failure detected", color="red", bold=True)
```

### Separation of Concerns

**Best Practice:** Keep logging and user output separate:

```python
from provide.foundation import logger, pout, perr

def process_file(filename: str):
    # Log for operators/debugging
    logger.info("file_processing_started", filename=filename)

    # Show to user
    pout(f"Processing {filename}...", color="cyan")

    try:
        result = do_work(filename)

        # Log result
        logger.info("file_processed", filename=filename, records=len(result))

        # Show to user
        pout(f"âœ… Processed {len(result)} records", color="green")

    except Exception as e:
        # Log error with context
        logger.error("file_processing_failed", filename=filename, error=str(e))

        # Show to user
        perr(f"âŒ Failed to process {filename}: {e}", color="red")
        raise
```

## User Input

### pin() - Basic Input

Get user input with optional type conversion and validation:

```python
from provide.foundation import pin

# Simple string input
name = pin("Enter your name: ")

# Integer input with type conversion
age = pin("Enter your age: ", type=int)

# With default value
city = pin("Enter city: ", default="San Francisco")

# Boolean input
confirmed = pin("Proceed? (y/n): ", type=bool)
```

### Password Input

Hide sensitive input:

```python
from provide.foundation import pin

# Hidden input
password = pin("Enter password: ", password=True)

# With confirmation
password = pin(
    "Enter password: ",
    password=True,
    confirmation_prompt=True
)
```

### Input with Validation

```python
from provide.foundation import pin

def validate_email(value):
    """Validate email format."""
    if "@" not in value:
        raise ValueError("Invalid email address")
    return value.lower()

email = pin(
    "Enter email: ",
    value_proc=validate_email
)
```

### pin_stream() - Stream Input

Read multiple lines from stdin:

```python
from provide.foundation import pin_stream

pout("Enter text (Ctrl+D to finish):")

for line in pin_stream():
    process_line(line)
```

### pin_lines() - Read Multiple Lines

```python
from provide.foundation import pin_lines

# Read exactly 3 lines
lines = pin_lines(count=3)

# Read until EOF
all_lines = pin_lines()
```

## Async I/O

All console I/O functions have async equivalents:

```python
from provide.foundation.console import apin, apin_lines, apin_stream
import asyncio

async def get_user_info():
    # Async input
    name = await apin("Enter name: ")
    age = await apin("Enter age: ", type=int)

    # Async multi-line input
    lines = await apin_lines(count=3)

    # Async streaming
    async for line in apin_stream():
        await process_line(line)

asyncio.run(get_user_info())
```

## JSON Output Mode

Foundation automatically detects JSON mode for machine-readable output:

```python
from provide.foundation import pout

# Automatically outputs JSON when in JSON mode
pout({"status": "success", "records": 42}, json_key="result")

# In JSON mode outputs:
# {"result": {"status": "success", "records": 42}}

# In normal mode outputs:
# {'status': 'success', 'records': 42}
```

**Enable JSON mode:**
```python
from provide.foundation.context import CLIContext
from provide.foundation import pout

ctx = CLIContext(json_output=True)
pout("Success", json_key="message", ctx=ctx)
# {"message": "Success"}
```

## Color Support

Foundation automatically detects color support:

### Environment Variables

```bash
# Force color output
export FORCE_COLOR=1

# Disable color output
export NO_COLOR=1
```

### Manual Control

```python
from provide.foundation.context import CLIContext
from provide.foundation import pout

# Disable colors
ctx = CLIContext(no_color=True)
pout("No colors", color="red", ctx=ctx)  # Plain text
```

## Best Practices

### âœ… DO: Use pout/perr for User Output

```python
# âœ… Good: User-facing output via pout/perr
from provide.foundation import pout, perr, logger

pout("âœ… Deployment successful", color="green")
logger.info("deployment_completed", duration_ms=1234)

# âŒ Bad: Using print() or logger for user output
print("Deployment successful")  # Wrong: bypasses Foundation's I/O
logger.info("Deployment successful")  # Wrong: logging is not for users
```

### âœ… DO: Use Colors Consistently

```python
# âœ… Good: Consistent color scheme
pout("âœ… Success", color="green")
perr("âŒ Error", color="red")
perr("âš ï¸  Warning", color="yellow")
pout("â„¹ï¸  Info", color="cyan")

# âŒ Bad: Inconsistent colors
pout("âœ… Success", color="red")  # Confusing
perr("Error", color="green")  # Very confusing
```

### âœ… DO: Add Emojis for Visual Clarity

```python
# âœ… Good: Clear visual indicators
pout("âœ… File saved successfully", color="green")
perr("âŒ Connection failed", color="red")
perr("âš ï¸  Deprecated feature", color="yellow")
pout("ðŸ“ Opening file...", color="cyan")
pout("ðŸ”„ Syncing data...", color="blue")
```

### âŒ DON'T: Mix Output Methods

```python
# âŒ Bad: Mixing print() with pout()
pout("Starting process...")
print("Step 1 complete")  # Inconsistent
pout("Process complete")

# âœ… Good: Consistent use of pout()
pout("Starting process...")
pout("Step 1 complete")
pout("Process complete", color="green")
```

## Common Patterns

### Progress Indicator

```python
from provide.foundation import pout
import time

pout("Processing files: ", nl=False)
for i in range(5):
    pout(".", nl=False)
    time.sleep(0.5)
pout(" Done!", color="green")
```

### Interactive Confirmation

```python
from provide.foundation import pin, pout, perr

def confirm_action(message: str) -> bool:
    """Ask user to confirm an action."""
    response = pin(f"{message} (y/n): ").lower()
    return response in ("y", "yes")

if confirm_action("Delete all files?"):
    pout("âœ… Confirmed, proceeding...", color="yellow")
else:
    perr("âŒ Cancelled", color="red")
```

### Multi-Step Input

```python
from provide.foundation import pin, pout

pout("ðŸ“ User Registration", bold=True)
pout()  # Empty line

name = pin("Name: ")
email = pin("Email: ")
age = pin("Age: ", type=int)
password = pin("Password: ", password=True)

pout()
pout(f"âœ… User {name} registered successfully!", color="green")
```

### Error Handling with Retry

```python
from provide.foundation import pin, perr

def get_valid_number(prompt: str, max_attempts: int = 3) -> int:
    """Get valid integer input with retry."""
    for attempt in range(max_attempts):
        try:
            return pin(prompt, type=int)
        except ValueError:
            perr(f"âŒ Invalid number. {max_attempts - attempt - 1} attempts remaining.", color="red")

    raise ValueError("Max attempts exceeded")

age = get_valid_number("Enter your age: ")
```

## Integration with Click

Foundation's console I/O works seamlessly with Click:

```python
import click
from provide.foundation import pout, perr, pin

@click.command()
@click.option("--name", prompt="Your name", help="User name")
@click.option("--confirm", is_flag=True, help="Skip confirmation")
def greet(name: str, confirm: bool):
    """Greet the user."""
    if not confirm:
        if not pin(f"Greet {name}? (y/n): ").lower().startswith("y"):
            perr("Cancelled", color="red")
            return

    pout(f"Hello, {name}!", color="green", bold=True)
```

## Next Steps

- **[CLI Commands](../cli/commands.md)**: Build CLI applications
- **[Logging](../logging/basic-logging.md)**: Use logging for debugging
- **[Architecture](../../explanation/architecture.md)**: Understand Foundation's design

---

**Tip**: Always use `pout()`/`perr()` for user-facing output and `logger` for system logging.
>>> EOF >>>

### FILE 18: how-to-guides/crypto/certificates.md | checksum=f3ce29a8f0f4... | modified=2025-10-24T19:28:42 | op=+ | size=16378 | tokens=3720 | type=markdown ###
<<< BOF <<<
# X.509 Certificates

Learn how to generate and manage X.509 certificates for secure communication.

## Overview

X.509 certificates are digital documents that bind a public key to an identity. Foundation provides utilities for creating self-signed certificates, certificate signing requests (CSRs), and managing certificate chains.

**Common use cases:**
- Development TLS/SSL certificates
- Internal service authentication
- Client certificates for mutual TLS
- Code signing certificates

## Prerequisites

Install crypto extras:
```bash
pip install "provide-foundation[crypto]"
```

## Generate Self-Signed Certificate

Create a self-signed certificate for development or testing:

```python
from provide.foundation.crypto.certificates import generate_self_signed_cert
from pathlib import Path

# Generate certificate
cert_pem, private_key_pem = generate_self_signed_cert(
    common_name="example.com",
    organization="My Company",
    validity_days=365
)

# Save to files
Path("cert.pem").write_text(cert_pem)
Path("key.pem").write_text(private_key_pem)
```

**Output:**
- `cert.pem`: Public certificate in PEM format
- `key.pem`: Private key in PEM format

## Certificate with Subject Alternative Names (SAN)

Create certificates valid for multiple domains:

```python
from provide.foundation.crypto.certificates import generate_self_signed_cert

# Certificate valid for multiple domains
cert_pem, key_pem = generate_self_signed_cert(
    common_name="api.example.com",
    organization="Example Corp",
    subject_alt_names=[
        "api.example.com",
        "www.api.example.com",
        "*.api.example.com",  # Wildcard
        "192.168.1.100",      # IP address
    ],
    validity_days=365
)
```

**When to use SAN:**
- Multiple subdomains on same certificate
- Load balancers with multiple backends
- Development environments with various hostnames
- Microservices with service discovery

## Certificate Configuration

### Organizational Details

Provide complete organizational information:

```python
cert_pem, key_pem = generate_self_signed_cert(
    common_name="services.mycompany.com",
    organization="My Company Inc",
    organizational_unit="Engineering",
    country="US",
    state="California",
    locality="San Francisco",
    email="admin@mycompany.com",
    validity_days=730,  # 2 years
)
```

### Key Size and Algorithm

Specify cryptographic parameters:

```python
from provide.foundation.crypto.certificates import generate_self_signed_cert

# RSA 4096-bit key
cert_pem, key_pem = generate_self_signed_cert(
    common_name="secure.example.com",
    key_size=4096,  # Default is 2048
    algorithm="RSA",
    validity_days=365,
)

# ED25519 (faster, smaller keys)
cert_pem, key_pem = generate_self_signed_cert(
    common_name="fast.example.com",
    algorithm="ED25519",
    validity_days=365,
)
```

**Algorithm comparison:**

| Algorithm | Key Size | Speed | Security | Use Case |
|-----------|----------|-------|----------|----------|
| RSA-2048  | 2048 bits | Medium | High | Standard web servers |
| RSA-4096  | 4096 bits | Slow | Very High | High-security applications |
| ED25519   | 256 bits | Very Fast | High | Modern applications, IoT |

## Certificate Signing Request (CSR)

Generate a CSR for submission to a Certificate Authority:

```python
from provide.foundation.crypto.certificates import generate_csr

# Generate private key and CSR
csr_pem, private_key_pem = generate_csr(
    common_name="www.example.com",
    organization="Example Inc",
    country="US",
    state="California",
    locality="San Francisco",
    email="admin@example.com",
)

# Save CSR for submission to CA
Path("request.csr").write_text(csr_pem)
Path("private.key").write_text(private_key_pem)
```

**Next steps with CSR:**
1. Submit CSR to Certificate Authority (Let's Encrypt, DigiCert, etc.)
2. Complete domain validation
3. Receive signed certificate from CA
4. Use signed certificate with your private key

## Certificate Chain

Work with certificate chains (certificate + intermediate + root):

```python
from provide.foundation.crypto.certificates import load_certificate_chain

# Load certificate chain
chain = load_certificate_chain("fullchain.pem")

print(f"Chain contains {len(chain)} certificates")
for i, cert in enumerate(chain):
    print(f"Certificate {i}: {cert.subject}")
    print(f"  Issuer: {cert.issuer}")
    print(f"  Valid until: {cert.not_valid_after}")
```

### Create Certificate Chain

Combine certificates into a chain:

```python
from pathlib import Path

# Read individual certificates
server_cert = Path("server.crt").read_text()
intermediate_cert = Path("intermediate.crt").read_text()
root_cert = Path("root.crt").read_text()

# Create full chain
full_chain = server_cert + intermediate_cert + root_cert

# Save full chain
Path("fullchain.pem").write_text(full_chain)
```

## Certificate Verification

Verify certificate validity and properties:

```python
from provide.foundation.crypto.certificates import verify_certificate
from datetime import datetime

# Load and verify certificate
cert = load_certificate("cert.pem")

# Check expiration
if cert.not_valid_after < datetime.now():
    print("âš ï¸ Certificate has expired!")
else:
    days_remaining = (cert.not_valid_after - datetime.now()).days
    print(f"âœ… Certificate valid for {days_remaining} more days")

# Verify hostname
if verify_hostname(cert, "example.com"):
    print("âœ… Certificate valid for example.com")
else:
    print("âŒ Certificate not valid for this hostname")
```

### Extract Certificate Information

Get certificate details programmatically:

```python
from provide.foundation.crypto.certificates import get_certificate_info

# Load certificate
info = get_certificate_info("cert.pem")

print(f"Subject: {info['subject']}")
print(f"Issuer: {info['issuer']}")
print(f"Valid from: {info['not_before']}")
print(f"Valid until: {info['not_after']}")
print(f"Serial number: {info['serial_number']}")
print(f"Key algorithm: {info['key_algorithm']}")
print(f"Key size: {info['key_size']} bits")
print(f"SAN: {info['subject_alt_names']}")
```

## Common Patterns

### Development TLS Server

Create certificates for local HTTPS development:

```python
from provide.foundation.crypto.certificates import generate_self_signed_cert
from pathlib import Path

def setup_dev_tls():
    """Set up TLS certificates for local development."""
    cert_dir = Path("certs")
    cert_dir.mkdir(exist_ok=True)

    # Generate cert for localhost
    cert_pem, key_pem = generate_self_signed_cert(
        common_name="localhost",
        organization="Development",
        subject_alt_names=[
            "localhost",
            "127.0.0.1",
            "::1",
            "*.localhost",  # For subdomains
        ],
        validity_days=365,
    )

    # Save certificates
    cert_file = cert_dir / "localhost.crt"
    key_file = cert_dir / "localhost.key"

    cert_file.write_text(cert_pem)
    key_file.write_text(key_pem)

    print(f"âœ… Development certificates created:")
    print(f"   Certificate: {cert_file}")
    print(f"   Private key: {key_file}")

    return cert_file, key_file

# Use with web server
cert_file, key_file = setup_dev_tls()

# Example with uvicorn (FastAPI)
# uvicorn main:app --ssl-keyfile=certs/localhost.key --ssl-certfile=certs/localhost.crt
```

### Client Certificate Authentication

Generate client certificates for mutual TLS:

```python
from provide.foundation.crypto.certificates import generate_client_cert

def create_client_cert(client_name):
    """Create client certificate for mutual TLS."""
    cert_pem, key_pem = generate_client_cert(
        common_name=client_name,
        organization="Client Services",
        email=f"{client_name}@example.com",
        validity_days=365,
    )

    # Save client credentials
    Path(f"{client_name}.crt").write_text(cert_pem)
    Path(f"{client_name}.key").write_text(key_pem)

    return cert_pem, key_pem

# Create certificates for different clients
create_client_cert("service-a")
create_client_cert("service-b")
create_client_cert("mobile-app")
```

### Certificate Rotation

Automate certificate renewal:

```python
from provide.foundation.crypto.certificates import (
    generate_self_signed_cert,
    load_certificate,
)
from datetime import datetime, timedelta
from pathlib import Path

def rotate_certificate_if_needed(cert_path, key_path, days_before_expiry=30):
    """Rotate certificate if it's expiring soon."""
    try:
        cert = load_certificate(cert_path)
        days_remaining = (cert.not_valid_after - datetime.now()).days

        if days_remaining > days_before_expiry:
            print(f"âœ… Certificate valid for {days_remaining} days")
            return False

        print(f"âš ï¸ Certificate expiring in {days_remaining} days, rotating...")

    except FileNotFoundError:
        print("âš ï¸ Certificate not found, generating new one...")

    # Generate new certificate
    cert_pem, key_pem = generate_self_signed_cert(
        common_name="example.com",
        organization="Example Inc",
        validity_days=365,
    )

    # Save new certificate
    Path(cert_path).write_text(cert_pem)
    Path(key_path).write_text(key_pem)

    print("âœ… Certificate rotated successfully")
    return True

# Check and rotate if needed
rotate_certificate_if_needed("server.crt", "server.key")
```

### Certificate for Service Mesh

Generate certificates for microservices:

```python
from provide.foundation.crypto.certificates import generate_service_cert

def setup_service_mesh_certs(service_name, namespace="default"):
    """Generate certificates for Kubernetes service mesh."""
    # Generate certificate with proper SAN for k8s DNS
    cert_pem, key_pem = generate_service_cert(
        common_name=f"{service_name}.{namespace}.svc.cluster.local",
        organization="Service Mesh",
        subject_alt_names=[
            f"{service_name}",
            f"{service_name}.{namespace}",
            f"{service_name}.{namespace}.svc",
            f"{service_name}.{namespace}.svc.cluster.local",
        ],
        validity_days=90,  # Shorter validity for security
    )

    # Save as Kubernetes secret format
    import base64
    secret_manifest = f"""
apiVersion: v1
kind: Secret
metadata:
  name: {service_name}-tls
  namespace: {namespace}
type: kubernetes.io/tls
data:
  tls.crt: {base64.b64encode(cert_pem.encode()).decode()}
  tls.key: {base64.b64encode(key_pem.encode()).decode()}
"""

    Path(f"{service_name}-secret.yaml").write_text(secret_manifest)
    print(f"âœ… Generated certificate secret for {service_name}")

# Generate certs for services
setup_service_mesh_certs("user-service")
setup_service_mesh_certs("payment-service")
setup_service_mesh_certs("inventory-service")
```

## Converting Certificate Formats

### PEM to DER

Convert from PEM (text) to DER (binary):

```python
from provide.foundation.crypto.certificates import pem_to_der
from pathlib import Path

# Load PEM certificate
pem_cert = Path("cert.pem").read_text()

# Convert to DER
der_cert = pem_to_der(pem_cert)

# Save DER certificate
Path("cert.der").write_bytes(der_cert)
```

### Create PKCS#12 Bundle

Create a PKCS#12 (.p12/.pfx) file with certificate and private key:

```python
from provide.foundation.crypto.certificates import create_pkcs12

# Create PKCS#12 bundle
p12_data = create_pkcs12(
    certificate_pem=cert_pem,
    private_key_pem=key_pem,
    passphrase="secret-password",
    friendly_name="My Certificate",
)

# Save to file
Path("certificate.p12").write_bytes(p12_data)
```

## Best Practices

### âœ… DO: Use Appropriate Validity Periods

```python
# âœ… Good: Reasonable validity periods
# Development
cert = generate_self_signed_cert(
    common_name="dev.local",
    validity_days=90,  # 3 months for dev
)

# Production (with proper CA)
cert = generate_self_signed_cert(
    common_name="prod.example.com",
    validity_days=365,  # 1 year max
)

# âŒ Bad: Too long validity
cert = generate_self_signed_cert(
    common_name="example.com",
    validity_days=3650,  # 10 years - security risk!
)
```

### âœ… DO: Protect Private Keys

```python
# âœ… Good: Secure file permissions
import os
from pathlib import Path

key_file = Path("private.key")
key_file.write_text(private_key_pem)
os.chmod(key_file, 0o600)  # Read/write for owner only

# âœ… Good: Never log private keys
logger.info("Certificate generated", cert_path=cert_path)  # OK
# âŒ Never do this:
# logger.info("Key generated", key=private_key_pem)  # NEVER!
```

### âœ… DO: Use SAN for Multiple Hostnames

```python
# âœ… Good: Proper SAN usage
cert = generate_self_signed_cert(
    common_name="api.example.com",
    subject_alt_names=[
        "api.example.com",
        "www.api.example.com",
        "api-staging.example.com",
    ],
)

# âŒ Bad: Creating separate certs for each hostname
# More certs = more to manage and rotate
```

### âœ… DO: Monitor Certificate Expiration

```python
# âœ… Good: Automated monitoring
from datetime import datetime, timedelta

def check_certificate_expiration(cert_path, warn_days=30):
    """Monitor certificate expiration."""
    cert = load_certificate(cert_path)
    expires = cert.not_valid_after
    days_remaining = (expires - datetime.now()).days

    if days_remaining < 0:
        logger.critical("Certificate expired", cert_path=cert_path)
    elif days_remaining < warn_days:
        logger.warning(
            "Certificate expiring soon",
            cert_path=cert_path,
            days_remaining=days_remaining,
        )
    else:
        logger.info(
            "Certificate valid",
            cert_path=cert_path,
            days_remaining=days_remaining,
        )

    return days_remaining
```

### âŒ DON'T: Use Self-Signed Certs in Production

```python
# âŒ Bad: Self-signed in production
cert = generate_self_signed_cert(
    common_name="production.example.com",  # Don't!
)

# âœ… Good: Use proper CA for production
# - Let's Encrypt (free, automated)
# - DigiCert, GlobalSign, etc. (commercial)
# - Internal CA for private services
```

### âŒ DON'T: Commit Private Keys to Version Control

```python
# âœ… Good: .gitignore
"""
*.key
*.pem
*.p12
*.pfx
certs/
"""

# âŒ Bad: Committing keys
# git add private.key  # NEVER!
```

## Certificate Management Tools

### List Certificates

Get information about multiple certificates:

```python
from provide.foundation.crypto.certificates import list_certificates
from pathlib import Path

def audit_certificates(cert_dir):
    """Audit all certificates in directory."""
    cert_dir = Path(cert_dir)

    for cert_file in cert_dir.glob("*.crt"):
        info = get_certificate_info(cert_file)

        print(f"\nðŸ“„ {cert_file.name}")
        print(f"   Subject: {info['subject']}")
        print(f"   Expires: {info['not_after']}")

        # Check expiration
        days = (info['not_after'] - datetime.now()).days
        if days < 30:
            print(f"   âš ï¸ Expiring in {days} days!")
        else:
            print(f"   âœ… Valid for {days} days")

audit_certificates("certs/")
```

### Validate Certificate Chain

Verify a certificate chain is valid:

```python
from provide.foundation.crypto.certificates import validate_chain

def verify_cert_chain(server_cert, intermediate_cert, root_cert):
    """Verify certificate chain is valid."""
    try:
        is_valid = validate_chain(
            server_cert=server_cert,
            intermediate_cert=intermediate_cert,
            root_cert=root_cert,
        )

        if is_valid:
            print("âœ… Certificate chain is valid")
        else:
            print("âŒ Certificate chain is invalid")

        return is_valid
    except Exception as e:
        logger.exception("Chain validation failed")
        return False
```

## Next Steps

### Related Guides
- **[Key Generation](keys.md)**: Generate cryptographic keys
- **[Signing & Verification](signing.md)**: Sign and verify data

### Examples
- See `examples/crypto/` for certificate examples
- See `examples/production/` for TLS configuration patterns

### API Reference
- **[API Reference: Crypto](../../reference/provide/foundation/crypto/index.md)**: Complete API documentation

---

**Tip**: For development, self-signed certificates are fine. For production, always use certificates from a trusted CA like Let's Encrypt (free and automated) or a commercial provider.
>>> EOF >>>

### FILE 19: how-to-guides/crypto/keys.md | checksum=036289f5822f... | modified=2025-10-24T19:31:48 | op=+ | size=24592 | tokens=5535 | type=markdown ###
<<< BOF <<<
# Key Generation & Management

Learn how to generate, store, and manage cryptographic keys with Foundation's secure key management utilities.

## Overview

Foundation provides comprehensive utilities for generating and managing cryptographic keys with secure defaults. The crypto module supports modern elliptic curve cryptography (Ed25519, ECDSA) and traditional RSA keys, with built-in security best practices.

**What you'll learn:**
- Generate Ed25519, RSA, and ECDSA keypairs
- Store keys securely with proper file permissions
- Convert between key formats (PEM, DER, raw bytes)
- Implement key rotation strategies
- Use environment-based key management
- Test key generation code
- Apply security best practices

**Key Features:**
- ðŸ” **Secure Defaults**: All keys generated with cryptographically secure randomness
- ðŸŽ¯ **Modern Algorithms**: Ed25519 (recommended), ECDSA, RSA support
- ðŸ’¾ **Safe Storage**: Atomic writes with restricted file permissions
- ðŸ”„ **Format Flexibility**: PEM, DER, and raw byte formats
- âš¡ **High Performance**: Fast key generation and operations
- ðŸ§ª **Testable**: Easy to mock and test key operations

## Prerequisites

```bash
# Core cryptography support (included by default)
pip install provide-foundation

# For advanced crypto features (optional)
pip install provide-foundation[crypto]
```

## Basic Key Generation

### Ed25519 Keys (Recommended)

Ed25519 provides the best performance and security for most use cases:

```python
from provide.foundation.crypto import generate_ed25519_keypair
from provide.foundation import logger

# Generate a new keypair
public_key, private_key = generate_ed25519_keypair()

logger.info(
    "keypair_generated",
    algorithm="ed25519",
    public_key_size=len(public_key),
    private_key_size=len(private_key)
)

# Keys are returned as bytes
print(f"Public key (hex): {public_key.hex()}")
print(f"Private key (hex): {private_key.hex()}")
```

### RSA Keys

RSA keys are widely supported but slower than Ed25519:

```python
from provide.foundation.crypto import generate_rsa_keypair

# Generate 2048-bit RSA keypair (minimum recommended)
public_key, private_key = generate_rsa_keypair(key_size=2048)

# For higher security, use 4096-bit keys
public_key_4k, private_key_4k = generate_rsa_keypair(key_size=4096)

logger.info(
    "rsa_keypair_generated",
    key_size=4096,
    public_key_format="PKCS#1 PEM"
)
```

### ECDSA Keys

ECDSA provides a good balance of security and compatibility:

```python
from provide.foundation.crypto import generate_ecdsa_keypair

# Generate P-256 ECDSA keypair
public_key, private_key = generate_ecdsa_keypair(curve="P-256")

# Other supported curves
p384_pub, p384_priv = generate_ecdsa_keypair(curve="P-384")  # Higher security
p521_pub, p521_priv = generate_ecdsa_keypair(curve="P-521")  # Maximum security
```

## Secure Key Storage

### Save Keys with Proper Permissions

Always save private keys with restricted file permissions:

```python
from pathlib import Path
from provide.foundation.file import atomic_write
from provide.foundation import logger

def save_keypair(
    public_key: bytes,
    private_key: bytes,
    key_dir: Path
) -> tuple[Path, Path]:
    """Save a keypair securely to disk."""
    key_dir.mkdir(parents=True, exist_ok=True)

    # Save private key with owner-only permissions
    private_path = key_dir / "private_key.pem"
    atomic_write(
        path=private_path,
        content=private_key,
        permissions=0o600  # -rw-------
    )

    # Save public key with standard permissions
    public_path = key_dir / "public_key.pem"
    atomic_write(
        path=public_path,
        content=public_key,
        permissions=0o644  # -rw-r--r--
    )

    logger.info(
        "keypair_saved",
        private_path=str(private_path),
        public_path=str(public_path),
        private_permissions="0600"
    )

    return public_path, private_path

# Usage
public_key, private_key = generate_ed25519_keypair()
save_keypair(public_key, private_key, Path("~/.ssh/myapp").expanduser())
```

### Load Keys from Files

```python
from pathlib import Path
from provide.foundation import logger

def load_keypair(key_dir: Path) -> tuple[bytes, bytes]:
    """Load a keypair from disk."""
    private_path = key_dir / "private_key.pem"
    public_path = key_dir / "public_key.pem"

    # Verify permissions before loading private key
    private_stat = private_path.stat()
    if private_stat.st_mode & 0o077:
        logger.warning(
            "insecure_private_key_permissions",
            path=str(private_path),
            permissions=oct(private_stat.st_mode & 0o777)
        )

    private_key = private_path.read_bytes()
    public_key = public_path.read_bytes()

    logger.debug("keypair_loaded", public_path=str(public_path))

    return public_key, private_key
```

## Advanced Key Management

### Environment-Based Key Loading

Load keys from environment variables for containerized deployments:

```python
import os
from base64 import b64decode
from provide.foundation.utils.environment import get_str, require
from provide.foundation import logger

def load_keys_from_env() -> tuple[bytes, bytes]:
    """Load keys from environment variables."""
    # Support both direct value and file:// prefix
    public_key_value = require("PUBLIC_KEY")
    private_key_value = require("PRIVATE_KEY")

    # Decode base64-encoded keys
    public_key = b64decode(public_key_value)
    private_key = b64decode(private_key_value)

    logger.info("keys_loaded_from_environment")

    return public_key, private_key

# Usage with file:// prefix (reads from secret files)
# export PUBLIC_KEY="file:///run/secrets/public_key"
# export PRIVATE_KEY="file:///run/secrets/private_key"
```

### Key Format Conversion

Convert between PEM, DER, and raw formats:

```python
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric import ed25519
from provide.foundation import logger

def convert_key_formats(private_key_bytes: bytes) -> dict[str, bytes]:
    """Convert a key to multiple formats."""
    # Load the private key
    private_key = ed25519.Ed25519PrivateKey.from_private_bytes(private_key_bytes)

    # PEM format (text-based, widely compatible)
    pem_private = private_key.private_bytes(
        encoding=serialization.Encoding.PEM,
        format=serialization.PrivateFormat.PKCS8,
        encryption_algorithm=serialization.NoEncryption()
    )

    # DER format (binary, more compact)
    der_private = private_key.private_bytes(
        encoding=serialization.Encoding.DER,
        format=serialization.PrivateFormat.PKCS8,
        encryption_algorithm=serialization.NoEncryption()
    )

    # Raw format (32 bytes for Ed25519)
    raw_private = private_key.private_bytes(
        encoding=serialization.Encoding.Raw,
        format=serialization.PrivateFormat.Raw,
        encryption_algorithm=serialization.NoEncryption()
    )

    logger.debug(
        "key_formats_generated",
        pem_size=len(pem_private),
        der_size=len(der_private),
        raw_size=len(raw_private)
    )

    return {
        "pem": pem_private,
        "der": der_private,
        "raw": raw_private
    }
```

### Encrypted Private Key Storage

Protect private keys with password-based encryption:

```python
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric import rsa
from provide.foundation.utils.environment import get_str
from provide.foundation import logger

def save_encrypted_private_key(
    private_key: bytes,
    password: str,
    output_path: Path
) -> None:
    """Save a private key encrypted with a password."""
    from provide.foundation.file import atomic_write

    # Convert bytes to key object (example for RSA)
    key_obj = serialization.load_pem_private_key(
        private_key,
        password=None
    )

    # Encrypt with password
    encrypted_pem = key_obj.private_bytes(
        encoding=serialization.Encoding.PEM,
        format=serialization.PrivateFormat.PKCS8,
        encryption_algorithm=serialization.BestAvailableEncryption(
            password.encode()
        )
    )

    # Save with restricted permissions
    atomic_write(
        path=output_path,
        content=encrypted_pem,
        permissions=0o600
    )

    logger.info(
        "encrypted_private_key_saved",
        path=str(output_path),
        encryption="PKCS8"
    )

# Usage
password = get_str("KEY_PASSWORD", required=True)
save_encrypted_private_key(private_key, password, Path("encrypted_key.pem"))
```

## Key Rotation Patterns

### Automatic Key Rotation

Implement periodic key rotation for enhanced security:

```python
from datetime import datetime, timedelta
from pathlib import Path
from provide.foundation.crypto import generate_ed25519_keypair
from provide.foundation.file import atomic_write
from provide.foundation import logger

class KeyRotationManager:
    """Manage automatic key rotation."""

    def __init__(self, key_dir: Path, rotation_days: int = 90):
        self.key_dir = key_dir
        self.rotation_days = rotation_days
        self.key_dir.mkdir(parents=True, exist_ok=True)

    def should_rotate(self) -> bool:
        """Check if keys should be rotated."""
        current_key = self.key_dir / "private_key.pem"

        if not current_key.exists():
            return True

        # Check key age
        key_age = datetime.now() - datetime.fromtimestamp(
            current_key.stat().st_mtime
        )

        should_rotate = key_age > timedelta(days=self.rotation_days)

        logger.debug(
            "key_rotation_check",
            key_age_days=key_age.days,
            rotation_threshold=self.rotation_days,
            should_rotate=should_rotate
        )

        return should_rotate

    def rotate_keys(self) -> tuple[bytes, bytes]:
        """Generate new keys and archive old ones."""
        # Archive existing keys
        current_private = self.key_dir / "private_key.pem"
        current_public = self.key_dir / "public_key.pem"

        if current_private.exists():
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            archive_dir = self.key_dir / "archive"
            archive_dir.mkdir(exist_ok=True)

            current_private.rename(
                archive_dir / f"private_key_{timestamp}.pem"
            )
            current_public.rename(
                archive_dir / f"public_key_{timestamp}.pem"
            )

            logger.info("old_keys_archived", timestamp=timestamp)

        # Generate new keys
        public_key, private_key = generate_ed25519_keypair()

        # Save new keys
        atomic_write(
            path=current_private,
            content=private_key,
            permissions=0o600
        )
        atomic_write(
            path=current_public,
            content=public_key,
            permissions=0o644
        )

        logger.info("keys_rotated", key_dir=str(self.key_dir))

        return public_key, private_key

# Usage
manager = KeyRotationManager(Path("~/.ssh/myapp").expanduser())
if manager.should_rotate():
    public_key, private_key = manager.rotate_keys()
```

### Multi-Key Management

Maintain multiple keys for different purposes:

```python
from enum import Enum
from pathlib import Path
from provide.foundation.crypto import generate_ed25519_keypair
from provide.foundation import logger

class KeyPurpose(Enum):
    """Key usage purposes."""
    SIGNING = "signing"
    ENCRYPTION = "encryption"
    AUTHENTICATION = "authentication"

class MultiKeyManager:
    """Manage multiple keys by purpose."""

    def __init__(self, key_dir: Path):
        self.key_dir = key_dir
        self.key_dir.mkdir(parents=True, exist_ok=True)

    def get_key_path(self, purpose: KeyPurpose, key_type: str) -> Path:
        """Get path for a specific key."""
        return self.key_dir / f"{purpose.value}_{key_type}.pem"

    def generate_keys(self, purpose: KeyPurpose) -> tuple[bytes, bytes]:
        """Generate keys for a specific purpose."""
        public_key, private_key = generate_ed25519_keypair()

        # Save keys
        private_path = self.get_key_path(purpose, "private")
        public_path = self.get_key_path(purpose, "public")

        from provide.foundation.file import atomic_write
        atomic_write(path=private_path, content=private_key, permissions=0o600)
        atomic_write(path=public_path, content=public_key, permissions=0o644)

        logger.info("purpose_keys_generated", purpose=purpose.value)

        return public_key, private_key

    def load_keys(self, purpose: KeyPurpose) -> tuple[bytes, bytes]:
        """Load keys for a specific purpose."""
        private_path = self.get_key_path(purpose, "private")
        public_path = self.get_key_path(purpose, "public")

        if not private_path.exists():
            logger.info(
                "generating_missing_keys",
                purpose=purpose.value
            )
            return self.generate_keys(purpose)

        return public_path.read_bytes(), private_path.read_bytes()

# Usage
manager = MultiKeyManager(Path("~/.keys").expanduser())
signing_pub, signing_priv = manager.load_keys(KeyPurpose.SIGNING)
auth_pub, auth_priv = manager.load_keys(KeyPurpose.AUTHENTICATION)
```

## Common Patterns

### API Key Generation

Generate secure random API keys:

```python
import secrets
from base64 import urlsafe_b64encode
from provide.foundation import logger

def generate_api_key(prefix: str = "pk", length: int = 32) -> str:
    """Generate a secure API key."""
    # Generate cryptographically secure random bytes
    random_bytes = secrets.token_bytes(length)

    # Encode as URL-safe base64
    encoded = urlsafe_b64encode(random_bytes).decode().rstrip("=")

    # Add prefix for identification
    api_key = f"{prefix}_{encoded}"

    logger.info(
        "api_key_generated",
        prefix=prefix,
        length=len(api_key)
    )

    return api_key

# Usage
api_key = generate_api_key(prefix="prod", length=32)
print(f"API Key: {api_key}")
```

### SSH Key Generation

Generate SSH-compatible Ed25519 keys:

```python
from pathlib import Path
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric import ed25519
from provide.foundation.file import atomic_write
from provide.foundation import logger

def generate_ssh_keypair(
    key_path: Path,
    comment: str = ""
) -> tuple[Path, Path]:
    """Generate SSH-compatible Ed25519 keypair."""
    # Generate keypair
    private_key = ed25519.Ed25519PrivateKey.generate()
    public_key = private_key.public_key()

    # Format private key (OpenSSH format)
    private_pem = private_key.private_bytes(
        encoding=serialization.Encoding.PEM,
        format=serialization.PrivateFormat.OpenSSH,
        encryption_algorithm=serialization.NoEncryption()
    )

    # Format public key (OpenSSH format)
    public_ssh = public_key.public_bytes(
        encoding=serialization.Encoding.OpenSSH,
        format=serialization.PublicFormat.OpenSSH
    )

    # Add comment if provided
    if comment:
        public_ssh = public_ssh + f" {comment}".encode()

    # Save keys
    private_path = key_path
    public_path = key_path.with_suffix(".pub")

    atomic_write(path=private_path, content=private_pem, permissions=0o600)
    atomic_write(path=public_path, content=public_ssh, permissions=0o644)

    logger.info(
        "ssh_keypair_generated",
        private_key=str(private_path),
        public_key=str(public_path)
    )

    return public_path, private_path

# Usage
ssh_key_path = Path("~/.ssh/id_ed25519_myapp").expanduser()
generate_ssh_keypair(ssh_key_path, comment="user@host")
```

### Deterministic Key Derivation

Derive keys from a master secret:

```python
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.hkdf import HKDF
from provide.foundation import logger

def derive_key(
    master_secret: bytes,
    purpose: str,
    key_length: int = 32
) -> bytes:
    """Derive a key from a master secret using HKDF."""
    hkdf = HKDF(
        algorithm=hashes.SHA256(),
        length=key_length,
        salt=None,
        info=purpose.encode()
    )

    derived_key = hkdf.derive(master_secret)

    logger.debug(
        "key_derived",
        purpose=purpose,
        key_length=key_length
    )

    return derived_key

# Usage
master_secret = secrets.token_bytes(32)
signing_key = derive_key(master_secret, "signing-key-v1")
encryption_key = derive_key(master_secret, "encryption-key-v1")
```

## Best Practices

### âœ… DO: Use Modern Algorithms

```python
# âœ… GOOD: Use Ed25519 for new applications
from provide.foundation.crypto import generate_ed25519_keypair

public_key, private_key = generate_ed25519_keypair()
```

### âŒ DON'T: Use Small RSA Keys

```python
# âŒ BAD: RSA keys smaller than 2048 bits are insecure
public_key, private_key = generate_rsa_keypair(key_size=1024)  # Too small!

# âœ… GOOD: Use at least 2048-bit RSA, prefer 4096-bit
public_key, private_key = generate_rsa_keypair(key_size=4096)
```

### âœ… DO: Restrict Private Key Permissions

```python
# âœ… GOOD: Save private keys with 0600 permissions
from provide.foundation.file import atomic_write

atomic_write(
    path="private_key.pem",
    content=private_key,
    permissions=0o600  # Owner read/write only
)
```

### âŒ DON'T: Store Keys in Code

```python
# âŒ BAD: Hardcoded keys in source code
PRIVATE_KEY = b"-----BEGIN PRIVATE KEY-----\n..."  # Never do this!

# âœ… GOOD: Load from environment or secure storage
from provide.foundation.utils.environment import require

private_key = require("PRIVATE_KEY")  # From environment or file://
```

### âœ… DO: Rotate Keys Periodically

```python
# âœ… GOOD: Implement automatic key rotation
manager = KeyRotationManager(key_dir, rotation_days=90)
if manager.should_rotate():
    new_public, new_private = manager.rotate_keys()
```

### âŒ DON'T: Reuse Keys Across Purposes

```python
# âŒ BAD: Using same key for signing and encryption
signing_key = private_key
encryption_key = private_key  # Don't reuse!

# âœ… GOOD: Generate separate keys for different purposes
signing_pub, signing_priv = manager.load_keys(KeyPurpose.SIGNING)
encryption_pub, encryption_priv = manager.load_keys(KeyPurpose.ENCRYPTION)
```

### âœ… DO: Validate Key Permissions

```python
# âœ… GOOD: Check permissions before loading private keys
import stat

key_stat = key_path.stat()
if key_stat.st_mode & stat.S_IRWXG or key_stat.st_mode & stat.S_IRWXO:
    logger.error("insecure_key_permissions", path=str(key_path))
    raise PermissionError("Private key has insecure permissions")
```

### âŒ DON'T: Log Private Keys

```python
# âŒ BAD: Logging sensitive key material
logger.info("key_loaded", private_key=private_key.hex())  # NEVER!

# âœ… GOOD: Log only metadata
logger.info(
    "key_loaded",
    key_type="ed25519",
    key_size=len(private_key)
)
```

### âœ… DO: Use Secure Random Generation

```python
# âœ… GOOD: Use secrets module for randomness
import secrets

api_key = secrets.token_urlsafe(32)

# âŒ BAD: Don't use random module for crypto
import random
api_key = random.randbytes(32)  # Not cryptographically secure!
```

### âŒ DON'T: Store Unencrypted Keys in Version Control

```python
# âŒ BAD: Keys in git repository
# .gitignore should include:
# *.pem
# *.key
# *_key
# secrets/

# âœ… GOOD: Store key paths in config, keys in secure storage
config = {
    "private_key_path": "file:///run/secrets/app_private_key"
}
```

### âœ… DO: Archive Old Keys During Rotation

```python
# âœ… GOOD: Keep old keys for a grace period
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
old_key_path.rename(f"archive/private_key_{timestamp}.pem")

# Allow verification of old signatures during transition
```

### âŒ DON'T: Share Private Keys Between Environments

```python
# âŒ BAD: Same key for dev/staging/prod
key_path = Path("shared_key.pem")

# âœ… GOOD: Separate keys per environment
env = os.getenv("ENVIRONMENT", "dev")
key_path = Path(f"keys/{env}_private_key.pem")
```

### âœ… DO: Use Key Derivation for Related Keys

```python
# âœ… GOOD: Derive multiple keys from one master secret
master = secrets.token_bytes(32)
db_key = derive_key(master, "database-encryption")
api_key = derive_key(master, "api-signing")

# Only need to protect one master secret
```

### âŒ DON'T: Ignore Key Format Errors

```python
# âŒ BAD: Silently failing on invalid keys
try:
    key = load_key(path)
except Exception:
    key = generate_new_key()  # Masks real issues

# âœ… GOOD: Validate and fail fast
try:
    key = load_key(path)
except ValueError as e:
    logger.error("invalid_key_format", path=path, error=str(e))
    raise
```

### âœ… DO: Test Key Operations

```python
# âœ… GOOD: Verify key generation and loading
public_key, private_key = generate_ed25519_keypair()

# Test that keys can be used
from provide.foundation.crypto import sign_ed25519, verify_ed25519
message = b"test"
signature = sign_ed25519(message, private_key)
assert verify_ed25519(message, signature, public_key)
```

## Testing Key Generation

### Unit Testing

```python
import pytest
from pathlib import Path
from provide.foundation.crypto import generate_ed25519_keypair
from provide.foundation.file import atomic_write
from provide.testkit import FoundationTestCase

class TestKeyGeneration(FoundationTestCase):
    """Test key generation and management."""

    def setup_method(self) -> None:
        """Set up test fixtures."""
        super().setup_method()
        self.test_dir = Path("/tmp/test_keys")
        self.test_dir.mkdir(exist_ok=True)

    def teardown_method(self) -> None:
        """Clean up test files."""
        import shutil
        if self.test_dir.exists():
            shutil.rmtree(self.test_dir)
        super().teardown_method()

    def test_ed25519_generation(self) -> None:
        """Test Ed25519 keypair generation."""
        public_key, private_key = generate_ed25519_keypair()

        # Verify key sizes
        assert len(public_key) == 32
        assert len(private_key) == 32

        # Verify keys are different
        assert public_key != private_key

    def test_key_file_permissions(self) -> None:
        """Test that private keys have correct permissions."""
        public_key, private_key = generate_ed25519_keypair()

        private_path = self.test_dir / "private_key.pem"
        atomic_write(
            path=private_path,
            content=private_key,
            permissions=0o600
        )

        # Verify permissions
        import stat
        mode = private_path.stat().st_mode
        assert stat.S_IMODE(mode) == 0o600

    def test_key_rotation(self) -> None:
        """Test key rotation manager."""
        manager = KeyRotationManager(self.test_dir, rotation_days=0)

        # Should rotate on first run
        assert manager.should_rotate()

        # Generate keys
        pub1, priv1 = manager.rotate_keys()

        # Should not rotate immediately
        assert not manager.should_rotate()
```

### Mocking Key Generation

```python
from unittest.mock import patch

def test_with_mock_keys():
    """Test using mocked key generation."""
    mock_public = b"mock_public_key_32bytes_long!!!!"
    mock_private = b"mock_private_key_32bytes_long!!"

    with patch("provide.foundation.crypto.generate_ed25519_keypair") as mock_gen:
        mock_gen.return_value = (mock_public, mock_private)

        # Your test code here
        public, private = generate_ed25519_keypair()
        assert public == mock_public
        assert private == mock_private
```

## Next Steps

### Related Guides
- **[Signing & Verification](signing.md)**: Use keys to sign and verify data
- **[Certificates](certificates.md)**: Generate X.509 certificates from keys
- **[Basic Logging](../logging/basic-logging.md)**: Log key management operations

### Examples
- See `examples/crypto/01_key_generation.py` for key generation examples
- See `examples/crypto/02_key_rotation.py` for rotation patterns
- See `examples/production/05_secret_management.py` for production key management

### API Reference
- **[Crypto Module](../../reference/provide/foundation/crypto/index.md)**: Complete crypto API
- **[File Operations](../../reference/provide/foundation/file/index.md)**: Atomic file writes

---

**Tip**: Prefer Ed25519 for new applications - it's faster, more secure, and has smaller key sizes than RSA. Always store private keys with 0600 permissions and never commit them to version control.
>>> EOF >>>

### FILE 20: how-to-guides/crypto/signing.md | checksum=9c7d909139ff... | modified=2025-10-24T19:33:45 | op=+ | size=27121 | tokens=6088 | type=markdown ###
<<< BOF <<<
# Digital Signatures & Verification

Learn how to sign data and verify signatures using cryptographic keys with Foundation's signing utilities.

## Overview

Digital signatures provide cryptographic proof of authenticity and integrity. Foundation supports multiple signature algorithms including Ed25519 (recommended), RSA, and ECDSA, with simple APIs for signing messages, files, and structured data.

**What you'll learn:**
- Sign and verify data with Ed25519, RSA, and ECDSA
- Sign files and verify file integrity
- Implement detached signatures
- Create and verify JWT tokens
- Build multi-signature schemes
- Apply timestamping to signatures
- Test signature code
- Follow security best practices

**Key Features:**
- ðŸ” **Multiple Algorithms**: Ed25519, RSA, ECDSA support
- âš¡ **High Performance**: Fast signature generation and verification
- ðŸ“ **Message Signing**: Sign arbitrary data and messages
- ðŸ“„ **File Signing**: Built-in file integrity verification
- ðŸŽ¯ **JWT Support**: Create and verify JSON Web Tokens
- ðŸ”’ **Detached Signatures**: Separate signature files for large data
- ðŸ§ª **Testable**: Easy to mock and test signature operations

## Prerequisites

```bash
# Core cryptography support (included by default)
pip install provide-foundation

# For JWT support (optional)
pip install provide-foundation[jwt]
```

## Basic Signing & Verification

### Ed25519 Signatures (Recommended)

Ed25519 provides the fastest and most secure signatures:

```python
from provide.foundation.crypto import (
    generate_ed25519_keypair,
    sign_ed25519,
    verify_ed25519
)
from provide.foundation import logger

# Generate keypair
public_key, private_key = generate_ed25519_keypair()

# Sign a message
message = b"Important data to sign"
signature = sign_ed25519(message, private_key)

logger.info(
    "message_signed",
    algorithm="ed25519",
    message_size=len(message),
    signature_size=len(signature)
)

# Verify signature
is_valid = verify_ed25519(message, signature, public_key)
logger.info("signature_verified", valid=is_valid)

print(f"Signature valid: {is_valid}")  # True
```

### RSA Signatures

RSA signatures are widely supported but slower:

```python
from provide.foundation.crypto import (
    generate_rsa_keypair,
    sign_rsa,
    verify_rsa
)

# Generate RSA keypair
public_key, private_key = generate_rsa_keypair(key_size=4096)

# Sign data
message = b"Important message"
signature = sign_rsa(message, private_key)

# Verify signature
is_valid = verify_rsa(message, signature, public_key)

logger.info(
    "rsa_signature_verified",
    key_size=4096,
    valid=is_valid
)
```

### ECDSA Signatures

ECDSA provides good balance of security and performance:

```python
from provide.foundation.crypto import (
    generate_ecdsa_keypair,
    sign_ecdsa,
    verify_ecdsa
)

# Generate ECDSA keypair (P-256 curve)
public_key, private_key = generate_ecdsa_keypair(curve="P-256")

# Sign message
message = b"Data to sign"
signature = sign_ecdsa(message, private_key)

# Verify signature
is_valid = verify_ecdsa(message, signature, public_key)
```

## File Signing

### Sign and Verify Files

Sign files to ensure integrity and authenticity:

```python
from pathlib import Path
from provide.foundation.crypto import sign_ed25519, verify_ed25519
from provide.foundation import logger

def sign_file(file_path: Path, private_key: bytes) -> bytes:
    """Sign a file's contents."""
    data = file_path.read_bytes()
    signature = sign_ed25519(data, private_key)

    logger.info(
        "file_signed",
        file=str(file_path),
        file_size=len(data),
        signature_size=len(signature)
    )

    return signature

def verify_file(
    file_path: Path,
    signature: bytes,
    public_key: bytes
) -> bool:
    """Verify a file's signature."""
    data = file_path.read_bytes()
    is_valid = verify_ed25519(data, signature, public_key)

    logger.info(
        "file_verification",
        file=str(file_path),
        valid=is_valid
    )

    return is_valid

# Usage
file_path = Path("document.pdf")
signature = sign_file(file_path, private_key)

# Later, verify the file hasn't changed
if verify_file(file_path, signature, public_key):
    print("File is authentic and unmodified")
else:
    print("File has been tampered with!")
```

### Detached Signature Files

Store signatures separately for large files:

```python
from pathlib import Path
from provide.foundation.crypto import sign_ed25519, verify_ed25519
from provide.foundation.file import atomic_write
from provide.foundation import logger

def create_detached_signature(
    file_path: Path,
    private_key: bytes
) -> Path:
    """Create a detached signature file."""
    # Sign the file
    data = file_path.read_bytes()
    signature = sign_ed25519(data, private_key)

    # Save signature to .sig file
    sig_path = file_path.with_suffix(file_path.suffix + ".sig")
    atomic_write(path=sig_path, content=signature)

    logger.info(
        "detached_signature_created",
        file=str(file_path),
        signature_file=str(sig_path)
    )

    return sig_path

def verify_detached_signature(
    file_path: Path,
    public_key: bytes
) -> bool:
    """Verify a detached signature file."""
    sig_path = file_path.with_suffix(file_path.suffix + ".sig")

    if not sig_path.exists():
        logger.warning("signature_file_missing", file=str(file_path))
        return False

    # Read file and signature
    data = file_path.read_bytes()
    signature = sig_path.read_bytes()

    # Verify
    is_valid = verify_ed25519(data, signature, public_key)

    logger.info(
        "detached_signature_verified",
        file=str(file_path),
        valid=is_valid
    )

    return is_valid

# Usage
file_path = Path("release.tar.gz")
sig_path = create_detached_signature(file_path, private_key)

# Verify later
if verify_detached_signature(file_path, public_key):
    print("Release package is authentic")
```

## JSON Web Tokens (JWT)

### Create JWT Tokens

Sign structured data as JWT tokens:

```python
import json
from datetime import datetime, timedelta
from base64 import urlsafe_b64encode, urlsafe_b64decode
from provide.foundation.crypto import sign_ed25519, verify_ed25519
from provide.foundation import logger

def create_jwt(
    payload: dict,
    private_key: bytes,
    expires_in: timedelta = timedelta(hours=1)
) -> str:
    """Create a JWT token."""
    # Add standard claims
    now = datetime.utcnow()
    payload["iat"] = int(now.timestamp())
    payload["exp"] = int((now + expires_in).timestamp())

    # Encode header and payload
    header = {"alg": "EdDSA", "typ": "JWT"}
    header_b64 = urlsafe_b64encode(
        json.dumps(header).encode()
    ).decode().rstrip("=")
    payload_b64 = urlsafe_b64encode(
        json.dumps(payload).encode()
    ).decode().rstrip("=")

    # Create signing input
    message = f"{header_b64}.{payload_b64}".encode()

    # Sign
    signature = sign_ed25519(message, private_key)
    signature_b64 = urlsafe_b64encode(signature).decode().rstrip("=")

    # Combine into JWT
    jwt_token = f"{header_b64}.{payload_b64}.{signature_b64}"

    logger.info(
        "jwt_created",
        subject=payload.get("sub"),
        expires_in=expires_in.total_seconds()
    )

    return jwt_token

def verify_jwt(token: str, public_key: bytes) -> dict | None:
    """Verify and decode a JWT token."""
    try:
        # Split token
        parts = token.split(".")
        if len(parts) != 3:
            logger.warning("invalid_jwt_format")
            return None

        header_b64, payload_b64, signature_b64 = parts

        # Verify signature
        message = f"{header_b64}.{payload_b64}".encode()
        signature = urlsafe_b64decode(signature_b64 + "==")

        if not verify_ed25519(message, signature, public_key):
            logger.warning("jwt_signature_invalid")
            return None

        # Decode payload
        payload_json = urlsafe_b64decode(payload_b64 + "==")
        payload = json.loads(payload_json)

        # Check expiration
        if "exp" in payload:
            exp = datetime.fromtimestamp(payload["exp"])
            if datetime.utcnow() > exp:
                logger.warning("jwt_expired", exp=payload["exp"])
                return None

        logger.info("jwt_verified", subject=payload.get("sub"))
        return payload

    except Exception as e:
        logger.error("jwt_verification_failed", error=str(e))
        return None

# Usage
payload = {
    "sub": "user123",
    "name": "Alice",
    "admin": True
}
token = create_jwt(payload, private_key, expires_in=timedelta(hours=24))

# Later, verify the token
verified_payload = verify_jwt(token, public_key)
if verified_payload:
    print(f"Token valid for user: {verified_payload['name']}")
```

## Advanced Signature Patterns

### Multi-Signature Verification

Require multiple signatures for critical operations:

```python
from provide.foundation.crypto import sign_ed25519, verify_ed25519
from provide.foundation import logger

class MultiSignature:
    """Manage multi-signature verification."""

    def __init__(self, threshold: int):
        self.threshold = threshold
        self.signatures: list[tuple[bytes, bytes]] = []  # (public_key, signature)

    def add_signature(self, public_key: bytes, signature: bytes) -> None:
        """Add a signature from a signer."""
        self.signatures.append((public_key, signature))
        logger.debug(
            "signature_added",
            total_signatures=len(self.signatures),
            threshold=self.threshold
        )

    def verify(self, message: bytes) -> bool:
        """Verify that threshold of signatures is valid."""
        valid_count = 0

        for public_key, signature in self.signatures:
            if verify_ed25519(message, signature, public_key):
                valid_count += 1

        is_valid = valid_count >= self.threshold

        logger.info(
            "multisig_verification",
            valid_signatures=valid_count,
            threshold=self.threshold,
            valid=is_valid
        )

        return is_valid

# Usage: Require 2 of 3 signatures
message = b"Transfer $1,000,000"

# Three signers
pub1, priv1 = generate_ed25519_keypair()
pub2, priv2 = generate_ed25519_keypair()
pub3, priv3 = generate_ed25519_keypair()

# Create multi-signature
multisig = MultiSignature(threshold=2)
multisig.add_signature(pub1, sign_ed25519(message, priv1))
multisig.add_signature(pub2, sign_ed25519(message, priv2))

# Verify (2 signatures meets threshold)
if multisig.verify(message):
    print("Transaction approved by quorum")
```

### Timestamped Signatures

Include timestamps to prevent replay attacks:

```python
import time
from datetime import datetime, timedelta
from provide.foundation.crypto import sign_ed25519, verify_ed25519
from provide.foundation import logger

def create_timestamped_signature(
    message: bytes,
    private_key: bytes
) -> tuple[bytes, int]:
    """Create a signature with timestamp."""
    timestamp = int(time.time())

    # Include timestamp in signed data
    data_to_sign = message + timestamp.to_bytes(8, 'big')
    signature = sign_ed25519(data_to_sign, private_key)

    logger.info("timestamped_signature_created", timestamp=timestamp)

    return signature, timestamp

def verify_timestamped_signature(
    message: bytes,
    signature: bytes,
    timestamp: int,
    public_key: bytes,
    max_age: timedelta = timedelta(minutes=5)
) -> bool:
    """Verify a timestamped signature."""
    # Check timestamp freshness
    now = int(time.time())
    age = now - timestamp

    if age > max_age.total_seconds():
        logger.warning(
            "signature_expired",
            age_seconds=age,
            max_age_seconds=max_age.total_seconds()
        )
        return False

    if age < 0:
        logger.warning("signature_from_future", timestamp=timestamp)
        return False

    # Verify signature with timestamp
    data_to_verify = message + timestamp.to_bytes(8, 'big')
    is_valid = verify_ed25519(data_to_verify, signature, public_key)

    logger.info(
        "timestamped_signature_verified",
        valid=is_valid,
        age_seconds=age
    )

    return is_valid

# Usage
message = b"API request"
signature, timestamp = create_timestamped_signature(message, private_key)

# Verify with 5-minute window
if verify_timestamped_signature(message, signature, timestamp, public_key):
    print("Signature is fresh and valid")
```

### Signature with Metadata

Include additional context in signatures:

```python
import json
from provide.foundation.crypto import sign_ed25519, verify_ed25519
from provide.foundation import logger

class SignedMessage:
    """Message with signature and metadata."""

    def __init__(
        self,
        message: bytes,
        metadata: dict,
        signature: bytes,
        public_key: bytes
    ):
        self.message = message
        self.metadata = metadata
        self.signature = signature
        self.public_key = public_key

    @classmethod
    def create(
        cls,
        message: bytes,
        private_key: bytes,
        public_key: bytes,
        **metadata
    ) -> "SignedMessage":
        """Create a signed message with metadata."""
        # Add default metadata
        metadata.setdefault("timestamp", int(time.time()))
        metadata.setdefault("version", "1.0")

        # Create signing payload
        payload = {
            "message": message.hex(),
            "metadata": metadata
        }
        payload_bytes = json.dumps(payload, sort_keys=True).encode()

        # Sign
        signature = sign_ed25519(payload_bytes, private_key)

        logger.info("signed_message_created", metadata=metadata)

        return cls(message, metadata, signature, public_key)

    def verify(self) -> bool:
        """Verify the signature."""
        # Reconstruct payload
        payload = {
            "message": self.message.hex(),
            "metadata": self.metadata
        }
        payload_bytes = json.dumps(payload, sort_keys=True).encode()

        # Verify
        is_valid = verify_ed25519(payload_bytes, self.signature, self.public_key)

        logger.info(
            "signed_message_verified",
            valid=is_valid,
            metadata=self.metadata
        )

        return is_valid

# Usage
signed_msg = SignedMessage.create(
    message=b"Important announcement",
    private_key=private_key,
    public_key=public_key,
    author="alice@example.com",
    purpose="announcement"
)

if signed_msg.verify():
    print(f"Message from {signed_msg.metadata['author']} is authentic")
```

## Common Patterns

### API Request Signing

Sign API requests for authentication:

```python
import hashlib
import hmac
from datetime import datetime
from provide.foundation.crypto import sign_ed25519
from provide.foundation import logger

def sign_api_request(
    method: str,
    path: str,
    body: bytes,
    private_key: bytes
) -> dict[str, str]:
    """Create signature headers for API request."""
    timestamp = datetime.utcnow().isoformat()

    # Create canonical request
    canonical = f"{method}\n{path}\n{timestamp}\n{hashlib.sha256(body).hexdigest()}"

    # Sign
    signature = sign_ed25519(canonical.encode(), private_key)

    headers = {
        "X-Signature": signature.hex(),
        "X-Timestamp": timestamp,
        "X-Algorithm": "EdDSA"
    }

    logger.info("api_request_signed", method=method, path=path)

    return headers

# Usage
headers = sign_api_request(
    method="POST",
    path="/api/transfers",
    body=b'{"amount": 100}',
    private_key=private_key
)
```

### Code Signing

Sign code releases for distribution:

```python
from pathlib import Path
import tarfile
from provide.foundation.crypto import sign_ed25519
from provide.foundation.file import atomic_write
from provide.foundation import logger

def sign_release(
    release_dir: Path,
    version: str,
    private_key: bytes
) -> Path:
    """Sign a code release package."""
    # Create tarball
    tarball_path = release_dir.parent / f"release-{version}.tar.gz"

    with tarfile.open(tarball_path, "w:gz") as tar:
        tar.add(release_dir, arcname=f"release-{version}")

    # Sign the tarball
    tarball_data = tarball_path.read_bytes()
    signature = sign_ed25519(tarball_data, private_key)

    # Save signature
    sig_path = tarball_path.with_suffix(".tar.gz.sig")
    atomic_write(path=sig_path, content=signature)

    # Create manifest
    manifest = {
        "version": version,
        "file": tarball_path.name,
        "signature": sig_path.name,
        "sha256": hashlib.sha256(tarball_data).hexdigest()
    }
    manifest_path = tarball_path.parent / f"release-{version}.manifest.json"
    atomic_write(
        path=manifest_path,
        content=json.dumps(manifest, indent=2).encode()
    )

    logger.info(
        "release_signed",
        version=version,
        tarball=str(tarball_path),
        signature=str(sig_path)
    )

    return tarball_path

# Usage
release_path = sign_release(
    release_dir=Path("dist/myapp"),
    version="1.0.0",
    private_key=private_key
)
```

### Document Signing

Sign documents with verification metadata:

```python
from datetime import datetime
from provide.foundation.crypto import sign_ed25519, verify_ed25519
from provide.foundation import logger

class DocumentSignature:
    """Signed document with verification info."""

    def __init__(
        self,
        document: bytes,
        signature: bytes,
        signer: str,
        timestamp: datetime
    ):
        self.document = document
        self.signature = signature
        self.signer = signer
        self.timestamp = timestamp

    @classmethod
    def sign_document(
        cls,
        document: bytes,
        private_key: bytes,
        signer: str
    ) -> "DocumentSignature":
        """Sign a document."""
        signature = sign_ed25519(document, private_key)
        timestamp = datetime.utcnow()

        logger.info("document_signed", signer=signer)

        return cls(document, signature, signer, timestamp)

    def verify(self, public_key: bytes) -> bool:
        """Verify the document signature."""
        is_valid = verify_ed25519(self.document, self.signature, public_key)

        logger.info(
            "document_verified",
            valid=is_valid,
            signer=self.signer,
            timestamp=self.timestamp.isoformat()
        )

        return is_valid

    def to_dict(self) -> dict:
        """Export signature info."""
        return {
            "signer": self.signer,
            "timestamp": self.timestamp.isoformat(),
            "signature": self.signature.hex(),
            "document_hash": hashlib.sha256(self.document).hexdigest()
        }

# Usage
doc = Path("contract.pdf").read_bytes()
signed_doc = DocumentSignature.sign_document(
    document=doc,
    private_key=private_key,
    signer="alice@company.com"
)

if signed_doc.verify(public_key):
    print(f"Document signed by {signed_doc.signer} at {signed_doc.timestamp}")
```

## Best Practices

### âœ… DO: Use Ed25519 for New Applications

```python
# âœ… GOOD: Ed25519 is fast and secure
from provide.foundation.crypto import sign_ed25519, verify_ed25519

signature = sign_ed25519(message, private_key)
```

### âŒ DON'T: Use MD5 or SHA1 for Signatures

```python
# âŒ BAD: MD5 and SHA1 are cryptographically broken
import hashlib
signature = hashlib.md5(message).hexdigest()  # NEVER!

# âœ… GOOD: Use proper signature algorithms
from provide.foundation.crypto import sign_ed25519
signature = sign_ed25519(message, private_key)
```

### âœ… DO: Verify Signatures Before Trusting Data

```python
# âœ… GOOD: Always verify before using data
if verify_ed25519(message, signature, public_key):
    # Process trusted data
    process_message(message)
else:
    logger.error("signature_verification_failed")
    raise ValueError("Invalid signature")
```

### âŒ DON'T: Sign Sensitive Data Directly

```python
# âŒ BAD: Signing reveals the data
signature = sign_ed25519(password, private_key)  # Don't sign secrets!

# âœ… GOOD: Sign a hash of sensitive data
import hashlib
password_hash = hashlib.sha256(password).digest()
signature = sign_ed25519(password_hash, private_key)
```

### âœ… DO: Include Context in Signed Data

```python
# âœ… GOOD: Include purpose to prevent signature reuse
context = f"transfer:amount={amount}:to={recipient}".encode()
signature = sign_ed25519(context, private_key)

# Prevents using signature for different purpose
```

### âŒ DON'T: Reuse Signatures

```python
# âŒ BAD: Signature reuse across different data
signature = sign_ed25519(message1, private_key)
# Later using same signature for message2... NEVER!

# âœ… GOOD: Generate fresh signature for each message
sig1 = sign_ed25519(message1, private_key)
sig2 = sign_ed25519(message2, private_key)
```

### âœ… DO: Use Detached Signatures for Large Files

```python
# âœ… GOOD: Separate signature file for large data
signature = sign_file(large_file, private_key)
sig_path.write_bytes(signature)

# Allows distributing signature separately
```

### âŒ DON'T: Ignore Signature Verification Failures

```python
# âŒ BAD: Silently proceeding on verification failure
try:
    if not verify_ed25519(msg, sig, pub_key):
        pass  # Oops, data could be tampered!
except Exception:
    pass

# âœ… GOOD: Fail fast on verification failure
if not verify_ed25519(message, signature, public_key):
    logger.error("signature_invalid")
    raise SecurityError("Signature verification failed")
```

### âœ… DO: Include Timestamps to Prevent Replay

```python
# âœ… GOOD: Timestamp prevents reusing old signatures
timestamp = int(time.time())
data = message + timestamp.to_bytes(8, 'big')
signature = sign_ed25519(data, private_key)
```

### âŒ DON'T: Sign Untrusted User Input Directly

```python
# âŒ BAD: Signing arbitrary user data
user_input = request.get("data")
signature = sign_ed25519(user_input.encode(), private_key)

# âœ… GOOD: Validate and sanitize first
validated_data = validate_user_input(user_input)
signature = sign_ed25519(validated_data.encode(), private_key)
```

### âœ… DO: Use Different Keys for Different Purposes

```python
# âœ… GOOD: Separate signing and encryption keys
signing_pub, signing_priv = generate_ed25519_keypair()
encryption_pub, encryption_priv = generate_ed25519_keypair()

# Sign with signing key only
signature = sign_ed25519(message, signing_priv)
```

### âŒ DON'T: Expose Signatures Without Rate Limiting

```python
# âŒ BAD: Unlimited signature generation
@app.route("/sign")
def sign_endpoint():
    data = request.json["data"]
    return sign_ed25519(data.encode(), private_key).hex()

# âœ… GOOD: Rate limit signature operations
from provide.foundation.resilience import rate_limit

@rate_limit(max_calls=10, period=60)
def sign_data(data: bytes) -> bytes:
    return sign_ed25519(data, private_key)
```

### âœ… DO: Verify Signature Before Parsing Payload

```python
# âœ… GOOD: Verify signature first
if not verify_ed25519(payload, signature, public_key):
    raise ValueError("Invalid signature")

# Now safe to parse
data = json.loads(payload)
```

### âŒ DON'T: Log Signatures Carelessly

```python
# âŒ BAD: Logging signatures could aid attackers
logger.info("signature", sig=signature.hex())

# âœ… GOOD: Log only verification result
logger.info("signature_verification", valid=is_valid)
```

### âœ… DO: Test Signature Roundtrips

```python
# âœ… GOOD: Always test sign/verify cycle
message = b"test"
signature = sign_ed25519(message, private_key)
assert verify_ed25519(message, signature, public_key)

# Verify tampering is detected
assert not verify_ed25519(b"different", signature, public_key)
```

## Testing Signature Operations

### Unit Testing

```python
import pytest
from provide.foundation.crypto import (
    generate_ed25519_keypair,
    sign_ed25519,
    verify_ed25519
)
from provide.testkit import FoundationTestCase

class TestSignatures(FoundationTestCase):
    """Test signature operations."""

    def setup_method(self) -> None:
        """Set up test fixtures."""
        super().setup_method()
        self.public_key, self.private_key = generate_ed25519_keypair()

    def test_sign_and_verify(self) -> None:
        """Test basic signing and verification."""
        message = b"test message"
        signature = sign_ed25519(message, self.private_key)

        # Should verify with correct key
        assert verify_ed25519(message, signature, self.public_key)

        # Should fail with different message
        assert not verify_ed25519(b"different", signature, self.public_key)

    def test_signature_tampering_detection(self) -> None:
        """Test that tampering is detected."""
        message = b"original message"
        signature = sign_ed25519(message, self.private_key)

        # Tamper with signature
        tampered_sig = bytearray(signature)
        tampered_sig[0] ^= 1  # Flip one bit

        # Verification should fail
        assert not verify_ed25519(message, bytes(tampered_sig), self.public_key)

    def test_timestamped_signature(self) -> None:
        """Test timestamped signatures."""
        message = b"time-sensitive data"
        signature, timestamp = create_timestamped_signature(
            message,
            self.private_key
        )

        # Should verify within time window
        assert verify_timestamped_signature(
            message,
            signature,
            timestamp,
            self.public_key,
            max_age=timedelta(minutes=5)
        )
```

### Integration Testing

```python
def test_api_request_signing():
    """Test end-to-end API request signing."""
    # Create signed request
    headers = sign_api_request(
        method="POST",
        path="/api/data",
        body=b'{"key": "value"}',
        private_key=private_key
    )

    # Simulate server-side verification
    assert "X-Signature" in headers
    assert "X-Timestamp" in headers

    # Server would verify the signature
    # (implementation omitted for brevity)
```

## Next Steps

### Related Guides
- **[Key Generation](keys.md)**: Generate cryptographic keys for signing
- **[Certificates](certificates.md)**: X.509 certificates and PKI
- **[Basic Logging](../logging/basic-logging.md)**: Log signature operations

### Examples
- See `examples/crypto/03_digital_signatures.py` for signature examples
- See `examples/crypto/04_jwt_signing.py` for JWT patterns
- See `examples/production/06_api_signing.py` for API authentication

### API Reference
- **[Crypto Module](../../reference/provide/foundation/crypto/index.md)**: Complete crypto API
- **[Ed25519 Functions](../../reference/provide/foundation/crypto/ed25519.md)**: Ed25519 signatures
- **[RSA Functions](../../reference/provide/foundation/crypto/rsa.md)**: RSA signatures

---

**Tip**: Prefer Ed25519 signatures for new applications - they're faster and more secure than RSA while providing smaller signature sizes. Always verify signatures before trusting data, and include timestamps or nonces to prevent replay attacks.
>>> EOF >>>

### FILE 21: how-to-guides/file/atomic-writes.md | checksum=acbadc5ce1b5... | modified=2025-10-21T17:41:51 | op=+ | size=3512 | tokens=814 | type=markdown ###
<<< BOF <<<
# Atomic File Writes

Learn how to safely write files with atomic operations to prevent data corruption.

## Overview

Atomic writes ensure that file operations either complete fully or not at all, preventing partial writes and corruption.

## Basic Atomic Write

```python
from provide.foundation.file import atomic_write

# Write safely - creates temp file, then renames
atomic_write(
    path="config.json",
    content='{"setting": "value"}'
)

# If interrupted, original file is unchanged
# If successful, original is replaced atomically
```

## Atomic Write with Path Object

```python
from pathlib import Path
from provide.foundation.file import atomic_write

config_file = Path("config.json")

atomic_write(
    path=config_file,
    content='{"updated": true}'
)
```

## Binary File Writes

```python
# Write binary data atomically
image_data = b'\x89PNG\r\n\x1a\n...'

atomic_write(
    path="image.png",
    content=image_data,
    mode="wb"
)
```

## Atomic JSON Write

```python
from provide.foundation.serialization import provide_dumps
from provide.foundation.file import atomic_write

data = {
    "users": [
        {"id": 1, "name": "Alice"},
        {"id": 2, "name": "Bob"}
    ]
}

# Serialize and write atomically
atomic_write(
    path="users.json",
    content=provide_dumps(data, indent=2)
)
```

## Create Parent Directories

```python
# Automatically create parent directories
atomic_write(
    path="data/config/settings.json",
    content='{"key": "value"}',
    create_parents=True
)
```

## Preserve File Permissions

```python
import os

# Set specific permissions
atomic_write(
    path="secrets.txt",
    content="secret_data",
    mode="w",
    permissions=0o600  # Only owner can read/write
)
```

## Atomic Update Pattern

Safe read-modify-write:

```python
from pathlib import Path
from provide.foundation.serialization import provide_loads, provide_dumps
from provide.foundation.file import atomic_write

def update_config(key: str, value: str):
    """Safely update a single config value."""
    config_file = Path("config.json")

    # Read current config
    if config_file.exists():
        config = provide_loads(config_file.read_text())
    else:
        config = {}

    # Modify
    config[key] = value

    # Write atomically
    atomic_write(
        path=config_file,
        content=provide_dumps(config, indent=2)
    )
```

## Why Atomic Writes Matter

**Without atomic writes:**
```python
# âŒ Dangerous - can corrupt file if interrupted
with open("config.json", "w") as f:
    f.write('{"partial":')  # If crash happens here...
    f.write(' "data"}')     # ...file is corrupted
```

**With atomic writes:**
```python
# âœ… Safe - original file unchanged until write completes
atomic_write(
    path="config.json",
    content='{"complete": "data"}'
)
# Original file only replaced when write is complete
```

## Error Handling

```python
from provide.foundation.file import atomic_write
from provide.foundation.errors import FileOperationError

try:
    atomic_write(
        path="/readonly/file.txt",
        content="data"
    )
except FileOperationError as e:
    logger.error("write_failed", error=str(e))
```

## Next Steps

- **[File Watching](watching.md)** - Monitor file changes
- **[API Reference: File](../../reference/provide/foundation/file/index.md)** - Complete file API

---

**See also:** [examples/file_operations/01_basic_usage.py](https://github.com/provide-io/provide-foundation/blob/main/examples/file_operations/01_basic_usage.py)
>>> EOF >>>

### FILE 22: how-to-guides/file/watching.md | checksum=ddf796387225... | modified=2025-10-21T17:41:51 | op=+ | size=4950 | tokens=982 | type=markdown ###
<<< BOF <<<
# File Watching

Learn how to monitor files and directories for changes in real-time.

## Overview

Foundation provides file watching capabilities to detect when files are created, modified, or deleted.

## Basic File Watching

```python
from provide.foundation.file.operations import FileOperationDetector
from provide.foundation import logger

async def watch_config():
    """Watch configuration file for changes."""
    detector = FileOperationDetector()

    async for event in detector.watch("config.yaml"):
        logger.info(
            "config_changed",
            file=event.file_path,
            operation=event.operation_type
        )
        # Reload configuration
        reload_config()
```

## Watch Multiple Files

```python
files_to_watch = [
    "config.yaml",
    "secrets.env",
    "database.json"
]

async for event in detector.watch_multiple(files_to_watch):
    logger.info("file_changed", file=event.file_path)
    handle_change(event)
```

## Watch Directory

```python
async def watch_directory():
    """Watch all files in a directory."""
    detector = FileOperationDetector()

    async for event in detector.watch_directory("./config"):
        if event.operation_type == "created":
            logger.info("new_file", file=event.file_path)
        elif event.operation_type == "modified":
            logger.info("file_updated", file=event.file_path)
        elif event.operation_type == "deleted":
            logger.warning("file_removed", file=event.file_path)
```

## Filter by File Type

```python
from pathlib import Path

async def watch_yaml_files():
    """Watch only YAML files."""
    detector = FileOperationDetector()

    async for event in detector.watch_directory("./"):
        path = Path(event.file_path)
        if path.suffix in [".yaml", ".yml"]:
            logger.info("yaml_changed", file=path.name)
            process_yaml(path)
```

## Debouncing Changes

Handle rapid successive changes:

```python
import asyncio
from collections import defaultdict

async def watch_with_debounce():
    """Debounce file changes to avoid processing too frequently."""
    detector = FileOperationDetector()
    pending_changes = defaultdict(asyncio.Event)

    async def process_after_delay(file_path: str):
        """Process file after 500ms of no changes."""
        await asyncio.sleep(0.5)
        logger.info("processing_file", file=file_path)
        process_file(file_path)

    async for event in detector.watch_directory("./watched"):
        # Cancel pending task if exists
        if event.file_path in pending_changes:
            pending_changes[event.file_path].set()

        # Start new debounced task
        event = asyncio.Event()
        pending_changes[event.file_path] = event
        asyncio.create_task(process_after_delay(event.file_path))
```

## Streaming File Detection

Detect when files are being actively written:

```python
async def detect_streaming():
    """Detect when files are being streamed/written."""
    detector = FileOperationDetector()

    async for event in detector.detect_streaming_operations("logfile.txt"):
        if event.is_streaming:
            logger.debug("file_being_written", file=event.file_path)
        else:
            logger.info("file_write_complete", file=event.file_path)
            # Safe to process now
            process_complete_file(event.file_path)
```

## Hot Reload Pattern

Automatically reload configuration on changes:

```python
from provide.foundation.file.operations import FileOperationDetector
from provide.foundation.serialization import provide_loads
from pathlib import Path

class ConfigWatcher:
    def __init__(self, config_file: str):
        self.config_file = Path(config_file)
        self.config = {}
        self.detector = FileOperationDetector()

    async def start(self):
        """Start watching and reload on changes."""
        # Load initial config
        self.reload()

        # Watch for changes
        async for event in self.detector.watch(self.config_file):
            logger.info("config_reloaded")
            self.reload()

    def reload(self):
        """Reload configuration from file."""
        content = self.config_file.read_text()
        self.config = provide_loads(content)

# Usage
watcher = ConfigWatcher("config.json")
asyncio.run(watcher.start())
```

## Stop Watching

```python
detector = FileOperationDetector()

# Watch in background task
watch_task = asyncio.create_task(
    watch_files(detector)
)

# Later, stop watching
watch_task.cancel()
await detector.stop()
```

## Next Steps

- **[Atomic Writes](atomic-writes.md)** - Safe file writes
- **[API Reference: File Operations](../../reference/provide/foundation/file/operations/index.md)** - Complete file operations API

---

**See also:** [examples/file_operations/02_streaming_detection.py](https://github.com/provide-io/provide-foundation/blob/main/examples/file_operations/02_streaming_detection.py)
>>> EOF >>>

### FILE 23: how-to-guides/formatting/text-formatting.md | checksum=ba9c4ef605c8... | modified=2025-10-24T20:13:57 | op=+ | size=4176 | tokens=1147 | type=markdown ###
<<< BOF <<<
# How to Use Formatting Utilities

Foundation provides comprehensive formatting utilities for text, numbers, and data structures.

## Text Formatting

### Case Conversion

```python
from provide.foundation.formatting import (
    to_snake_case,
    to_camel_case,
    to_kebab_case
)

# Convert to snake_case
to_snake_case("HelloWorld")      # "hello_world"
to_snake_case("helloWorld")      # "hello_world"
to_snake_case("hello-world")     # "hello_world"

# Convert to camelCase
to_camel_case("hello_world")     # "helloWorld"
to_camel_case("hello-world")     # "helloWorld"

# Convert to kebab-case
to_kebab_case("hello_world")     # "hello-world"
to_kebab_case("HelloWorld")      # "hello-world"
```

### Text Manipulation

```python
from provide.foundation.formatting import (
    truncate,
    wrap_text,
    indent,
    pluralize,
    strip_ansi
)

# Truncate long text
truncate("This is a very long string", max_length=10)
# "This is..."

# Wrap text to width
wrap_text("This is a long line that needs wrapping", width=20)
# "This is a long line\nthat needs wrapping"

# Indent text
indent("Line 1\nLine 2", spaces=4)
# "    Line 1\n    Line 2"

# Pluralize words
pluralize("item", count=1)   # "item"
pluralize("item", count=2)   # "items"
pluralize("box", count=2)    # "boxes"

# Strip ANSI color codes
strip_ansi("\033[31mRed text\033[0m")
# "Red text"
```

## Number Formatting

### Size Formatting

```python
from provide.foundation.formatting import format_size

format_size(1024)              # "1.0 KB"
format_size(1024 * 1024)       # "1.0 MB"
format_size(1024 * 1024 * 1024)  # "1.0 GB"
format_size(1500)              # "1.5 KB"
```

### Duration Formatting

```python
from provide.foundation.formatting import format_duration

format_duration(1.5)           # "1.5s"
format_duration(90)            # "1m 30s"
format_duration(3661)          # "1h 1m 1s"
format_duration(0.001)         # "1ms"
```

### Number Formatting

```python
from provide.foundation.formatting import format_number, format_percentage

# Format numbers with thousands separators
format_number(1000)            # "1,000"
format_number(1000000)         # "1,000,000"

# Format percentages
format_percentage(0.855)       # "85.5%"
format_percentage(0.8555, decimals=1)  # "85.6%"
```

## Data Formatting

### Table Formatting

```python
from provide.foundation.formatting import format_table

data = [
    {"name": "Alice", "age": 30, "city": "NYC"},
    {"name": "Bob", "age": 25, "city": "SF"},
    {"name": "Charlie", "age": 35, "city": "LA"}
]

table = format_table(data)
print(table)
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
# â”‚ name    â”‚ age â”‚ city â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
# â”‚ Alice   â”‚ 30  â”‚ NYC  â”‚
# â”‚ Bob     â”‚ 25  â”‚ SF   â”‚
# â”‚ Charlie â”‚ 35  â”‚ LA   â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
```

### Grouped Formatting

```python
from provide.foundation.formatting import format_grouped

items = ["apple", "banana", "cherry", "date", "elderberry"]
grouped = format_grouped(items, items_per_group=2)
# [["apple", "banana"], ["cherry", "date"], ["elderberry"]]
```

## Best Practices

### âœ… DO: Use Consistent Case Conversion

```python
# âœ… Good: Consistent API naming
from provide.foundation.formatting import to_snake_case

api_field = to_snake_case(user_input)  # Always snake_case for database

# âŒ Bad: Manual string manipulation
api_field = user_input.replace(" ", "_").lower()  # Inconsistent
```

### âœ… DO: Format User-Facing Numbers

```python
# âœ… Good: Readable numbers
from provide.foundation.formatting import format_number, format_size
from provide.foundation import pout

pout(f"Processed {format_number(1234567)} records")
pout(f"File size: {format_size(1024 * 1024 * 500)}")

# âŒ Bad: Raw numbers
pout(f"Processed {1234567} records")  # Hard to read
```

## Next Steps

- **[Console I/O](../console/console-io.md)**: Output formatted text
- **[Logging](../logging/basic-logging.md)**: Log formatted data

---

**Tip**: Use formatting utilities for consistent, readable output across your application.
>>> EOF >>>

### FILE 24: how-to-guides/http/middleware.md | checksum=a6977468e9e2... | modified=2025-10-24T18:45:20 | op=+ | size=21466 | tokens=4457 | type=markdown ###
<<< BOF <<<
# Custom HTTP Middleware

Learn how to create custom HTTP middleware for logging, authentication, rate limiting, and more.

## Overview

Middleware allows you to intercept and modify HTTP requests and responses, enabling cross-cutting concerns like:
- **Logging** - Track all requests and responses
- **Authentication** - Verify API keys, tokens, or sessions
- **Rate Limiting** - Protect against abuse
- **Caching** - Cache responses for performance
- **Error Handling** - Centralized error responses
- **Request/Response Modification** - Add headers, transform payloads

Foundation's middleware system is compatible with modern Python HTTP clients and servers.

## Prerequisites

Install transport extras:
```bash
pip install "provide-foundation[transport]"
```

## Basic Middleware

### Logging Middleware

Log all HTTP requests and responses:

```python
from provide.foundation.transport import HTTPClient
from provide.foundation import logger

async def logging_middleware(request, call_next):
    """Log all HTTP requests and responses."""
    # Log request
    logger.info(
        "http_request",
        method=request.method,
        url=str(request.url),
        headers=dict(request.headers),
    )

    # Process request
    response = await call_next(request)

    # Log response
    logger.info(
        "http_response",
        status=response.status_code,
        url=str(request.url),
        duration_ms=response.elapsed.total_seconds() * 1000,
    )

    return response

# Use middleware
client = HTTPClient(middleware=[logging_middleware])
response = await client.get("https://api.example.com/data")
```

### Authentication Middleware

Add authentication headers to all requests:

```python
async def auth_middleware(request, call_next):
    """Add authentication header to all requests."""
    # Get token (from config, env, etc.)
    token = get_api_token()

    # Add authorization header
    request.headers["Authorization"] = f"Bearer {token}"

    return await call_next(request)

# Use middleware
client = HTTPClient(middleware=[auth_middleware])
```

### Timing Middleware

Measure request duration:

```python
import time

async def timing_middleware(request, call_next):
    """Measure request duration."""
    start_time = time.time()

    response = await call_next(request)

    duration = time.time() - start_time
    logger.info(
        "request_timing",
        url=str(request.url),
        duration_seconds=duration,
        status=response.status_code,
    )

    return response
```

## Authentication Patterns

### API Key Authentication

Add API key to headers or query parameters:

```python
class APIKeyAuth:
    """API key authentication middleware."""

    def __init__(self, api_key, header_name="X-API-Key"):
        self.api_key = api_key
        self.header_name = header_name

    async def __call__(self, request, call_next):
        """Add API key to request."""
        request.headers[self.header_name] = self.api_key
        return await call_next(request)

# Usage
api_key_auth = APIKeyAuth(api_key="secret-key-123")
client = HTTPClient(middleware=[api_key_auth])
```

### OAuth2 Bearer Token

Handle OAuth2 bearer token authentication:

```python
from datetime import datetime, timedelta

class OAuth2Middleware:
    """OAuth2 bearer token authentication."""

    def __init__(self, token_url, client_id, client_secret):
        self.token_url = token_url
        self.client_id = client_id
        self.client_secret = client_secret
        self.token = None
        self.token_expiry = None

    async def get_token(self):
        """Get or refresh access token."""
        # Check if token is still valid
        if self.token and self.token_expiry > datetime.now():
            return self.token

        # Request new token
        response = await httpx.post(
            self.token_url,
            data={
                "grant_type": "client_credentials",
                "client_id": self.client_id,
                "client_secret": self.client_secret,
            },
        )
        data = response.json()

        # Store token and expiry
        self.token = data["access_token"]
        expires_in = data.get("expires_in", 3600)
        self.token_expiry = datetime.now() + timedelta(seconds=expires_in)

        logger.info("OAuth2 token refreshed", expires_in=expires_in)
        return self.token

    async def __call__(self, request, call_next):
        """Add bearer token to request."""
        token = await self.get_token()
        request.headers["Authorization"] = f"Bearer {token}"
        return await call_next(request)

# Usage
oauth = OAuth2Middleware(
    token_url="https://auth.example.com/oauth/token",
    client_id="my-client-id",
    client_secret="my-client-secret",
)
client = HTTPClient(middleware=[oauth])
```

### JWT Token with Refresh

Automatically refresh JWT tokens:

```python
import jwt
from datetime import datetime

class JWTAuthMiddleware:
    """JWT authentication with automatic refresh."""

    def __init__(self, get_token_func, refresh_token_func):
        self.get_token = get_token_func
        self.refresh_token = refresh_token_func
        self.current_token = None

    def is_token_expired(self, token):
        """Check if JWT token is expired."""
        try:
            payload = jwt.decode(token, options={"verify_signature": False})
            exp = payload.get("exp")
            if exp:
                return datetime.fromtimestamp(exp) < datetime.now()
        except jwt.DecodeError:
            return True
        return False

    async def __call__(self, request, call_next):
        """Add JWT token, refreshing if needed."""
        # Get current token
        if not self.current_token:
            self.current_token = await self.get_token()

        # Refresh if expired
        if self.is_token_expired(self.current_token):
            logger.info("JWT token expired, refreshing")
            self.current_token = await self.refresh_token()

        # Add token to request
        request.headers["Authorization"] = f"Bearer {self.current_token}"
        return await call_next(request)
```

## Request Modification

### Add Custom Headers

Add standard headers to all requests:

```python
async def custom_headers_middleware(request, call_next):
    """Add custom headers to all requests."""
    request.headers["User-Agent"] = "MyApp/1.0"
    request.headers["X-Client-Version"] = "2.5.0"
    request.headers["X-Request-ID"] = str(uuid.uuid4())

    return await call_next(request)
```

### Request Body Transformation

Modify request payloads:

```python
import json

async def request_transform_middleware(request, call_next):
    """Transform request body."""
    if request.method in ["POST", "PUT", "PATCH"]:
        # Read current body
        body = json.loads(request.content)

        # Add metadata
        body["client_version"] = "1.0.0"
        body["timestamp"] = datetime.now().isoformat()

        # Update request body
        request.content = json.dumps(body).encode()
        request.headers["Content-Length"] = str(len(request.content))

    return await call_next(request)
```

### Query Parameter Injection

Add query parameters to all requests:

```python
from urllib.parse import urlencode, urlparse, parse_qs

async def query_param_middleware(request, call_next):
    """Add query parameters to all requests."""
    # Parse current URL
    parsed = urlparse(str(request.url))
    params = parse_qs(parsed.query)

    # Add new parameters
    params["api_version"] = ["2"]
    params["format"] = ["json"]

    # Rebuild URL
    new_query = urlencode(params, doseq=True)
    request.url = request.url.copy_with(query=new_query.encode())

    return await call_next(request)
```

## Response Handling

### Response Transformation

Modify response data:

```python
async def response_transform_middleware(request, call_next):
    """Transform response data."""
    response = await call_next(request)

    # Only transform JSON responses
    if "application/json" in response.headers.get("content-type", ""):
        data = response.json()

        # Add metadata
        transformed = {
            "data": data,
            "meta": {
                "request_id": response.headers.get("x-request-id"),
                "timestamp": datetime.now().isoformat(),
            },
        }

        # Create new response with transformed data
        response._content = json.dumps(transformed).encode()

    return response
```

### Error Response Handling

Standardize error responses:

```python
async def error_handling_middleware(request, call_next):
    """Handle and transform error responses."""
    try:
        response = await call_next(request)

        # Check for error status codes
        if response.status_code >= 400:
            logger.error(
                "http_error",
                status=response.status_code,
                url=str(request.url),
                response_body=response.text[:200],
            )

            # Optionally transform error response
            if response.status_code >= 500:
                # Server error
                raise ServerError(f"Server error: {response.status_code}")

        return response

    except httpx.RequestError as e:
        logger.exception("request_failed", url=str(request.url))
        raise NetworkError(f"Request failed: {e}") from e
```

## Rate Limiting

### Simple Rate Limiter

Implement client-side rate limiting:

```python
import asyncio
from collections import deque
from datetime import datetime, timedelta

class RateLimitMiddleware:
    """Client-side rate limiting middleware."""

    def __init__(self, max_requests=100, time_window=60):
        self.max_requests = max_requests
        self.time_window = timedelta(seconds=time_window)
        self.requests = deque()

    async def __call__(self, request, call_next):
        """Enforce rate limit."""
        now = datetime.now()

        # Remove old requests outside time window
        cutoff = now - self.time_window
        while self.requests and self.requests[0] < cutoff:
            self.requests.popleft()

        # Check if rate limit exceeded
        if len(self.requests) >= self.max_requests:
            wait_time = (self.requests[0] + self.time_window - now).total_seconds()
            logger.warning(
                "rate_limit_reached",
                wait_seconds=wait_time,
                max_requests=self.max_requests,
            )
            await asyncio.sleep(wait_time)

        # Record this request
        self.requests.append(now)

        return await call_next(request)

# Usage: Max 100 requests per minute
rate_limiter = RateLimitMiddleware(max_requests=100, time_window=60)
client = HTTPClient(middleware=[rate_limiter])
```

### Token Bucket Rate Limiter

More sophisticated rate limiting:

```python
class TokenBucketRateLimiter:
    """Token bucket rate limiting."""

    def __init__(self, rate=10, burst=20):
        """
        Args:
            rate: Tokens added per second
            burst: Maximum bucket size
        """
        self.rate = rate
        self.burst = burst
        self.tokens = burst
        self.last_update = time.time()

    async def acquire(self):
        """Acquire a token, waiting if necessary."""
        while True:
            now = time.time()
            elapsed = now - self.last_update

            # Add new tokens based on elapsed time
            self.tokens = min(
                self.burst,
                self.tokens + elapsed * self.rate
            )
            self.last_update = now

            # Try to acquire token
            if self.tokens >= 1:
                self.tokens -= 1
                return

            # Wait for next token
            wait_time = (1 - self.tokens) / self.rate
            await asyncio.sleep(wait_time)

    async def __call__(self, request, call_next):
        """Rate limit using token bucket."""
        await self.acquire()
        return await call_next(request)

# Usage: 10 requests/sec with burst of 20
rate_limiter = TokenBucketRateLimiter(rate=10, burst=20)
client = HTTPClient(middleware=[rate_limiter])
```

## Retry Middleware

Automatically retry failed requests:

```python
from provide.foundation.resilience import BackoffStrategy

class RetryMiddleware:
    """Retry failed requests with exponential backoff."""

    def __init__(self, max_retries=3, base_delay=1.0):
        self.max_retries = max_retries
        self.base_delay = base_delay

    async def __call__(self, request, call_next):
        """Retry request on failure."""
        last_exception = None

        for attempt in range(self.max_retries + 1):
            try:
                response = await call_next(request)

                # Retry on 5xx errors
                if response.status_code < 500:
                    return response

                logger.warning(
                    "server_error_retrying",
                    status=response.status_code,
                    attempt=attempt + 1,
                    max_retries=self.max_retries,
                )

            except httpx.RequestError as e:
                last_exception = e
                logger.warning(
                    "request_error_retrying",
                    error=str(e),
                    attempt=attempt + 1,
                    max_retries=self.max_retries,
                )

            # Don't sleep after last attempt
            if attempt < self.max_retries:
                # Exponential backoff
                delay = self.base_delay * (2 ** attempt)
                await asyncio.sleep(delay)

        # All retries failed
        if last_exception:
            raise last_exception
        return response  # Return last error response

# Usage
retry_middleware = RetryMiddleware(max_retries=3, base_delay=1.0)
client = HTTPClient(middleware=[retry_middleware])
```

## Caching Middleware

Cache responses for performance:

```python
from datetime import datetime, timedelta

class CachingMiddleware:
    """Cache HTTP responses."""

    def __init__(self, ttl=300):
        """
        Args:
            ttl: Cache time-to-live in seconds
        """
        self.ttl = timedelta(seconds=ttl)
        self.cache = {}

    def _cache_key(self, request):
        """Generate cache key from request."""
        return f"{request.method}:{str(request.url)}"

    async def __call__(self, request, call_next):
        """Cache GET requests."""
        # Only cache GET requests
        if request.method != "GET":
            return await call_next(request)

        cache_key = self._cache_key(request)

        # Check cache
        if cache_key in self.cache:
            response, timestamp = self.cache[cache_key]
            age = datetime.now() - timestamp

            if age < self.ttl:
                logger.debug("cache_hit", url=str(request.url), age_seconds=age.total_seconds())
                return response

        # Cache miss - make request
        logger.debug("cache_miss", url=str(request.url))
        response = await call_next(request)

        # Cache successful responses
        if response.status_code == 200:
            self.cache[cache_key] = (response, datetime.now())

        return response

# Usage: Cache for 5 minutes
cache = CachingMiddleware(ttl=300)
client = HTTPClient(middleware=[cache])
```

## Middleware Composition

Combine multiple middleware:

```python
from provide.foundation.transport import HTTPClient
from provide.foundation import logger

# Create individual middleware
logging_mw = logging_middleware
auth_mw = auth_middleware
retry_mw = RetryMiddleware(max_retries=3)
cache_mw = CachingMiddleware(ttl=300)

# Compose middleware stack (order matters!)
client = HTTPClient(
    middleware=[
        logging_mw,    # 1. Log first (outermost)
        auth_mw,       # 2. Add auth
        cache_mw,      # 3. Check cache
        retry_mw,      # 4. Retry on failure (innermost)
    ]
)

# Middleware execution order:
# Request:  logging â†’ auth â†’ cache â†’ retry â†’ HTTP call
# Response: HTTP call â†’ retry â†’ cache â†’ auth â†’ logging
```

## Production Patterns

### Correlation ID Middleware

Track requests across services:

```python
import uuid
from contextvars import ContextVar

correlation_id_var: ContextVar[str] = ContextVar("correlation_id")

async def correlation_id_middleware(request, call_next):
    """Add correlation ID to requests."""
    # Get or generate correlation ID
    correlation_id = correlation_id_var.get(None) or str(uuid.uuid4())

    # Add to request headers
    request.headers["X-Correlation-ID"] = correlation_id

    # Make request
    response = await call_next(request)

    # Log for tracing
    logger.info(
        "http_request_completed",
        correlation_id=correlation_id,
        status=response.status_code,
    )

    return response
```

### Circuit Breaker Middleware

Protect against cascading failures:

```python
from provide.foundation.resilience import CircuitBreaker, CircuitBreakerOpen

class CircuitBreakerMiddleware:
    """Circuit breaker for HTTP requests."""

    def __init__(self, failure_threshold=5, timeout=60):
        self.circuit = CircuitBreaker(
            failure_threshold=failure_threshold,
            timeout=timeout,
        )

    async def __call__(self, request, call_next):
        """Protect request with circuit breaker."""
        @self.circuit.protect
        async def make_request():
            response = await call_next(request)
            # Consider 5xx as failures
            if response.status_code >= 500:
                raise ServerError(f"Server error: {response.status_code}")
            return response

        try:
            return await make_request()
        except CircuitBreakerOpen:
            logger.error("circuit_breaker_open", url=str(request.url))
            # Return cached response or error
            raise ServiceUnavailable("Service circuit breaker is open")
```

### Metrics Collection Middleware

Track HTTP metrics:

```python
from provide.foundation.metrics import Counter, Histogram

# Define metrics
http_requests_total = Counter(
    "http_requests_total",
    labels=["method", "status", "endpoint"]
)
http_request_duration = Histogram(
    "http_request_duration_seconds",
    labels=["method", "endpoint"]
)

async def metrics_middleware(request, call_next):
    """Collect HTTP metrics."""
    start_time = time.time()

    try:
        response = await call_next(request)
        status = response.status_code
    except Exception:
        status = 0  # Failed request
        raise
    finally:
        duration = time.time() - start_time

        # Record metrics
        http_requests_total.increment(labels={
            "method": request.method,
            "status": str(status),
            "endpoint": str(request.url.path),
        })

        http_request_duration.observe(
            duration,
            labels={
                "method": request.method,
                "endpoint": str(request.url.path),
            },
        )

    return response
```

## Best Practices

### âœ… DO: Order Middleware Correctly

```python
# âœ… Good: Logical order
client = HTTPClient(
    middleware=[
        logging_middleware,      # Log everything
        auth_middleware,          # Add auth
        caching_middleware,       # Check cache
        retry_middleware,         # Retry failures
    ]
)

# âŒ Bad: Cache before auth
client = HTTPClient(
    middleware=[
        caching_middleware,  # Cache without auth!
        auth_middleware,
    ]
)
```

### âœ… DO: Handle Errors Gracefully

```python
# âœ… Good: Error handling
async def safe_middleware(request, call_next):
    try:
        return await call_next(request)
    except Exception as e:
        logger.exception("middleware_error")
        raise  # Re-raise after logging
```

### âœ… DO: Log Middleware Actions

```python
# âœ… Good: Visibility
async def transparent_middleware(request, call_next):
    logger.debug("middleware_processing", url=str(request.url))
    response = await call_next(request)
    logger.debug("middleware_completed", status=response.status_code)
    return response
```

### âŒ DON'T: Modify Response Objects Directly

```python
# âŒ Bad: Mutating response
async def bad_middleware(request, call_next):
    response = await call_next(request)
    response.status_code = 200  # Don't mutate!
    return response

# âœ… Good: Create new response if needed
async def good_middleware(request, call_next):
    response = await call_next(request)
    if response.status_code == 404:
        return httpx.Response(
            status_code=200,
            json={"error": "Not found"},
        )
    return response
```

## Next Steps

### Related Guides
- **[Making Requests](requests.md)**: HTTP request patterns
- **[Retry Patterns](../resilience/retry.md)**: Retry logic
- **[Circuit Breakers](../resilience/circuit-breaker.md)**: Circuit breaker pattern

### Examples
- See `examples/transport/01_http_client.py` for middleware examples
- See `examples/production/` for production middleware patterns

### API Reference
- **[API Reference: Transport](../../reference/provide/foundation/transport/index.md)**: Complete API documentation

---

**Tip**: Keep middleware focused on a single responsibility. Combine multiple simple middleware instead of creating one complex middleware that does everything.
>>> EOF >>>

### FILE 25: how-to-guides/http/requests.md | checksum=ad96fcd88875... | modified=2025-10-24T18:49:50 | op=+ | size=19913 | tokens=4283 | type=markdown ###
<<< BOF <<<
# Making HTTP Requests

Learn how to make HTTP requests with Foundation's transport client built on httpx.

## Overview

Foundation provides a powerful HTTP client with automatic retries, logging, middleware support, and async/await patterns. Built on httpx, it adds production-ready features while maintaining a familiar API.

**Key features:**
- Async-first with httpx backend
- Automatic request/response logging
- Middleware support for cross-cutting concerns
- Connection pooling and keep-alive
- Timeout management
- Streaming support for large responses

## Prerequisites

Install transport extras:
```bash
pip install "provide-foundation[transport]"
```

## Basic Requests

### GET Request

Fetch data from an API:

```python
from provide.foundation.transport import HTTPClient

async def fetch_data():
    """Simple GET request."""
    async with HTTPClient() as client:
        response = await client.get("https://api.example.com/data")
        return response.json()

# Use in async context
data = await fetch_data()
```

### POST Request

Send data to an API:

```python
async def create_user(name: str, email: str):
    """Create a new user."""
    async with HTTPClient() as client:
        response = await client.post(
            "https://api.example.com/users",
            json={"name": name, "email": email}
        )
        return response.json()
```

### PUT and PATCH Requests

Update existing resources:

```python
# Full update with PUT
async def update_user(user_id: str, user_data: dict):
    """Update user with PUT."""
    async with HTTPClient() as client:
        response = await client.put(
            f"https://api.example.com/users/{user_id}",
            json=user_data
        )
        return response.json()

# Partial update with PATCH
async def patch_user(user_id: str, changes: dict):
    """Partially update user with PATCH."""
    async with HTTPClient() as client:
        response = await client.patch(
            f"https://api.example.com/users/{user_id}",
            json=changes
        )
        return response.json()
```

### DELETE Request

Remove resources:

```python
async def delete_user(user_id: str):
    """Delete a user."""
    async with HTTPClient() as client:
        response = await client.delete(
            f"https://api.example.com/users/{user_id}"
        )
        return response.status_code == 204
```

## Headers and Authentication

### Custom Headers

Add headers to requests:

```python
async def authenticated_request():
    """Request with custom headers."""
    headers = {
        "Authorization": "Bearer your-token-here",
        "Content-Type": "application/json",
        "User-Agent": "MyApp/1.0",
        "X-API-Version": "2.0"
    }

    async with HTTPClient() as client:
        response = await client.get(
            "https://api.example.com/protected",
            headers=headers
        )
        return response.json()
```

### Bearer Token Authentication

Common authentication pattern:

```python
from provide.foundation.config import get_config

async def call_authenticated_api(endpoint: str):
    """Call API with bearer token."""
    config = get_config()

    headers = {
        "Authorization": f"Bearer {config.api_token}"
    }

    async with HTTPClient() as client:
        response = await client.get(
            f"https://api.example.com{endpoint}",
            headers=headers
        )
        return response.json()
```

### Basic Authentication

HTTP Basic Auth:

```python
import base64

async def basic_auth_request(username: str, password: str):
    """Request with HTTP Basic Auth."""
    # Encode credentials
    credentials = base64.b64encode(f"{username}:{password}".encode()).decode()

    headers = {
        "Authorization": f"Basic {credentials}"
    }

    async with HTTPClient() as client:
        response = await client.get(
            "https://api.example.com/data",
            headers=headers
        )
        return response.json()
```

## Query Parameters

### Simple Query Parameters

Add query parameters to requests:

```python
async def search_users(query: str, limit: int = 10):
    """Search users with query parameters."""
    params = {
        "q": query,
        "limit": limit,
        "sort": "created_at",
        "order": "desc"
    }

    async with HTTPClient() as client:
        response = await client.get(
            "https://api.example.com/users/search",
            params=params
        )
        return response.json()
```

### Complex Query Parameters

Handle lists and nested parameters:

```python
async def filter_items(categories: list[str], tags: list[str]):
    """Filter with multiple values."""
    # httpx handles lists automatically
    params = {
        "category": categories,  # ?category=a&category=b
        "tag": tags,
        "include_archived": "false"
    }

    async with HTTPClient() as client:
        response = await client.get(
            "https://api.example.com/items",
            params=params
        )
        return response.json()
```

## Request Body Formats

### JSON Payload

Most common API format:

```python
async def create_order(items: list[dict], customer_id: str):
    """Create order with JSON payload."""
    payload = {
        "customer_id": customer_id,
        "items": items,
        "total": sum(item["price"] for item in items),
        "currency": "USD"
    }

    async with HTTPClient() as client:
        response = await client.post(
            "https://api.example.com/orders",
            json=payload  # Automatically sets Content-Type: application/json
        )
        return response.json()
```

### Form Data

Send form-encoded data:

```python
async def login(username: str, password: str):
    """Login with form data."""
    data = {
        "username": username,
        "password": password,
        "grant_type": "password"
    }

    async with HTTPClient() as client:
        response = await client.post(
            "https://api.example.com/auth/token",
            data=data  # Sends as application/x-www-form-urlencoded
        )
        return response.json()
```

### Multipart Form Data

Upload files with other fields:

```python
async def upload_profile(user_id: str, avatar_path: str, bio: str):
    """Upload file with multipart form data."""
    files = {
        "avatar": open(avatar_path, "rb")
    }

    data = {
        "user_id": user_id,
        "bio": bio
    }

    async with HTTPClient() as client:
        response = await client.post(
            "https://api.example.com/profile/upload",
            files=files,
            data=data
        )
        return response.json()
```

## File Operations

### Upload Single File

Upload a file to an API:

```python
from pathlib import Path

async def upload_file(file_path: Path):
    """Upload a single file."""
    with open(file_path, "rb") as f:
        files = {"file": (file_path.name, f, "application/octet-stream")}

        async with HTTPClient() as client:
            response = await client.post(
                "https://api.example.com/upload",
                files=files
            )
            return response.json()
```

### Upload Multiple Files

Upload several files at once:

```python
async def upload_multiple_files(file_paths: list[Path]):
    """Upload multiple files."""
    files = [
        ("files", (path.name, open(path, "rb"), "application/octet-stream"))
        for path in file_paths
    ]

    async with HTTPClient() as client:
        response = await client.post(
            "https://api.example.com/bulk-upload",
            files=files
        )
        return response.json()
```

### Download File

Download and save a file:

```python
async def download_file(url: str, save_path: Path):
    """Download file to disk."""
    async with HTTPClient() as client:
        response = await client.get(url)

        # Save to file
        save_path.write_bytes(response.content)

        return save_path

# Usage
await download_file(
    "https://example.com/report.pdf",
    Path("downloads/report.pdf")
)
```

### Stream Large Files

Download large files efficiently:

```python
async def download_large_file(url: str, save_path: Path):
    """Download large file with streaming."""
    async with HTTPClient() as client:
        async with client.stream("GET", url) as response:
            with open(save_path, "wb") as f:
                async for chunk in response.aiter_bytes(chunk_size=8192):
                    f.write(chunk)

    return save_path
```

## Timeouts

### Configure Timeouts

Set request timeouts:

```python
from httpx import Timeout

async def request_with_timeout():
    """Request with custom timeout."""
    timeout = Timeout(
        connect=5.0,  # Max time to establish connection
        read=30.0,    # Max time to read response
        write=10.0,   # Max time to send request
        pool=5.0      # Max time to get connection from pool
    )

    async with HTTPClient(timeout=timeout) as client:
        response = await client.get("https://api.example.com/data")
        return response.json()
```

### Per-Request Timeout

Override timeout for specific requests:

```python
async def quick_health_check():
    """Health check with short timeout."""
    async with HTTPClient() as client:
        try:
            response = await client.get(
                "https://api.example.com/health",
                timeout=2.0  # 2 second timeout
            )
            return response.status_code == 200
        except httpx.TimeoutException:
            return False
```

## Error Handling

### Handle HTTP Errors

Catch and handle HTTP errors:

```python
from provide.foundation import logger
import httpx

async def safe_api_call(url: str):
    """API call with comprehensive error handling."""
    async with HTTPClient() as client:
        try:
            response = await client.get(url)
            response.raise_for_status()  # Raises for 4xx/5xx
            return response.json()

        except httpx.HTTPStatusError as e:
            logger.error(
                "http_error",
                status=e.response.status_code,
                url=str(url),
                response=e.response.text[:200]
            )
            raise

        except httpx.TimeoutException:
            logger.error("request_timeout", url=str(url))
            raise

        except httpx.RequestError as e:
            logger.error("request_failed", url=str(url), error=str(e))
            raise
```

### Retry Failed Requests

Automatically retry transient failures:

```python
from provide.foundation.resilience import retry, NetworkError

@retry(
    (httpx.TimeoutException, httpx.NetworkError),
    max_attempts=3,
    base_delay=1.0
)
async def resilient_request(url: str):
    """Request with automatic retries."""
    async with HTTPClient() as client:
        response = await client.get(url)

        # Retry on server errors
        if response.status_code >= 500:
            raise NetworkError(f"Server error: {response.status_code}")

        return response.json()
```

## Response Handling

### Check Status Codes

Validate response status:

```python
async def check_response_status():
    """Handle different status codes."""
    async with HTTPClient() as client:
        response = await client.get("https://api.example.com/data")

        if response.status_code == 200:
            return response.json()
        elif response.status_code == 404:
            logger.warning("resource_not_found")
            return None
        elif response.status_code >= 500:
            logger.error("server_error", status=response.status_code)
            raise ServerError()
        else:
            response.raise_for_status()
```

### Parse Response Formats

Handle different content types:

```python
async def parse_response(url: str):
    """Parse response based on content type."""
    async with HTTPClient() as client:
        response = await client.get(url)

        content_type = response.headers.get("content-type", "")

        if "application/json" in content_type:
            return response.json()
        elif "text/html" in content_type:
            return response.text
        elif "application/xml" in content_type:
            import xml.etree.ElementTree as ET
            return ET.fromstring(response.content)
        else:
            return response.content
```

### Access Response Headers

Read response headers:

```python
async def get_response_headers():
    """Access response headers."""
    async with HTTPClient() as client:
        response = await client.get("https://api.example.com/data")

        # Get specific headers
        content_type = response.headers.get("content-type")
        rate_limit = response.headers.get("x-ratelimit-remaining")
        request_id = response.headers.get("x-request-id")

        logger.info(
            "response_headers",
            content_type=content_type,
            rate_limit=rate_limit,
            request_id=request_id
        )

        return response.json()
```

## Connection Management

### Reuse Client

Reuse client for multiple requests:

```python
class APIClient:
    """Reusable API client."""

    def __init__(self, base_url: str, api_key: str):
        self.base_url = base_url
        self.headers = {"Authorization": f"Bearer {api_key}"}
        self.client = None

    async def __aenter__(self):
        """Create client."""
        self.client = HTTPClient(
            base_url=self.base_url,
            headers=self.headers
        )
        await self.client.__aenter__()
        return self

    async def __aexit__(self, *args):
        """Close client."""
        await self.client.__aexit__(*args)

    async def get_user(self, user_id: str):
        """Get user by ID."""
        response = await self.client.get(f"/users/{user_id}")
        return response.json()

    async def list_users(self, limit: int = 10):
        """List users."""
        response = await self.client.get("/users", params={"limit": limit})
        return response.json()

# Usage
async with APIClient("https://api.example.com", "your-api-key") as api:
    user = await api.get_user("123")
    users = await api.list_users(limit=50)
```

### Connection Pooling

Configure connection pool:

```python
import httpx

async def configure_connection_pool():
    """Client with custom connection pool."""
    limits = httpx.Limits(
        max_keepalive_connections=20,
        max_connections=100,
        keepalive_expiry=30.0
    )

    async with HTTPClient(limits=limits) as client:
        # Make multiple requests efficiently
        tasks = [
            client.get(f"https://api.example.com/item/{i}")
            for i in range(50)
        ]
        responses = await asyncio.gather(*tasks)
        return [r.json() for r in responses]
```

## Streaming Responses

### Stream JSON Lines

Process streaming JSON data:

```python
import json

async def stream_json_lines(url: str):
    """Process streaming JSON lines."""
    async with HTTPClient() as client:
        async with client.stream("GET", url) as response:
            async for line in response.aiter_lines():
                if line:
                    data = json.loads(line)
                    # Process each JSON object
                    yield data
```

### Server-Sent Events (SSE)

Handle server-sent events:

```python
async def subscribe_to_events(url: str):
    """Subscribe to server-sent events."""
    async with HTTPClient() as client:
        async with client.stream("GET", url) as response:
            async for line in response.aiter_lines():
                if line.startswith("data: "):
                    event_data = line[6:]  # Remove "data: " prefix
                    yield json.loads(event_data)
```

## Best Practices

### âœ… DO: Use Context Managers

```python
# âœ… Good: Automatic cleanup
async def good_request():
    async with HTTPClient() as client:
        return await client.get("https://api.example.com")

# âŒ Bad: Manual cleanup required
async def bad_request():
    client = HTTPClient()
    response = await client.get("https://api.example.com")
    await client.aclose()  # Easy to forget!
    return response
```

### âœ… DO: Set Appropriate Timeouts

```python
# âœ… Good: Reasonable timeout
async with HTTPClient(timeout=30.0) as client:
    response = await client.get(url)

# âŒ Bad: No timeout (can hang forever)
async with HTTPClient(timeout=None) as client:
    response = await client.get(url)
```

### âœ… DO: Handle Errors Gracefully

```python
# âœ… Good: Comprehensive error handling
try:
    response = await client.get(url)
    response.raise_for_status()
    return response.json()
except httpx.HTTPStatusError as e:
    logger.error("http_error", status=e.response.status_code)
    return None
except httpx.TimeoutException:
    logger.error("timeout", url=url)
    return None
```

### âœ… DO: Reuse Clients

```python
# âœ… Good: Reuse client for multiple requests
async with HTTPClient() as client:
    user = await client.get("/users/1")
    posts = await client.get("/users/1/posts")
    comments = await client.get("/users/1/comments")

# âŒ Bad: Create new client for each request
for i in range(100):
    async with HTTPClient() as client:  # Wasteful!
        await client.get(f"/items/{i}")
```

### âŒ DON'T: Ignore Status Codes

```python
# âŒ Bad: Assuming success
response = await client.get(url)
data = response.json()  # Might fail if status is 404!

# âœ… Good: Check status
response = await client.get(url)
if response.status_code == 200:
    data = response.json()
else:
    logger.error("request_failed", status=response.status_code)
```

## Common Patterns

### Pagination

Handle paginated API responses:

```python
async def fetch_all_pages(base_url: str):
    """Fetch all pages from paginated API."""
    all_items = []
    page = 1

    async with HTTPClient() as client:
        while True:
            response = await client.get(
                base_url,
                params={"page": page, "per_page": 100}
            )
            data = response.json()

            items = data.get("items", [])
            all_items.extend(items)

            # Check if there are more pages
            if not data.get("has_more"):
                break

            page += 1

    return all_items
```

### Rate Limiting

Respect API rate limits:

```python
import asyncio
from datetime import datetime, timedelta

class RateLimitedClient:
    """HTTP client with rate limiting."""

    def __init__(self, requests_per_second=10):
        self.delay = 1.0 / requests_per_second
        self.last_request = None

    async def get(self, url: str):
        """GET request with rate limiting."""
        # Wait if needed
        if self.last_request:
            elapsed = (datetime.now() - self.last_request).total_seconds()
            if elapsed < self.delay:
                await asyncio.sleep(self.delay - elapsed)

        async with HTTPClient() as client:
            response = await client.get(url)
            self.last_request = datetime.now()
            return response
```

## Next Steps

### Related Guides
- **[Custom Middleware](middleware.md)**: Add middleware for auth, logging, retries
- **[Retry Patterns](../resilience/retry.md)**: Automatically retry failed requests
- **[Circuit Breakers](../resilience/circuit-breaker.md)**: Protect against cascading failures

### Examples
- See `examples/transport/01_http_client.py` for comprehensive HTTP client examples
- See `examples/production/` for production HTTP patterns

### API Reference
- **[API Reference: Transport](../../reference/provide/foundation/transport/index.md)**: Complete API documentation

---

**Tip**: Always use async context managers (`async with`) with HTTPClient to ensure proper connection cleanup. Set appropriate timeouts to prevent hanging requests.
>>> EOF >>>

### FILE 26: how-to-guides/logging/basic-logging.md | checksum=6350047ad60d... | modified=2025-10-24T20:05:33 | op=+ | size=15236 | tokens=3533 | type=markdown ###
<<< BOF <<<
# How to Perform Basic Logging

This guide covers the fundamental patterns for logging in a `provide.foundation` application. You'll learn how to log messages at different severity levels, add structured data for context, use emoji-enhanced logging, and configure logging for different environments.

## Quick Start

The simplest way to use Foundation's logger:

```python
# From: examples/telemetry/01_basic_logging.py
from provide.foundation import logger

# Logger auto-initializes on first use - no setup required!
logger.info("Application started")
```

That's it! No configuration files, no complex setup. The logger works out of the box with sensible defaults.

## Logging at Different Severity Levels

The global `logger` instance provides methods for each standard log level:

```python
from provide.foundation import logger

# DEBUG: Diagnostic information for developers
logger.debug("Cache lookup", key="user:123", hit=True)

# INFO: General informational messages (default level)
logger.info("User logged in", user_id="user_123")

# WARNING: Something unexpected happened, but app can continue
logger.warning("API rate limit approaching", current=95, limit=100)

# ERROR: A serious error occurred; an operation failed
logger.error("Failed to connect to database", error="ConnectionRefused")

# CRITICAL: A critical failure that may prevent the application from continuing
logger.critical("Out of memory", available_mb=50, required_mb=500)
```

## Adding Structured Data (Context)

Pass contextual information as keyword arguments. This is the core of **structured logging**:

```python
logger.info(
    "user_session_started",  # Event name
    user_id="user_123",       # Structured fields
    session_id="sess_456",
    source="web_app",
    ip_address="192.168.1.1"
)
```

### Why Structured Logging?

Traditional logging:
```python
# âŒ Hard to parse, search, and analyze
logging.info(f"User user_123 started session sess_456 from web_app")
```

Structured logging:
```python
# âœ… Easy to parse, search, filter, and aggregate
logger.info("session_started", user_id="user_123", session_id="sess_456", source="web_app")
```

Benefits:
- **Searchable**: Find all logs for `user_id="user_123"`
- **Aggregatable**: Count sessions by `source`
- **Analyzable**: Track patterns across fields
- **Machine-readable**: JSON output for log aggregation systems

## Event-Based Logging Pattern

Foundation encourages an **event-based** logging pattern:

```python
# Use event names as the first argument
logger.info("order_created", order_id="order_123", amount=99.99)
logger.info("payment_processed", order_id="order_123", method="credit_card")
logger.info("email_sent", order_id="order_123", recipient="user@example.com")
```

This creates a **timeline of events** that's easy to track and analyze.

### Domain-Action-Status Pattern

For complex operations, use the Domain-Action-Status pattern:

```python
# Domain: database | Action: connect | Status: (implied by log level)
logger.info("database_connect_started", host="localhost", port=5432)
logger.info("database_connect_success", connection_time_ms=150)

# Or explicitly:
logger.error("database_connect_failed", error="timeout", retry_count=3)
```

## Emoji-Enhanced Logging

Foundation supports **emoji prefixes** for visual log parsing (enabled by default in console output):

```python
logger.info("app_started", emoji="ðŸš€")
logger.info("data_loaded", emoji="ðŸ“Š", records=1000)
logger.warning("cache_miss", emoji="âš ï¸", key="user:123")
logger.error("connection_failed", emoji="âŒ", service="database")
```

Output:
```
ðŸš€ app_started
ðŸ“Š data_loaded | records=1000
âš ï¸  cache_miss | key=user:123
âŒ connection_failed | service=database
```

### Disable Emojis

```python
from provide.foundation import get_hub, LoggingConfig, TelemetryConfig

get_hub().initialize_foundation(
    TelemetryConfig(
        logging=LoggingConfig(
            logger_name_emoji_prefix_enabled=False,
            das_emoji_prefix_enabled=False,
        ),
    )
)
```

Or via environment variables:
```bash
export PROVIDE_LOG_LOGGER_NAME_EMOJI_ENABLED=false
export PROVIDE_LOG_DAS_EMOJI_ENABLED=false
```

## Using Named Loggers

For better organization, create named loggers for different components:

```python
# From: examples/telemetry/03_named_loggers.py
from provide.foundation import get_logger

# Create component-specific loggers
auth_logger = get_logger("auth.service")
db_logger = get_logger("database.connection")
api_logger = get_logger("api.handler")

# Each logger includes its name in the output
auth_logger.info("login_success", user_id="user123")
# Output: [auth.service] login_success | user_id=user123

db_logger.info("pool_initialized", pool_size=20)
# Output: [database.connection] pool_initialized | pool_size=20

api_logger.info("request_received", path="/api/users", method="GET")
# Output: [api.handler] request_received | path=/api/users | method=GET
```

### When to Use Named Loggers

- **Large applications**: Separate logs by module or component
- **Libraries**: Use your library name for namespacing
- **Microservices**: Identify which service component generated the log

## Binding Context to a Logger

Use `bind()` to create a logger instance with **persistent context**:

```python
def handle_request(request_id: str, user_id: str):
    # Create a logger with request-specific context
    request_logger = logger.bind(request_id=request_id, user_id=user_id)

    # All logs from this logger will include request_id and user_id
    request_logger.info("request_started")
    # Output: request_started | request_id=abc123 | user_id=user_456

    request_logger.info("database_query", table="users", duration_ms=45)
    # Output: database_query | request_id=abc123 | user_id=user_456 | table=users | duration_ms=45

    request_logger.info("request_completed", status_code=200)
    # Output: request_completed | request_id=abc123 | user_id=user_456 | status_code=200
```

This is extremely useful for:
- **Request tracing**: Track a request through your application
- **User context**: Include user information in all relevant logs
- **Transaction tracking**: Follow a database transaction
- **Distributed tracing**: Correlation IDs across services

## Configuration Options

### Console vs JSON Output

**Console** (default for development):
```python
from provide.foundation import get_hub, LoggingConfig, TelemetryConfig

config = TelemetryConfig(
    logging=LoggingConfig(
        console_formatter="key_value"
    )
)
get_hub().initialize_foundation(config)
```

Output:
```
2025-10-24 14:30:15 [INFO] user_login | user_id=user_123 | ip=192.168.1.1
```

**JSON** (recommended for production):
```python
from provide.foundation import get_hub, LoggingConfig, TelemetryConfig

config = TelemetryConfig(
    logging=LoggingConfig(
        console_formatter="json"
    )
)
get_hub().initialize_foundation(config)
```

Output:
```json
{"timestamp": "2025-10-24T14:30:15.123Z", "level": "info", "event": "user_login", "user_id": "user_123", "ip": "192.168.1.1"}
```

### Set Log Level

```python
# Via initialization
from provide.foundation import get_hub, LoggingConfig, TelemetryConfig

config = TelemetryConfig(
    logging=LoggingConfig(
        default_level="DEBUG"
    )
)
get_hub().initialize_foundation(config)

# Via environment variable
# export PROVIDE_LOG_LEVEL=DEBUG
```

Log levels (in order of severity):
1. **DEBUG**: Detailed diagnostic information
2. **INFO**: General informational messages (default)
3. **WARNING**: Warning messages
4. **ERROR**: Error messages
5. **CRITICAL**: Critical failures

### Module-Level Log Control

Control log levels for specific modules:

```python
from provide.foundation import get_hub, LoggingConfig, TelemetryConfig

config = TelemetryConfig(
    logging=LoggingConfig(
        default_level="INFO",  # Default level
        module_levels={
            "urllib3": "WARNING",      # Suppress urllib3 debug/info
            "asyncio": "INFO",          # Show asyncio info+
            "myapp.database": "DEBUG",  # Verbose logging for database module
        }
    )
)

get_hub().initialize_foundation(config)
```

Or via environment variable:
```bash
export PROVIDE_LOG_MODULE_LEVELS="urllib3:WARNING,asyncio:INFO,myapp.database:DEBUG"
```

## Logging Performance Metrics

Log performance metrics for operations:

```python
import time

def process_data(data_id: str):
    start_time = time.time()
    logger.info("processing_started", data_id=data_id)

    try:
        # Process data
        result = perform_processing(data_id)

        duration_ms = (time.time() - start_time) * 1000
        logger.info(
            "processing_completed",
            data_id=data_id,
            duration_ms=round(duration_ms, 2),
            records_processed=len(result),
            emoji="âœ…"
        )
    except Exception as e:
        duration_ms = (time.time() - start_time) * 1000
        logger.error(
            "processing_failed",
            data_id=data_id,
            duration_ms=round(duration_ms, 2),
            error=str(e),
            emoji="âŒ"
        )
        raise
```

## Logging Errors with Context

Always include context when logging errors:

```python
try:
    result = divide(a, b)
except ZeroDivisionError as e:
    logger.error(
        "division_error",
        numerator=a,
        denominator=b,
        error=str(e),
        error_type=type(e).__name__
    )
    raise
except Exception as e:
    logger.exception(  # Automatically includes stack trace
        "unexpected_error",
        operation="division",
        error=str(e)
    )
    raise
```

For more details on exception logging, see [Exception Logging](exception-logging.md).

## Best Practices

### âœ… DO: Use Event Names

```python
# âœ… Good: Clear event name
logger.info("user_registered", user_id="user_123", source="web")

# âŒ Bad: Generic message
logger.info("Something happened with user_123")
```

### âœ… DO: Include Context

```python
# âœ… Good: Rich context
logger.error("api_request_failed", endpoint="/users", status=500, duration_ms=1234)

# âŒ Bad: No context
logger.error("Request failed")
```

### âœ… DO: Use Consistent Field Names

```python
# âœ… Good: Consistent naming
logger.info("request_started", user_id="user_123", request_id="req_456")
logger.info("request_completed", user_id="user_123", request_id="req_456")

# âŒ Bad: Inconsistent naming
logger.info("request_started", user="user_123", req_id="req_456")
logger.info("request_completed", user_id="user_123", request_id="req_456")
```

### âœ… DO: Log at Appropriate Levels

```python
# âœ… Good: Appropriate levels
logger.debug("cache_lookup", key="user:123", hit=True)  # Diagnostic
logger.info("user_login", user_id="user_123")            # Informational
logger.warning("rate_limit_approached", usage=95)        # Warning
logger.error("database_connection_failed")                # Error

# âŒ Bad: Everything at INFO
logger.info("cache hit")        # Should be DEBUG
logger.info("CONNECTION FAILED")  # Should be ERROR
```

### âœ… DO: Use bind() for Request Context

```python
# âœ… Good: Bind request context
def handle_request(request_id):
    request_logger = logger.bind(request_id=request_id)
    request_logger.info("processing_started")
    request_logger.info("processing_completed")

# âŒ Bad: Repeat context everywhere
def handle_request(request_id):
    logger.info("processing_started", request_id=request_id)
    logger.info("processing_completed", request_id=request_id)
```

### âŒ DON'T: Log Sensitive Data

```python
# âŒ NEVER: Log passwords, tokens, credit cards
logger.info("user_login", password=user_password)  # DANGEROUS!

# âœ… Good: Mask or omit sensitive data
logger.info("user_login", user_id=user_id)  # Safe
```

### âŒ DON'T: Use String Formatting

```python
# âŒ Bad: String formatting loses structure
logger.info(f"User {user_id} logged in from {ip}")

# âœ… Good: Structured fields
logger.info("user_login", user_id=user_id, ip=ip)
```

## Common Patterns

### API Request Logging

```python
@app.route("/api/users/<user_id>")
def get_user(user_id):
    request_logger = logger.bind(
        request_id=request.headers.get("X-Request-ID"),
        endpoint="/api/users",
        method="GET",
        user_id=user_id
    )

    request_logger.info("api_request_received")

    try:
        user = database.get_user(user_id)
        request_logger.info("api_request_success", status=200)
        return jsonify(user), 200
    except UserNotFound:
        request_logger.warning("user_not_found", status=404)
        return jsonify({"error": "not found"}), 404
    except Exception as e:
        request_logger.error("api_request_failed", status=500, error=str(e))
        return jsonify({"error": "internal error"}), 500
```

### Background Task Logging

```python
def process_background_task(task_id, data):
    task_logger = logger.bind(task_id=task_id, task_type="data_processing")

    task_logger.info("task_started", records=len(data))

    processed = 0
    errors = 0

    for record in data:
        try:
            process_record(record)
            processed += 1
        except Exception as e:
            errors += 1
            task_logger.warning("record_processing_failed", record_id=record.id, error=str(e))

    task_logger.info(
        "task_completed",
        total=len(data),
        processed=processed,
        errors=errors,
        success_rate=round(processed / len(data) * 100, 2)
    )
```

### Database Operation Logging

```python
def execute_query(sql, params):
    query_logger = logger.bind(operation="database_query")

    start_time = time.time()
    query_logger.debug("query_started", sql=sql[:100])  # Truncate long queries

    try:
        result = database.execute(sql, params)
        duration_ms = (time.time() - start_time) * 1000

        query_logger.info(
            "query_completed",
            duration_ms=round(duration_ms, 2),
            rows_affected=result.rowcount
        )

        return result
    except Exception as e:
        duration_ms = (time.time() - start_time) * 1000

        query_logger.error(
            "query_failed",
            duration_ms=round(duration_ms, 2),
            error=str(e),
            error_type=type(e).__name__
        )
        raise
```

## Next Steps

### Related Logging Guides
- **[Exception Logging](exception-logging.md)**: Learn how to log exceptions with full context
- **[Structured Events](structured-events.md)**: Advanced patterns for event-driven logging
- **[Custom Processors](custom-processors.md)**: Extend logging with custom processors

### Integration & Production
- **[CLI Commands](../cli/commands.md)**: Use Foundation logging in CLI applications
- **[Production Patterns](../production/monitoring.md)**: Production logging best practices
- **[Configuration](../configuration/env-variables.md)**: Configure logging via environment variables

### Understanding Foundation
- **[Architecture](../../explanation/architecture.md)**: Understand how the logging system works

---

**Tip**: Start with simple `logger.info()` calls and add structure as your needs grow. Foundation makes it easy to evolve from basic to advanced logging patterns.
>>> EOF >>>

### FILE 27: how-to-guides/logging/custom-processors.md | checksum=4551544bae30... | modified=2025-10-24T19:35:50 | op=+ | size=26807 | tokens=5907 | type=markdown ###
<<< BOF <<<
# Custom Log Processors

Learn how to create custom log processors to transform, enrich, filter, and sanitize log events in Foundation applications.

## Overview

Log processors are the heart of Foundation's logging pipeline. They transform log events before they're written, allowing you to add context, filter sensitive data, format output, and implement custom logic. Foundation uses a processor chain where each processor can modify the event dictionary.

**What you'll learn:**
- Create custom processors for event transformation
- Add contextual information to logs
- Filter and sanitize sensitive data
- Implement conditional processing
- Build async processors
- Handle errors in processors
- Test custom processors
- Optimize processor performance

**Key Features:**
- ðŸ”§ **Flexible API**: Simple function-based processor interface
- ðŸ“Š **Event Enrichment**: Add context, metrics, and metadata
- ðŸ”’ **Data Sanitization**: Remove or mask sensitive information
- âš¡ **High Performance**: Minimal overhead, async support
- ðŸŽ¯ **Conditional Logic**: Process based on log level, context, or custom rules
- ðŸ§ª **Testable**: Easy to unit test and mock

## Prerequisites

```bash
# Foundation includes structlog processors
pip install provide-foundation

# For advanced async processing
pip install provide-foundation[async]
```

## Processor Basics

### Simple Processor

A processor is a callable that receives the logger, method name, and event dictionary:

```python
from provide.foundation import get_hub
from provide.foundation.logger.config import TelemetryConfig, LoggingConfig
from provide.foundation import logger

def add_hostname_processor(logger_instance, method_name, event_dict):
    """Add hostname to every log event."""
    import socket
    event_dict["hostname"] = socket.gethostname()
    return event_dict

# Register the processor
hub = get_hub()
hub.initialize_foundation(
    TelemetryConfig(
        service_name="my-app",
        logging=LoggingConfig(
            processors=[add_hostname_processor]
        )
    )
)

# Now all logs include hostname
logger.info("user_login", user_id="123")
# Output: {"event": "user_login", "hostname": "app-server-01", ...}
```

### Processor Signature

Every processor must follow this signature:

```python
from typing import Any

def my_processor(
    logger_instance: Any,  # The logger instance
    method_name: str,      # Method name (debug, info, warning, error)
    event_dict: dict[str, Any]  # Mutable event dictionary
) -> dict[str, Any]:
    """Process a log event.

    Args:
        logger_instance: The logger that created the event
        method_name: The log level method called (info, error, etc.)
        event_dict: The event dictionary to modify

    Returns:
        The modified event dictionary (or raise to suppress)
    """
    # Modify event_dict in place or create new dict
    event_dict["custom_field"] = "custom_value"
    return event_dict
```

## Context Enrichment

### Add Request Context

Add request information to all logs within a request:

```python
import contextvars
from provide.foundation import logger

# Context variable for request ID
request_context = contextvars.ContextVar("request_context", default={})

def add_request_context_processor(logger_instance, method_name, event_dict):
    """Add request context to logs."""
    context = request_context.get()

    if context:
        event_dict["request_id"] = context.get("request_id")
        event_dict["user_id"] = context.get("user_id")
        event_dict["client_ip"] = context.get("client_ip")

    return event_dict

# In your request handler
def handle_request(request):
    """Handle an HTTP request."""
    # Set context for this request
    request_context.set({
        "request_id": request.headers.get("X-Request-ID"),
        "user_id": request.user.id,
        "client_ip": request.remote_addr
    })

    # All logs in this context will include request info
    logger.info("request_started", path=request.path)
    process_request(request)
    logger.info("request_completed", status=200)
```

### Add Application Metadata

Include application version, environment, and deployment info:

```python
import os
from datetime import datetime
from provide.foundation import logger

# Load at startup
APP_METADATA = {
    "version": os.getenv("APP_VERSION", "unknown"),
    "environment": os.getenv("ENVIRONMENT", "dev"),
    "deployed_at": datetime.now().isoformat(),
    "region": os.getenv("AWS_REGION", "local")
}

def add_app_metadata_processor(logger_instance, method_name, event_dict):
    """Add application metadata to logs."""
    event_dict.update({
        "app_version": APP_METADATA["version"],
        "environment": APP_METADATA["environment"],
        "region": APP_METADATA["region"]
    })
    return event_dict

# All logs now include app context
logger.info("application_started")
# {"event": "application_started", "app_version": "1.2.3", "environment": "production", ...}
```

### Add Performance Metrics

Track performance metrics in logs:

```python
import time
from provide.foundation import logger

def add_duration_processor(logger_instance, method_name, event_dict):
    """Calculate duration if start_time is present."""
    if "start_time" in event_dict:
        duration_ms = (time.time() - event_dict["start_time"]) * 1000
        event_dict["duration_ms"] = round(duration_ms, 2)
        # Remove start_time from output
        del event_dict["start_time"]

    return event_dict

# Usage
start = time.time()
process_data()
logger.info("data_processed", start_time=start, records=1000)
# {"event": "data_processed", "duration_ms": 123.45, "records": 1000}
```

## Data Sanitization

### Filter Sensitive Data

Remove or mask sensitive information from logs:

```python
import re
from provide.foundation import logger

# Patterns for sensitive data
SENSITIVE_KEYS = {"password", "api_key", "token", "secret", "credit_card"}
EMAIL_PATTERN = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
CREDIT_CARD_PATTERN = re.compile(r'\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b')

def sanitize_sensitive_data_processor(logger_instance, method_name, event_dict):
    """Remove or mask sensitive data."""

    def sanitize_value(key: str, value: Any) -> Any:
        """Sanitize a single value."""
        # Mask sensitive keys
        if isinstance(key, str) and any(s in key.lower() for s in SENSITIVE_KEYS):
            return "***REDACTED***"

        # Mask email addresses in strings
        if isinstance(value, str):
            value = EMAIL_PATTERN.sub("***@***.***", value)
            value = CREDIT_CARD_PATTERN.sub("****-****-****-****", value)

        # Recursively sanitize dicts
        elif isinstance(value, dict):
            return {k: sanitize_value(k, v) for k, v in value.items()}

        # Recursively sanitize lists
        elif isinstance(value, list):
            return [sanitize_value("", v) for v in value]

        return value

    # Sanitize all event fields
    for key in list(event_dict.keys()):
        event_dict[key] = sanitize_value(key, event_dict[key])

    return event_dict

# Usage
logger.info(
    "user_created",
    user_id="123",
    email="user@example.com",
    password="secret123",  # Will be redacted
    api_key="sk_live_abc123"  # Will be redacted
)
# Output: {"event": "user_created", "email": "***@***.***", "password": "***REDACTED***", ...}
```

### PII Masking

Mask personally identifiable information:

```python
from provide.foundation import logger

def mask_pii_processor(logger_instance, method_name, event_dict):
    """Mask PII fields."""
    pii_fields = {"ssn", "tax_id", "passport", "drivers_license"}

    for field in pii_fields:
        if field in event_dict and event_dict[field]:
            value = str(event_dict[field])
            # Show last 4 digits only
            if len(value) > 4:
                event_dict[field] = "*" * (len(value) - 4) + value[-4:]

    return event_dict

# Usage
logger.info("identity_verified", ssn="123-45-6789")
# Output: {"event": "identity_verified", "ssn": "*****6789"}
```

## Conditional Processing

### Log Level Filtering

Apply processors only for specific log levels:

```python
from provide.foundation.logger.levels import LogLevel
from provide.foundation import logger

def production_only_processor(logger_instance, method_name, event_dict):
    """Only process in production environment."""
    import os

    if os.getenv("ENVIRONMENT") != "production":
        return event_dict

    # Add production-specific context
    event_dict["environment"] = "production"
    event_dict["region"] = os.getenv("AWS_REGION", "unknown")
    event_dict["alert_pagerduty"] = method_name in ("error", "critical")

    return event_dict

def error_only_enrichment(logger_instance, method_name, event_dict):
    """Add extra context for errors."""
    if method_name not in ("error", "critical"):
        return event_dict

    # Add debugging context for errors
    import sys
    event_dict["python_version"] = sys.version
    event_dict["severity"] = "HIGH" if method_name == "critical" else "MEDIUM"

    return event_dict
```

### Sampling Processor

Sample high-volume logs to reduce noise:

```python
import random
from provide.foundation import logger

class SamplingProcessor:
    """Sample logs based on rate."""

    def __init__(self, sample_rate: float = 0.1):
        """Initialize with sample rate (0.0-1.0)."""
        self.sample_rate = sample_rate

    def __call__(self, logger_instance, method_name, event_dict):
        """Sample the log event."""
        # Always log errors and warnings
        if method_name in ("error", "critical", "warning"):
            return event_dict

        # Sample info and debug logs
        if random.random() > self.sample_rate:
            # Suppress this log by raising DropEvent
            from structlog.exceptions import DropEvent
            raise DropEvent

        event_dict["sampled"] = True
        return event_dict

# Use with 10% sampling for info logs
sampler = SamplingProcessor(sample_rate=0.1)
```

## Advanced Processors

### Async Processor

Process events asynchronously for expensive operations:

```python
import asyncio
from provide.foundation import logger

class AsyncEnrichmentProcessor:
    """Async processor for expensive enrichment."""

    def __init__(self):
        self.cache = {}

    async def enrich_user_data(self, user_id: str) -> dict:
        """Fetch user data (simulated)."""
        if user_id in self.cache:
            return self.cache[user_id]

        # Simulate async API call
        await asyncio.sleep(0.01)
        user_data = {"name": f"User{user_id}", "role": "member"}

        self.cache[user_id] = user_data
        return user_data

    def __call__(self, logger_instance, method_name, event_dict):
        """Add user data if user_id present."""
        user_id = event_dict.get("user_id")

        if not user_id:
            return event_dict

        # Run async enrichment
        try:
            loop = asyncio.get_event_loop()
            user_data = loop.run_until_complete(
                self.enrich_user_data(user_id)
            )
            event_dict["user_name"] = user_data["name"]
            event_dict["user_role"] = user_data["role"]
        except Exception as e:
            event_dict["enrichment_error"] = str(e)

        return event_dict
```

### Batching Processor

Buffer events for batch processing:

```python
from queue import Queue
from threading import Thread, Event
import time
from provide.foundation import logger

class BatchingProcessor:
    """Buffer and process logs in batches."""

    def __init__(self, batch_size: int = 100, flush_interval: float = 5.0):
        self.batch_size = batch_size
        self.flush_interval = flush_interval
        self.queue: Queue = Queue()
        self.stop_event = Event()

        # Start background thread
        self.thread = Thread(target=self._process_batches, daemon=True)
        self.thread.start()

    def _process_batches(self):
        """Process batches in background."""
        batch = []
        last_flush = time.time()

        while not self.stop_event.is_set():
            try:
                # Get event with timeout
                event = self.queue.get(timeout=1.0)
                batch.append(event)

                # Flush if batch is full or interval elapsed
                should_flush = (
                    len(batch) >= self.batch_size or
                    time.time() - last_flush >= self.flush_interval
                )

                if should_flush and batch:
                    self._flush_batch(batch)
                    batch = []
                    last_flush = time.time()

            except Exception:
                continue

        # Flush remaining
        if batch:
            self._flush_batch(batch)

    def _flush_batch(self, batch: list[dict]):
        """Send batch to external service."""
        logger.debug("batch_flushed", count=len(batch))
        # Send to log aggregation service
        # send_to_datadog(batch)

    def __call__(self, logger_instance, method_name, event_dict):
        """Queue event for batching."""
        self.queue.put(event_dict.copy())
        return event_dict

    def shutdown(self):
        """Stop background processing."""
        self.stop_event.set()
        self.thread.join(timeout=10)
```

### Metric Collection Processor

Collect metrics from log events:

```python
from collections import defaultdict
from threading import Lock
from provide.foundation import logger

class MetricsProcessor:
    """Collect metrics from log events."""

    def __init__(self):
        self.counters = defaultdict(int)
        self.timers = defaultdict(list)
        self.lock = Lock()

    def __call__(self, logger_instance, method_name, event_dict):
        """Extract metrics from events."""
        event_name = event_dict.get("event", "unknown")

        with self.lock:
            # Count events
            self.counters[f"{event_name}.count"] += 1
            self.counters[f"log.{method_name}.count"] += 1

            # Track durations
            if "duration_ms" in event_dict:
                self.timers[f"{event_name}.duration"].append(
                    event_dict["duration_ms"]
                )

        return event_dict

    def get_metrics(self) -> dict:
        """Get collected metrics."""
        with self.lock:
            metrics = dict(self.counters)

            # Calculate timer stats
            for key, values in self.timers.items():
                if values:
                    metrics[f"{key}.avg"] = sum(values) / len(values)
                    metrics[f"{key}.max"] = max(values)
                    metrics[f"{key}.min"] = min(values)

            return metrics

    def reset(self):
        """Reset all metrics."""
        with self.lock:
            self.counters.clear()
            self.timers.clear()

# Usage
metrics_processor = MetricsProcessor()

# Later, export metrics
metrics = metrics_processor.get_metrics()
logger.info("metrics_export", metrics=metrics)
```

## Error Handling

### Safe Processor Wrapper

Wrap processors to handle errors gracefully:

```python
from provide.foundation import logger

def safe_processor(processor_func):
    """Wrap a processor to handle errors safely."""
    def wrapper(logger_instance, method_name, event_dict):
        try:
            return processor_func(logger_instance, method_name, event_dict)
        except Exception as e:
            # Log the error but don't break logging
            event_dict["processor_error"] = str(e)
            event_dict["failed_processor"] = processor_func.__name__
            return event_dict

    wrapper.__name__ = processor_func.__name__
    return wrapper

# Usage
@safe_processor
def risky_processor(logger_instance, method_name, event_dict):
    """Processor that might fail."""
    # Risky operation
    external_data = fetch_from_api()  # Could fail
    event_dict["external_data"] = external_data
    return event_dict
```

## Processor Ordering

### Order Matters

Processors run in the order they're defined:

```python
from provide.foundation import get_hub
from provide.foundation.logger.config import TelemetryConfig, LoggingConfig

def add_timestamp(logger_instance, method_name, event_dict):
    """Add timestamp first."""
    import time
    event_dict["timestamp"] = time.time()
    return event_dict

def format_timestamp(logger_instance, method_name, event_dict):
    """Format timestamp (requires timestamp field)."""
    if "timestamp" in event_dict:
        from datetime import datetime
        ts = datetime.fromtimestamp(event_dict["timestamp"])
        event_dict["timestamp"] = ts.isoformat()
    return event_dict

def add_severity(logger_instance, method_name, event_dict):
    """Add severity based on method name."""
    severity_map = {
        "debug": 10,
        "info": 20,
        "warning": 30,
        "error": 40,
        "critical": 50
    }
    event_dict["severity"] = severity_map.get(method_name, 0)
    return event_dict

# Correct order: timestamp â†’ format â†’ severity
processors = [
    add_timestamp,       # Runs first
    format_timestamp,    # Needs timestamp
    add_severity,        # Independent
]

hub = get_hub()
hub.initialize_foundation(
    TelemetryConfig(
        service_name="app",
        logging=LoggingConfig(processors=processors)
    )
)
```

## Testing Custom Processors

### Unit Testing

```python
import pytest
from provide.testkit import FoundationTestCase

class TestCustomProcessors(FoundationTestCase):
    """Test custom log processors."""

    def setup_method(self) -> None:
        """Set up test fixtures."""
        super().setup_method()

    def test_add_hostname_processor(self) -> None:
        """Test hostname processor."""
        event_dict = {"event": "test"}

        result = add_hostname_processor(None, "info", event_dict)

        assert "hostname" in result
        assert isinstance(result["hostname"], str)

    def test_sanitize_processor(self) -> None:
        """Test sanitization processor."""
        event_dict = {
            "event": "user_created",
            "email": "user@example.com",
            "password": "secret123",
            "user_id": "123"
        }

        result = sanitize_sensitive_data_processor(None, "info", event_dict)

        # Password should be redacted
        assert result["password"] == "***REDACTED***"

        # Email should be masked
        assert "@" not in result["email"] or "***" in result["email"]

        # user_id should be unchanged
        assert result["user_id"] == "123"

    def test_conditional_processor(self) -> None:
        """Test conditional processing."""
        error_dict = {"event": "error_occurred"}
        info_dict = {"event": "info_message"}

        # Should add context for errors
        error_result = error_only_enrichment(None, "error", error_dict)
        assert "severity" in error_result

        # Should not modify info logs
        info_result = error_only_enrichment(None, "info", info_dict)
        assert "severity" not in info_result
```

### Integration Testing

```python
from provide.foundation import logger, get_hub
from provide.foundation.logger.config import TelemetryConfig, LoggingConfig
from provide.testkit import set_log_stream_for_testing
import io

def test_processor_integration():
    """Test processors in logging pipeline."""
    # Create test stream
    stream = io.StringIO()
    set_log_stream_for_testing(stream)

    # Configure with custom processor
    hub = get_hub()
    hub.initialize_foundation(
        TelemetryConfig(
            service_name="test",
            logging=LoggingConfig(
                processors=[add_hostname_processor],
                log_format="json"
            )
        )
    )

    # Log something
    logger.info("test_event", key="value")

    # Check output
    output = stream.getvalue()
    assert "hostname" in output
    assert "test_event" in output
```

## Best Practices

### âœ… DO: Keep Processors Fast

```python
# âœ… GOOD: Fast, synchronous processor
def add_request_id(logger_instance, method_name, event_dict):
    """Add request ID from context."""
    event_dict["request_id"] = request_context.get("request_id", "none")
    return event_dict
```

### âŒ DON'T: Block in Processors

```python
# âŒ BAD: Blocking I/O in processor
def slow_processor(logger_instance, method_name, event_dict):
    """Don't do expensive operations!"""
    user_data = requests.get(f"https://api.example.com/users/{user_id}").json()
    event_dict["user_data"] = user_data  # Blocks logging!
    return event_dict

# âœ… GOOD: Use cached or async approach
cache = {}
def fast_processor(logger_instance, method_name, event_dict):
    """Use cached data."""
    user_id = event_dict.get("user_id")
    if user_id in cache:
        event_dict["user_name"] = cache[user_id]
    return event_dict
```

### âœ… DO: Handle Missing Fields Gracefully

```python
# âœ… GOOD: Check before accessing
def safe_enrichment(logger_instance, method_name, event_dict):
    """Safely enrich with optional fields."""
    user_id = event_dict.get("user_id")

    if user_id:
        event_dict["user_context"] = get_user_context(user_id)

    return event_dict
```

### âŒ DON'T: Mutate Original Objects

```python
# âŒ BAD: Mutating passed objects
def bad_processor(logger_instance, method_name, event_dict):
    """Don't mutate objects from event_dict!"""
    if "config" in event_dict:
        event_dict["config"]["modified"] = True  # Mutates original!
    return event_dict

# âœ… GOOD: Copy if you need to modify
def good_processor(logger_instance, method_name, event_dict):
    """Copy before modifying."""
    if "config" in event_dict:
        event_dict["config"] = {**event_dict["config"], "modified": True}
    return event_dict
```

### âœ… DO: Use Processor Classes for State

```python
# âœ… GOOD: Class-based processor with state
class CountingProcessor:
    """Processor with state."""

    def __init__(self):
        self.count = 0

    def __call__(self, logger_instance, method_name, event_dict):
        """Add counter to events."""
        self.count += 1
        event_dict["log_number"] = self.count
        return event_dict

processor = CountingProcessor()
```

### âŒ DON'T: Raise Exceptions Without Handling

```python
# âŒ BAD: Unhandled exceptions break logging
def unsafe_processor(logger_instance, method_name, event_dict):
    """Might break logging!"""
    required_field = event_dict["required"]  # KeyError if missing!
    return event_dict

# âœ… GOOD: Handle errors gracefully
def safe_processor(logger_instance, method_name, event_dict):
    """Handles errors properly."""
    try:
        required_field = event_dict["required"]
    except KeyError:
        event_dict["error"] = "missing_required_field"
    return event_dict
```

### âœ… DO: Document Processor Behavior

```python
# âœ… GOOD: Well-documented processor
def add_trace_context(logger_instance, method_name, event_dict):
    """Add distributed tracing context to log events.

    Adds the following fields if available in context:
    - trace_id: Unique identifier for the trace
    - span_id: Unique identifier for this span
    - parent_span_id: Parent span identifier

    Requires trace context to be set via contextvars.
    """
    from provide.foundation.tracer.context import get_current_trace

    trace = get_current_trace()
    if trace:
        event_dict.update({
            "trace_id": trace.trace_id,
            "span_id": trace.span_id,
            "parent_span_id": trace.parent_span_id
        })

    return event_dict
```

### âŒ DON'T: Log Inside Processors

```python
# âŒ BAD: Logging in a processor creates recursion risk
def logging_processor(logger_instance, method_name, event_dict):
    """Don't log in processors!"""
    logger.info("processing_event")  # Can cause infinite recursion!
    return event_dict

# âœ… GOOD: Use print for debugging or external logging
def debug_processor(logger_instance, method_name, event_dict):
    """Debug processor output."""
    if os.getenv("DEBUG_PROCESSORS"):
        print(f"Processing: {event_dict.get('event')}", file=sys.stderr)
    return event_dict
```

### âœ… DO: Test Processor Performance

```python
# âœ… GOOD: Benchmark processor performance
import time

def benchmark_processor(processor_func, iterations=1000):
    """Benchmark a processor."""
    event_dict = {"event": "test", "data": "sample"}

    start = time.time()
    for _ in range(iterations):
        processor_func(None, "info", event_dict.copy())
    duration = time.time() - start

    avg_ms = (duration / iterations) * 1000
    print(f"{processor_func.__name__}: {avg_ms:.4f}ms per call")

# Usage
benchmark_processor(add_hostname_processor)
benchmark_processor(sanitize_sensitive_data_processor)
```

### âŒ DON'T: Store Large Data in Events

```python
# âŒ BAD: Adding large data to every log
def bad_context_processor(logger_instance, method_name, event_dict):
    """Don't add huge objects!"""
    event_dict["entire_request_body"] = huge_json_blob  # Too large!
    return event_dict

# âœ… GOOD: Add summaries or IDs
def good_context_processor(logger_instance, method_name, event_dict):
    """Add summary data only."""
    event_dict["request_size"] = len(huge_json_blob)
    event_dict["request_hash"] = hashlib.sha256(huge_json_blob).hexdigest()[:8]
    return event_dict
```

### âœ… DO: Use Sampling for High-Volume Logs

```python
# âœ… GOOD: Sample verbose debug logs
import random

def sampling_processor(logger_instance, method_name, event_dict):
    """Sample debug logs at 1%."""
    if method_name == "debug" and random.random() > 0.01:
        from structlog.exceptions import DropEvent
        raise DropEvent
    return event_dict
```

## Next Steps

### Related Guides
- **[Structured Events](structured-events.md)**: Learn event naming conventions
- **[Basic Logging](basic-logging.md)**: Foundation logging basics
- **[Exception Logging](exception-logging.md)**: Log and handle exceptions

### Examples
- See `examples/telemetry/03_custom_processors.py` for processor examples
- See `examples/production/07_log_pipeline.py` for production patterns

### API Reference
- **[Processors API](../../reference/provide/foundation/logger/processors/index.md)**: Built-in processors
- **[Logger Config](../../reference/provide/foundation/logger/config/index.md)**: Configuration options

---

**Tip**: Keep processors fast and focused on a single concern. Use processor chaining to build complex pipelines from simple, testable components. Always handle errors gracefully to avoid breaking the logging pipeline.
>>> EOF >>>

### FILE 28: how-to-guides/logging/exception-logging.md | checksum=ed6d13c43d8f... | modified=2025-10-24T18:21:30 | op=+ | size=16764 | tokens=3402 | type=markdown ###
<<< BOF <<<
# How to Log Exceptions

Properly logging exceptions is crucial for debugging and operational visibility. `provide.foundation` provides simple and powerful ways to capture error context with full stack traces and structured metadata.

## Overview

Exception logging serves multiple purposes:
- **Debugging** - Understand what went wrong and where
- **Monitoring** - Alert on error rates and patterns
- **Auditing** - Track failures for compliance
- **Context** - Preserve relevant data for post-mortem analysis

Foundation's structured logging ensures exceptions are logged with rich context, making them easy to search, filter, and analyze in log aggregation systems.

## Basic Exception Logging

### Using `logger.exception()`

The `logger.exception()` method is the preferred way to log exceptions. It should be called from within an `except` block and automatically captures the full stack trace.

```python
from provide.foundation import logger

def risky_operation():
    raise ValueError("Something went wrong")

try:
    risky_operation()
except Exception:
    logger.exception(
        "Operation failed unexpectedly",
        operation_name="risky_operation",
        user_id="user_xyz",
    )
```

**Output includes:**
- Event message: "Operation failed unexpectedly"
- Structured fields: `operation_name`, `user_id`
- Full stack trace with file names and line numbers
- Exception type and message

### Using `logger.error()` with `exc_info`

For more control, you can use `logger.error()` and pass `exc_info=True` to include the traceback.

```python
try:
    risky_operation()
except Exception as e:
    logger.error(
        "Operation failed",
        exc_info=True,
        error_type=type(e).__name__,
        error_details=str(e),
    )
```

## Exception Logging Patterns

### Pattern 1: Log and Re-raise

Log the exception for visibility, then re-raise it for upstream handling:

```python
def process_payment(transaction):
    """Process payment with exception logging."""
    try:
        payment_gateway.charge(transaction)
    except PaymentError as e:
        logger.exception(
            "Payment processing failed",
            transaction_id=transaction.id,
            amount=transaction.amount,
            error_code=e.code,
        )
        raise  # Re-raise for caller to handle
```

**When to use:** When you want visibility at this layer but need upstream code to handle the error.

### Pattern 2: Log and Transform

Log the original exception, then raise a different exception type:

```python
from provide.foundation.errors import DatabaseError

def save_user(user_data):
    """Save user with exception transformation."""
    try:
        db.insert("users", user_data)
    except ConnectionError as e:
        logger.exception(
            "Database connection failed during user save",
            user_email=user_data.get("email"),
        )
        raise DatabaseError("Failed to save user") from e
```

**When to use:** When converting low-level exceptions to domain-specific exceptions.

### Pattern 3: Log and Handle

Log the exception and handle it completely (don't re-raise):

```python
def send_notification(user_id, message):
    """Send notification with graceful failure."""
    try:
        notification_service.send(user_id, message)
    except NotificationError as e:
        logger.exception(
            "Failed to send notification",
            user_id=user_id,
            notification_type=message.type,
        )
        # Don't raise - notification failure shouldn't break the flow
        return False
    return True
```

**When to use:** When the operation is optional or has a sensible fallback.

### Pattern 4: Log with Retry Context

Log exceptions within retry loops to track retry attempts:

```python
from provide.foundation.resilience import retry

@retry(NetworkError, max_attempts=3, base_delay=1.0)
def call_external_api(endpoint):
    """Call API with retry and exception logging."""
    try:
        response = requests.get(endpoint, timeout=5)
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        logger.exception(
            "API call failed",
            endpoint=endpoint,
            status_code=getattr(e.response, 'status_code', None),
            attempt="will_retry",  # Retry decorator will retry
        )
        raise
```

## Adding Context to Exceptions

### User Context

Include user information for debugging user-specific issues:

```python
def process_user_action(user_id, action):
    """Process action with user context."""
    try:
        result = perform_action(action)
    except Exception:
        logger.exception(
            "User action failed",
            user_id=user_id,
            user_email=get_user_email(user_id),
            action_type=action.type,
            action_id=action.id,
            session_id=get_current_session_id(),
        )
        raise
```

### Request Context

Capture HTTP request details for API debugging:

```python
def handle_api_request(request):
    """Handle API request with request context."""
    try:
        response = process_request(request)
    except Exception:
        logger.exception(
            "API request processing failed",
            method=request.method,
            path=request.path,
            request_id=request.headers.get("X-Request-ID"),
            user_agent=request.headers.get("User-Agent"),
            client_ip=request.remote_addr,
        )
        raise
```

### Business Context

Add domain-specific information:

```python
def finalize_order(order):
    """Finalize order with business context."""
    try:
        payment_result = process_payment(order)
        inventory_result = reserve_inventory(order)
        ship_order(order)
    except Exception:
        logger.exception(
            "Order finalization failed",
            order_id=order.id,
            customer_id=order.customer_id,
            total_amount=order.total,
            order_status=order.status,
            payment_status=getattr(payment_result, 'status', 'unknown'),
            inventory_status=getattr(inventory_result, 'status', 'unknown'),
        )
        raise
```

## Correlation IDs

Use correlation IDs to track requests across multiple services:

```python
import uuid
from contextvars import ContextVar

# Global context variable for correlation ID
correlation_id: ContextVar[str] = ContextVar("correlation_id", default=None)

def set_correlation_id(cid=None):
    """Set correlation ID for current context."""
    if cid is None:
        cid = str(uuid.uuid4())
    correlation_id.set(cid)
    return cid

def get_correlation_id():
    """Get current correlation ID."""
    return correlation_id.get()

def api_handler(request):
    """API handler with correlation ID."""
    # Extract or generate correlation ID
    cid = request.headers.get("X-Correlation-ID") or set_correlation_id()

    try:
        result = process_request(request)
    except Exception:
        logger.exception(
            "Request processing failed",
            correlation_id=cid,
            request_path=request.path,
        )
        raise

    return result
```

**Benefits:**
- Track errors across microservices
- Correlate logs from different systems
- Debug distributed request flows

## Exception Aggregation Patterns

### Collecting Multiple Failures

When processing batches, collect all failures for comprehensive error reporting:

```python
from typing import NamedTuple

class ProcessingResult(NamedTuple):
    success_count: int
    failure_count: int
    errors: list

def process_batch(items):
    """Process batch and collect all failures."""
    successes = 0
    failures = 0
    errors = []

    for item in items:
        try:
            process_item(item)
            successes += 1
        except Exception as e:
            failures += 1
            errors.append({
                "item_id": item.id,
                "error_type": type(e).__name__,
                "error_message": str(e),
            })
            logger.exception(
                "Item processing failed",
                item_id=item.id,
                batch_position=items.index(item),
            )

    # Log batch summary
    if failures > 0:
        logger.error(
            "Batch processing completed with failures",
            total_items=len(items),
            successes=successes,
            failures=failures,
            failure_rate=failures / len(items),
            errors=errors[:5],  # First 5 errors for visibility
        )

    return ProcessingResult(successes, failures, errors)
```

### Error Rate Tracking

Monitor error rates over time:

```python
from collections import deque
from datetime import datetime, timedelta

class ErrorRateTracker:
    """Track error rate over time window."""

    def __init__(self, window_seconds=60):
        self.window = timedelta(seconds=window_seconds)
        self.errors = deque()

    def record_error(self, exception):
        """Record an error occurrence."""
        now = datetime.now()
        self.errors.append((now, exception))

        # Remove old errors outside window
        cutoff = now - self.window
        while self.errors and self.errors[0][0] < cutoff:
            self.errors.popleft()

        # Log if error rate is high
        error_count = len(self.errors)
        if error_count > 10:  # More than 10 errors in window
            logger.warning(
                "High error rate detected",
                error_count=error_count,
                window_seconds=self.window.total_seconds(),
                recent_errors=[
                    type(e).__name__ for _, e in list(self.errors)[-5:]
                ],
            )

# Global tracker
error_tracker = ErrorRateTracker(window_seconds=60)

def monitored_operation():
    """Operation with error rate monitoring."""
    try:
        perform_operation()
    except Exception as e:
        error_tracker.record_error(e)
        logger.exception("Operation failed")
        raise
```

## Custom Exception Handlers

### Application-Wide Exception Handler

Set up a global exception handler for your application:

```python
import sys
from provide.foundation import logger

def global_exception_handler(exc_type, exc_value, exc_traceback):
    """Handle uncaught exceptions globally."""
    # Don't log KeyboardInterrupt
    if issubclass(exc_type, KeyboardInterrupt):
        sys.__excepthook__(exc_type, exc_value, exc_traceback)
        return

    logger.critical(
        "Uncaught exception",
        exc_type=exc_type.__name__,
        exc_message=str(exc_value),
        exc_info=(exc_type, exc_value, exc_traceback),
    )

# Install global handler
sys.excepthook = global_exception_handler
```

### Async Exception Handler

Handle exceptions in async code:

```python
import asyncio
from provide.foundation import logger

def async_exception_handler(loop, context):
    """Handle exceptions in async tasks."""
    exception = context.get("exception")
    message = context.get("message", "Async exception occurred")

    if exception:
        logger.exception(
            message,
            task=context.get("task"),
            future=context.get("future"),
        )
    else:
        logger.error(message, context=context)

# Set async exception handler
loop = asyncio.get_event_loop()
loop.set_exception_handler(async_exception_handler)
```

## Production Patterns

### Exception with Metric Tracking

Log exceptions and track metrics:

```python
from provide.foundation.metrics import Counter

# Define metrics
error_counter = Counter("app_errors_total", labels=["error_type", "operation"])

def tracked_operation(operation_name):
    """Operation with error tracking."""
    try:
        perform_operation()
    except Exception as e:
        error_type = type(e).__name__

        # Track metric
        error_counter.increment(labels={
            "error_type": error_type,
            "operation": operation_name,
        })

        # Log exception
        logger.exception(
            "Tracked operation failed",
            operation=operation_name,
            error_type=error_type,
        )
        raise
```

### Exception with Alerting

Trigger alerts for critical errors:

```python
def critical_operation():
    """Operation where failures trigger alerts."""
    try:
        result = perform_critical_task()
    except Exception as e:
        # Log with high severity
        logger.critical(
            "Critical operation failed - ALERT",
            operation="critical_task",
            severity="high",
            alert_team=True,  # Signal to monitoring system
            error_type=type(e).__name__,
        )

        # Send immediate notification
        send_pagerduty_alert(
            f"Critical operation failed: {type(e).__name__}",
            details=str(e),
        )

        raise
```

### Exception Sanitization

Sanitize sensitive data before logging:

```python
from provide.foundation.security import mask_secrets

def safe_exception_logging(user_data):
    """Log exceptions without exposing sensitive data."""
    try:
        process_user(user_data)
    except Exception:
        # Sanitize data before logging
        safe_data = {
            "user_id": user_data.get("user_id"),
            "email": mask_email(user_data.get("email")),
            "account_type": user_data.get("account_type"),
            # Exclude password, API keys, etc.
        }

        logger.exception(
            "User processing failed",
            user_data=safe_data,
        )
        raise

def mask_email(email):
    """Mask email for logging."""
    if not email or "@" not in email:
        return "***"
    username, domain = email.split("@")
    return f"{username[0]}***@{domain}"
```

## Best Practices

### âœ… DO: Always Preserve Stack Traces

```python
# âœ… Good: Preserves full stack trace
try:
    operation()
except Exception:
    logger.exception("Operation failed")
    raise

# âŒ Bad: Loses stack trace
try:
    operation()
except Exception as e:
    logger.error(f"Operation failed: {e}")  # No traceback!
    raise
```

### âœ… DO: Add Structured Context

```python
# âœ… Good: Rich structured context
try:
    process_order(order)
except Exception:
    logger.exception(
        "Order processing failed",
        order_id=order.id,
        customer_id=order.customer_id,
        amount=order.total,
    )

# âŒ Bad: String concatenation loses structure
try:
    process_order(order)
except Exception:
    logger.exception(
        f"Order {order.id} for customer {order.customer_id} failed"
    )
```

### âœ… DO: Use Appropriate Log Levels

```python
# âœ… Good: Appropriate severity levels
try:
    optional_operation()
except Exception:
    logger.warning("Optional operation failed")  # Not critical

try:
    critical_operation()
except Exception:
    logger.critical("Critical operation failed")  # Needs immediate attention
```

### âŒ DON'T: Log the Same Exception Multiple Times

```python
# âŒ Bad: Logs exception at every layer
def layer1():
    try:
        layer2()
    except Exception:
        logger.exception("Layer 1 failed")
        raise

def layer2():
    try:
        operation()
    except Exception:
        logger.exception("Layer 2 failed")  # Duplicate!
        raise

# âœ… Good: Log once at the appropriate layer
def layer1():
    try:
        layer2()
    except Exception:
        logger.exception("Operation failed", layer="layer1")
        raise

def layer2():
    operation()  # Let exception propagate
```

### âŒ DON'T: Swallow Exceptions Silently

```python
# âŒ Bad: Silent failure
try:
    important_operation()
except Exception:
    pass  # Lost forever!

# âœ… Good: At minimum, log it
try:
    important_operation()
except Exception:
    logger.exception("Operation failed but continuing")
    # Explicitly choosing to continue
```

## Next Steps

### Related Guides
- **[Basic Logging](basic-logging.md)**: Core logging patterns
- **[Structured Events](structured-events.md)**: Event-driven logging

### Error Handling & Resilience
- **[Retry Patterns](../resilience/retry.md)**: Automatically retry failed operations
- **[Circuit Breakers](../resilience/circuit-breaker.md)**: Prevent cascading failures
- **[Production Monitoring](../production/monitoring.md)**: Production-ready error handling

### Examples
- See `examples/telemetry/05_exception_handling.py` for comprehensive exception logging examples
- See `examples/production/02_error_handling.py` for production error patterns

---

**Tip**: Always log exceptions with `logger.exception()` or `exc_info=True` to preserve stack traces. Add structured context fields to make errors searchable and debuggable.
>>> EOF >>>

### FILE 29: how-to-guides/logging/structured-events.md | checksum=9ca14fb48b64... | modified=2025-10-24T19:05:43 | op=+ | size=19438 | tokens=4577 | type=markdown ###
<<< BOF <<<
# Structured Events

Learn how to use structured event logging with Domain-Action-Status patterns for better observability.

## Overview

Structured events provide a consistent way to log important application events with rich context that's both human-readable and machine-parseable. Foundation's Domain-Action-Status (DAS) pattern creates predictable, searchable log events that work seamlessly with log aggregation systems.

**Key benefits:**
- **Searchability** - Consistent naming makes events easy to find
- **Aggregation** - Group related events for metrics
- **Alerting** - Pattern-based alerts on event types
- **Analysis** - Query and analyze event patterns
- **Debugging** - Rich context for troubleshooting

## Basic Event Logging

Use structured key-value pairs to capture event context:

```python
from provide.foundation import logger

# Simple event with context
logger.info(
    "user_login",
    user_id="user_123",
    source="web_app",
    ip_address="192.168.1.100"
)

# Event with duration
logger.info(
    "api_request_completed",
    endpoint="/users",
    method="GET",
    duration_ms=45,
    status_code=200
)
```

**Output:**
```json
{
  "event": "user_login",
  "user_id": "user_123",
  "source": "web_app",
  "ip_address": "192.168.1.100",
  "timestamp": "2025-10-24T10:00:00Z",
  "level": "info"
}
```

## Domain-Action-Status Pattern

The DAS pattern organizes events into three components:

- **Domain** - System area (auth, payment, api, database)
- **Action** - What happened (login, process, request, query)
- **Status** - Outcome (success, failed, started, completed)

### Pattern Format

```
{domain}_{action}_{status}
```

### Authentication Domain

Track authentication events:

```python
# Login events
logger.info("auth_login_success", user_id="user_123", duration_ms=45)
logger.warning("auth_login_failed", username="alice", reason="invalid_password")
logger.info("auth_login_started", username="alice", source="mobile_app")

# Logout events
logger.info("auth_logout_success", user_id="user_123", session_duration_s=3600)

# Token events
logger.info("auth_token_issued", user_id="user_123", token_type="jwt", ttl_s=3600)
logger.warning("auth_token_expired", user_id="user_123", token_id="tok_456")
logger.error("auth_token_invalid", token_id="tok_789", reason="signature_mismatch")

# Password events
logger.info("auth_password_changed", user_id="user_123")
logger.warning("auth_password_reset_requested", email="user@example.com")
```

### Payment Domain

Track payment processing:

```python
# Payment lifecycle
logger.info(
    "payment_process_started",
    order_id="ORD-123",
    amount=99.99,
    currency="USD",
    payment_method="credit_card"
)

logger.info(
    "payment_process_success",
    order_id="ORD-123",
    transaction_id="txn_456",
    amount=99.99,
    duration_ms=1234
)

logger.error(
    "payment_process_failed",
    order_id="ORD-123",
    amount=99.99,
    reason="insufficient_funds",
    error_code="E1001"
)

# Refund events
logger.info("payment_refund_initiated", transaction_id="txn_456", amount=99.99)
logger.info("payment_refund_completed", refund_id="ref_789", amount=99.99)

# Fraud detection
logger.warning(
    "payment_fraud_detected",
    order_id="ORD-123",
    user_id="user_123",
    risk_score=0.95,
    rules_triggered=["velocity", "location"]
)
```

### API Domain

Track API requests and responses:

```python
# Request lifecycle
logger.info(
    "api_request_started",
    endpoint="/users",
    method="GET",
    request_id="req_123"
)

logger.info(
    "api_request_completed",
    endpoint="/users",
    method="GET",
    status_code=200,
    duration_ms=45,
    request_id="req_123"
)

logger.error(
    "api_request_failed",
    endpoint="/users",
    method="POST",
    status_code=500,
    error="database_unavailable",
    request_id="req_456"
)

# Rate limiting
logger.warning(
    "api_ratelimit_exceeded",
    user_id="user_123",
    endpoint="/users",
    limit=100,
    window_s=60
)

# Validation errors
logger.warning(
    "api_validation_failed",
    endpoint="/users",
    field="email",
    error="invalid_format",
    value="not-an-email"
)
```

### Database Domain

Track database operations:

```python
# Query events
logger.debug("database_query_started", table="users", operation="SELECT")

logger.debug(
    "database_query_completed",
    table="users",
    operation="SELECT",
    rows_returned=10,
    duration_ms=23
)

logger.warning(
    "database_query_slow",
    table="orders",
    operation="SELECT",
    duration_ms=5000,  # Over 5 seconds
    rows_returned=10000
)

# Connection events
logger.info("database_connection_opened", host="db.example.com", port=5432)
logger.warning("database_connection_failed", host="db.example.com", error="timeout")
logger.info("database_connection_closed", duration_s=3600)

# Transaction events
logger.debug("database_transaction_started", isolation_level="READ_COMMITTED")
logger.info("database_transaction_committed", changes=5, duration_ms=100)
logger.warning("database_transaction_rollback", reason="constraint_violation")
```

## Event Enrichment

Add contextual data to all events in a scope:

### Context Binding

```python
from provide.foundation import logger

# Bind request context
request_logger = logger.bind(
    request_id="req_123",
    user_id="user_123",
    session_id="sess_456"
)

# All subsequent logs include bound context
request_logger.info("page_view", page="/dashboard")
request_logger.info("action_taken", action="export_report")
request_logger.info("page_exit", time_on_page_s=45)
```

**Output:**
```json
{
  "event": "page_view",
  "page": "/dashboard",
  "request_id": "req_123",
  "user_id": "user_123",
  "session_id": "sess_456"
}
```

### Correlation IDs

Track requests across services:

```python
import uuid

def process_request(request):
    """Process request with correlation tracking."""
    # Get or generate correlation ID
    correlation_id = request.headers.get("X-Correlation-ID") or str(uuid.uuid4())

    # Bind to logger
    log = logger.bind(correlation_id=correlation_id)

    log.info("request_received", path=request.path, method=request.method)

    try:
        result = handle_request(request)
        log.info("request_completed", status="success")
        return result
    except Exception as e:
        log.error("request_failed", error=str(e))
        raise
```

### User Context

Add user information to events:

```python
def with_user_context(user_id: str):
    """Create logger with user context."""
    return logger.bind(
        user_id=user_id,
        user_role=get_user_role(user_id),
        tenant_id=get_tenant_id(user_id)
    )

# Usage
user_log = with_user_context("user_123")
user_log.info("feature_accessed", feature="export")
user_log.info("data_downloaded", format="csv", rows=1000)
```

## Event Schemas

Define consistent event structures:

### Order Events Schema

```python
from typing import TypedDict

class OrderEvent(TypedDict):
    """Schema for order-related events."""
    order_id: str
    customer_id: str
    total_amount: float
    currency: str
    items_count: int
    status: str

def log_order_event(action: str, status: str, order_data: OrderEvent):
    """Log order event with consistent schema."""
    logger.info(
        f"order_{action}_{status}",
        order_id=order_data["order_id"],
        customer_id=order_data["customer_id"],
        total_amount=order_data["total_amount"],
        currency=order_data["currency"],
        items_count=order_data["items_count"],
        order_status=order_data["status"]
    )

# Usage
order = OrderEvent(
    order_id="ORD-123",
    customer_id="cust_456",
    total_amount=99.99,
    currency="USD",
    items_count=3,
    status="pending"
)

log_order_event("create", "success", order)
log_order_event("process", "started", order)
log_order_event("fulfill", "completed", order)
```

### Error Events Schema

```python
from dataclasses import dataclass

@dataclass
class ErrorContext:
    """Schema for error events."""
    error_type: str
    error_message: str
    error_code: str | None = None
    stack_trace: str | None = None
    user_id: str | None = None
    request_id: str | None = None

def log_error_event(domain: str, action: str, error: ErrorContext):
    """Log error event with consistent schema."""
    logger.error(
        f"{domain}_{action}_failed",
        error_type=error.error_type,
        error_message=error.error_message,
        error_code=error.error_code,
        user_id=error.user_id,
        request_id=error.request_id,
        exc_info=error.stack_trace
    )

# Usage
try:
    process_payment(order)
except PaymentError as e:
    log_error_event(
        "payment",
        "process",
        ErrorContext(
            error_type="PaymentError",
            error_message=str(e),
            error_code="E1001",
            user_id=order.customer_id,
            request_id=current_request_id()
        )
    )
```

## Event Metrics

Track event counts and patterns:

### Event Counter

```python
from collections import defaultdict
from datetime import datetime, timedelta

class EventMetrics:
    """Track event metrics."""

    def __init__(self):
        self.events = defaultdict(int)
        self.last_reset = datetime.now()

    def record_event(self, event_name: str):
        """Record event occurrence."""
        self.events[event_name] += 1

        # Log event
        logger.info(
            event_name,
            event_count=self.events[event_name],
            time_since_reset=(datetime.now() - self.last_reset).total_seconds()
        )

    def get_metrics(self) -> dict[str, int]:
        """Get event counts."""
        return dict(self.events)

    def reset(self):
        """Reset counters."""
        self.events.clear()
        self.last_reset = datetime.now()

# Usage
metrics = EventMetrics()

metrics.record_event("api_request_completed")
metrics.record_event("api_request_completed")
metrics.record_event("api_request_failed")

# Get summary
logger.info("metrics_summary", metrics=metrics.get_metrics())
```

### Event Timing

```python
import time
from contextlib import contextmanager

@contextmanager
def timed_event(event_name: str, **context):
    """Context manager for timed events."""
    start = time.time()

    logger.info(f"{event_name}_started", **context)

    try:
        yield
        duration_ms = (time.time() - start) * 1000
        logger.info(
            f"{event_name}_completed",
            duration_ms=duration_ms,
            **context
        )
    except Exception as e:
        duration_ms = (time.time() - start) * 1000
        logger.error(
            f"{event_name}_failed",
            duration_ms=duration_ms,
            error=str(e),
            **context
        )
        raise

# Usage
with timed_event("database_query", table="users", operation="SELECT"):
    results = db.query("SELECT * FROM users WHERE active = true")

with timed_event("api_call", endpoint="/users", method="GET"):
    response = http_client.get("/users")
```

## Multi-Domain Events

Track events spanning multiple domains:

### Order Fulfillment Flow

```python
def fulfill_order(order_id: str):
    """Fulfill order with comprehensive event logging."""
    log = logger.bind(order_id=order_id)

    # Order domain
    log.info("order_fulfill_started")

    try:
        # Payment domain
        log.info("payment_charge_started")
        payment = process_payment(order_id)
        log.info("payment_charge_success", transaction_id=payment.id)

        # Inventory domain
        log.info("inventory_reserve_started")
        reservation = reserve_inventory(order_id)
        log.info("inventory_reserve_success", reservation_id=reservation.id)

        # Shipping domain
        log.info("shipping_create_started")
        shipment = create_shipment(order_id)
        log.info("shipping_create_success", tracking_number=shipment.tracking)

        # Notification domain
        log.info("notification_send_started", type="order_confirmation")
        send_confirmation_email(order_id)
        log.info("notification_send_success")

        # Final order status
        log.info("order_fulfill_completed", status="shipped")

    except PaymentError as e:
        log.error("payment_charge_failed", error=str(e))
        log.error("order_fulfill_failed", reason="payment_failed")
        raise
    except InventoryError as e:
        log.error("inventory_reserve_failed", error=str(e))
        log.error("order_fulfill_failed", reason="out_of_stock")
        raise
```

## Event Querying Patterns

Structure events for easy querying:

### Searchable Events

```python
# Events designed for search
logger.info(
    "user_action",
    action_type="purchase",  # Filterable
    category="electronics",  # Facetable
    amount=99.99,           # Aggregatable
    user_segment="premium",  # Groupable
    ab_test_variant="B"     # Analyzable
)

# Query examples (in log aggregation system):
# - All purchases: event:"user_action" AND action_type:"purchase"
# - Total revenue: SUM(amount) WHERE action_type:"purchase"
# - By category: GROUP BY category WHERE action_type:"purchase"
# - A/B test: GROUP BY ab_test_variant WHERE action_type:"purchase"
```

### Time-Based Events

```python
from datetime import datetime

logger.info(
    "session_activity",
    session_id="sess_123",
    user_id="user_456",
    activity_type="page_view",
    page="/dashboard",
    timestamp_iso=datetime.now().isoformat(),
    hour_of_day=datetime.now().hour,
    day_of_week=datetime.now().strftime("%A")
)

# Enables queries like:
# - Peak hours: GROUP BY hour_of_day
# - Weekly patterns: GROUP BY day_of_week
# - Time series: ORDER BY timestamp_iso
```

## Production Patterns

### High-Volume Events

For high-throughput scenarios, use sampling:

```python
import random

def log_high_volume_event(event_name: str, sample_rate: float = 0.01, **context):
    """Log event with sampling for high volumes."""
    if random.random() < sample_rate:
        logger.info(
            event_name,
            sampled=True,
            sample_rate=sample_rate,
            **context
        )

# Only log 1% of cache hits
log_high_volume_event("cache_hit", sample_rate=0.01, key="user:123")

# Always log cache misses
logger.info("cache_miss", key="user:123")
```

### Event Batching

Batch events for efficiency:

```python
from typing import Any

class EventBatcher:
    """Batch events for efficient logging."""

    def __init__(self, batch_size: int = 100):
        self.batch_size = batch_size
        self.batch: list[dict[str, Any]] = []

    def add_event(self, event_name: str, **context):
        """Add event to batch."""
        self.batch.append({"event": event_name, **context})

        if len(self.batch) >= self.batch_size:
            self.flush()

    def flush(self):
        """Flush batch to logs."""
        if self.batch:
            logger.info(
                "events_batch",
                event_count=len(self.batch),
                events=self.batch
            )
            self.batch.clear()

# Usage
batcher = EventBatcher(batch_size=100)

for item in process_items():
    batcher.add_event("item_processed", item_id=item.id, status="success")

batcher.flush()  # Flush remaining events
```

### Sensitive Data Masking

Mask sensitive information in events:

```python
import re

def mask_email(email: str) -> str:
    """Mask email for logging."""
    if "@" not in email:
        return "***"
    username, domain = email.split("@")
    return f"{username[0]}***@{domain}"

def mask_credit_card(card: str) -> str:
    """Mask credit card number."""
    return f"****-****-****-{card[-4:]}"

logger.info(
    "payment_process_started",
    user_email=mask_email("user@example.com"),  # u***@example.com
    card_number=mask_credit_card("4111111111111111"),  # ****-****-****-1111
    amount=99.99
)
```

## Best Practices

### âœ… DO: Use Consistent Naming

```python
# âœ… Good: Consistent DAS pattern
logger.info("auth_login_success")
logger.info("auth_logout_success")
logger.info("auth_token_issued")

# âŒ Bad: Inconsistent naming
logger.info("user_logged_in")
logger.info("logout_successful")
logger.info("token_created")
```

### âœ… DO: Include Rich Context

```python
# âœ… Good: Detailed context
logger.info(
    "order_created",
    order_id="ORD-123",
    customer_id="cust_456",
    total_amount=99.99,
    items_count=3,
    payment_method="credit_card"
)

# âŒ Bad: Minimal context
logger.info("order_created", order_id="ORD-123")
```

### âœ… DO: Use Appropriate Log Levels

```python
# âœ… Good: Correct severity
logger.debug("cache_hit", key="user:123")  # Low importance
logger.info("user_login_success", user_id="123")  # Normal operation
logger.warning("api_ratelimit_exceeded", user_id="123")  # Concerning
logger.error("payment_process_failed", order_id="ORD-123")  # Error
logger.critical("database_connection_lost")  # Critical failure

# âŒ Bad: Everything at same level
logger.info("cache_hit")  # Too noisy
logger.info("database_connection_lost")  # Too quiet
```

### âœ… DO: Structure for Searchability

```python
# âœ… Good: Structured for queries
logger.info(
    "api_request_completed",
    endpoint="/users",
    method="GET",
    status_code=200,
    duration_ms=45
)

# âŒ Bad: String concatenation
logger.info("API GET /users completed in 45ms with status 200")
```

### âŒ DON'T: Log Sensitive Data

```python
# âŒ Bad: Exposes sensitive data
logger.info("auth_login_success", password="secret123")

# âœ… Good: Masked or omitted
logger.info("auth_login_success", user_id="123")  # No password
```

### âŒ DON'T: Use Dynamic Event Names

```python
# âŒ Bad: Dynamic event names (hard to search)
logger.info(f"user_{action}_completed")  # Changes per action

# âœ… Good: Static event names with context
logger.info("user_action_completed", action=action)
```

## Event Catalog

Maintain an event catalog for your application:

```python
# events.py
class Events:
    """Centralized event definitions."""

    # Authentication
    AUTH_LOGIN_SUCCESS = "auth_login_success"
    AUTH_LOGIN_FAILED = "auth_login_failed"
    AUTH_LOGOUT_SUCCESS = "auth_logout_success"

    # Payments
    PAYMENT_PROCESS_STARTED = "payment_process_started"
    PAYMENT_PROCESS_SUCCESS = "payment_process_success"
    PAYMENT_PROCESS_FAILED = "payment_process_failed"

    # API
    API_REQUEST_STARTED = "api_request_started"
    API_REQUEST_COMPLETED = "api_request_completed"
    API_REQUEST_FAILED = "api_request_failed"

# Usage
from events import Events

logger.info(Events.AUTH_LOGIN_SUCCESS, user_id="123")
logger.error(Events.PAYMENT_PROCESS_FAILED, order_id="ORD-123")
```

## Next Steps

### Related Guides
- **[Basic Logging](basic-logging.md)**: Core logging patterns
- **[Exception Logging](exception-logging.md)**: Error logging with context
- **[Custom Processors](custom-processors.md)**: Extend logging with processors

### Examples
- See `examples/telemetry/04_das_pattern.py` for DAS pattern examples
- See `examples/production/01_production_patterns.py` for production event patterns

### API Reference
- **[API Reference: Logger](../../reference/provide/foundation/logger/index.md)**: Complete logger API

---

**Tip**: Start with basic DAS naming (domain_action_status) and add rich context. Use log aggregation queries to validate your event structure is searchable and useful.
>>> EOF >>>

### FILE 30: how-to-guides/observability/metrics.md | checksum=40d28d2025e8... | modified=2025-10-24T20:38:39 | op=+ | size=7150 | tokens=1573 | type=markdown ###
<<< BOF <<<
# How to Use Metrics

Foundation provides a lightweight metrics system for collecting and reporting application metrics. This guide shows you how to use built-in metrics and create custom metrics.

## Quick Start

```python
from provide.foundation.metrics import counter, gauge, histogram
from provide.foundation import logger

# Create metrics
request_counter = counter("http_requests_total", "Total HTTP requests")
active_connections = gauge("active_connections", "Number of active connections")
request_duration = histogram("http_request_duration_seconds", "HTTP request duration")

# Use metrics
request_counter.inc()  # Increment counter
active_connections.set(42)  # Set gauge value
request_duration.observe(0.123)  # Record histogram observation

logger.info("metrics_recorded", requests=request_counter.value)
```

## Available Metric Types

### Counter

Counters are monotonically increasing values (can only go up):

```python
from provide.foundation.metrics import counter

# Create counter
requests = counter("api_requests", "Total API requests")

# Increment by 1
requests.inc()

# Increment by N
requests.inc(5)

# Get current value
print(requests.value)  # 6
```

**Use counters for:**
- Total requests processed
- Total errors encountered
- Total bytes sent/received
- Total cache hits/misses

### Gauge

Gauges are values that can go up and down:

```python
from provide.foundation.metrics import gauge

# Create gauge
memory_usage = gauge("memory_usage_bytes", "Current memory usage")

# Set value
memory_usage.set(1024 * 1024 * 100)  # 100 MB

# Increment
memory_usage.inc(1024)

# Decrement
memory_usage.dec(512)

# Get current value
print(memory_usage.value)
```

**Use gauges for:**
- Current memory usage
- Active connections
- Queue size
- Temperature readings
- Cache size

### Histogram

Histograms track distributions of values:

```python
from provide.foundation.metrics import histogram

# Create histogram
response_time = histogram(
    "http_response_time_seconds",
    "HTTP response time in seconds"
)

# Record observations
response_time.observe(0.123)
response_time.observe(0.456)
response_time.observe(0.089)

# Get statistics
stats = response_time.stats()
print(stats["count"])  # Total observations
print(stats["sum"])    # Sum of all values
print(stats["mean"])   # Average value
```

**Use histograms for:**
- Request/response latencies
- Request sizes
- Response sizes
- Processing durations

## Metrics with Labels

Add labels to metrics for multi-dimensional data:

```python
from provide.foundation.metrics import counter

# Create labeled counter
http_requests = counter(
    "http_requests_total",
    "Total HTTP requests",
    labels=["method", "status"]
)

# Increment with label values
http_requests.labels(method="GET", status="200").inc()
http_requests.labels(method="POST", status="201").inc()
http_requests.labels(method="GET", status="404").inc()

# Access specific label combination
get_200_requests = http_requests.labels(method="GET", status="200")
print(get_200_requests.value)  # 1
```

## Integration with Logging

Combine metrics with structured logging:

```python
from provide.foundation import logger
from provide.foundation.metrics import counter, histogram
import time

# Metrics
requests_total = counter("requests_total", "Total requests")
request_duration = histogram("request_duration_seconds", "Request duration")

def handle_request(request_id: str):
    """Handle HTTP request with metrics and logging."""
    start_time = time.time()

    # Log request start
    logger.info("request_started", request_id=request_id)

    try:
        # Process request
        result = process(request_id)

        # Record metrics
        requests_total.labels(status="success").inc()
        duration = time.time() - start_time
        request_duration.observe(duration)

        # Log success
        logger.info(
            "request_completed",
            request_id=request_id,
            duration_ms=round(duration * 1000, 2)
        )

        return result

    except Exception as e:
        # Record error metrics
        requests_total.labels(status="error").inc()
        duration = time.time() - start_time
        request_duration.observe(duration)

        # Log error
        logger.error(
            "request_failed",
            request_id=request_id,
            error=str(e),
            duration_ms=round(duration * 1000, 2)
        )
        raise
```

## OpenTelemetry Integration

Foundation integrates with OpenTelemetry for distributed metrics:

```python
from provide.foundation import get_hub, TelemetryConfig
from provide.foundation.metrics import counter

# Enable OpenTelemetry metrics
config = TelemetryConfig(
    service_name="my-service",
    metrics_enabled=True,
    otlp_endpoint="http://localhost:4317"
)

hub = get_hub()
hub.initialize_foundation(config)

# Metrics automatically exported to OpenTelemetry
requests = counter("http_requests_total", "Total requests")
requests.inc()
```

## Best Practices

### âœ… DO: Use Descriptive Names

```python
# âœ… Good: Clear metric names
http_requests_total = counter("http_requests_total", "Total HTTP requests")
memory_usage_bytes = gauge("memory_usage_bytes", "Memory usage in bytes")

# âŒ Bad: Vague names
count = counter("count", "Count")
value = gauge("value", "Value")
```

### âœ… DO: Include Units in Names

```python
# âœ… Good: Units in metric names
response_time_seconds = histogram("response_time_seconds", "Response time")
memory_bytes = gauge("memory_bytes", "Memory usage")

# âŒ Bad: Missing units
response_time = histogram("response_time", "Response time")  # Seconds? Milliseconds?
```

### âœ… DO: Use Labels for Dimensions

```python
# âœ… Good: Use labels for dimensions
requests = counter("requests_total", "Requests", labels=["method", "status"])
requests.labels(method="GET", status="200").inc()

# âŒ Bad: Multiple metrics
get_requests_200 = counter("get_requests_200", "GET 200 requests")
post_requests_201 = counter("post_requests_201", "POST 201 requests")
```

### âŒ DON'T: Use High-Cardinality Labels

```python
# âŒ Bad: User ID creates too many unique label combinations
requests = counter("requests", "Requests", labels=["user_id"])
requests.labels(user_id="user_12345").inc()  # Creates thousands of metrics!

# âœ… Good: Use low-cardinality labels
requests = counter("requests", "Requests", labels=["endpoint", "method"])
requests.labels(endpoint="/api/users", method="GET").inc()
```

## Environment Configuration

Control metrics via environment variables:

```bash
# Enable/disable metrics
export PROVIDE_METRICS_ENABLED=true

# Set OpenTelemetry endpoint
export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

# Set service name
export OTEL_SERVICE_NAME=my-service
```

## Next Steps

- **[Monitoring Guide](../production/monitoring.md)**: Production monitoring patterns
- **[Architecture](../../explanation/architecture.md)**: Understanding Foundation's design
- **[Logging](../logging/basic-logging.md)**: Combine metrics with structured logging

---

**Tip**: Start with simple counters and gauges, then add histograms when you need distribution data.
>>> EOF >>>

### FILE 31: how-to-guides/platform/platform-detection.md | checksum=6790b4b56e0f... | modified=2025-10-24T20:14:41 | op=+ | size=3190 | tokens=783 | type=markdown ###
<<< BOF <<<
# How to Use Platform Detection

Foundation provides cross-platform detection and system information utilities.

## Platform Detection

### OS Detection

```python
from provide.foundation.platform import (
    is_linux,
    is_macos,
    is_windows,
    is_64bit,
    is_arm
)

# Check operating system
if is_linux():
    print("Running on Linux")
elif is_macos():
    print("Running on macOS")
elif is_windows():
    print("Running on Windows")

# Check architecture
if is_64bit():
    print("64-bit system")

if is_arm():
    print("ARM processor")
```

### Platform Information

```python
from provide.foundation.platform import (
    get_os_name,
    get_os_version,
    get_arch_name,
    get_platform_string
)

# Get OS details
os = get_os_name()         # "linux", "darwin", "windows"
version = get_os_version()  # "22.04", "13.0", etc.
arch = get_arch_name()      # "x86_64", "arm64", etc.

# Get combined platform string
platform = get_platform_string()
# "linux_x86_64", "darwin_arm64", etc.
```

## System Information

### Get Complete System Info

```python
from provide.foundation.platform import get_system_info, SystemInfo

info: SystemInfo = get_system_info()

print(f"OS: {info.os_name} {info.os_version}")
print(f"Architecture: {info.architecture}")
print(f"CPU: {info.cpu_brand}")
print(f"CPU Cores: {info.cpu_count}")
print(f"64-bit: {info.is_64bit}")
```

## CPU Information

**Requires**: `provide-foundation[platform]` extra

```python
from provide.foundation.platform import (
    get_cpu_info,
    get_cpu_brand,
    get_cpu_count,
    has_cpu_flag
)

# Get CPU brand
brand = get_cpu_brand()
# "Intel(R) Core(TM) i7-9750H"

# Get CPU count
count = get_cpu_count()
# 12

# Check CPU features
if has_cpu_flag("avx2"):
    print("AVX2 supported")
```

## Systemd Integration (Linux)

**Requires**: `systemd` on Linux systems

```python
from provide.foundation.platform import (
    has_systemd,
    notify_ready,
    notify_status,
    notify_stopping,
    notify_watchdog
)

if has_systemd():
    # Notify systemd that service is ready
    notify_ready()

    # Send status updates
    notify_status("Processing requests...")

    # Send watchdog ping
    notify_watchdog()

    # Notify before stopping
    notify_stopping()
```

## Best Practices

### âœ… DO: Use Platform Detection for OS-Specific Code

```python
# âœ… Good: Platform-specific paths
from provide.foundation.platform import is_windows, is_linux

if is_windows():
    config_dir = "C:\\ProgramData\\myapp"
elif is_linux():
    config_dir = "/etc/myapp"
else:
    config_dir = "~/myapp"
```

### âœ… DO: Log Platform Information

```python
# âœ… Good: Log system info at startup
from provide.foundation import logger
from provide.foundation.platform import get_system_info

info = get_system_info()
logger.info(
    "application_started",
    os=info.os_name,
    arch=info.architecture,
    cpu=info.cpu_brand
)
```

## Next Steps

- **[Process Execution](../process/subprocess.md)**: Platform-aware process execution
- **[Configuration](../configuration/env-variables.md)**: Platform-specific configuration

---

**Tip**: Install `provide-foundation[platform]` for full CPU detection capabilities.
>>> EOF >>>

### FILE 32: how-to-guides/process/subprocess.md | checksum=8e83def95f9c... | modified=2025-10-24T20:14:41 | op=+ | size=5632 | tokens=1247 | type=markdown ###
<<< BOF <<<
# How to Execute Subprocesses

Foundation provides secure subprocess execution with integrated logging and security features.

## Synchronous Execution

### Basic Command Execution

```python
from provide.foundation.process import run

# Run command
result = run(["ls", "-la"])

print(result.stdout)      # Command output
print(result.returncode)  # Exit code
```

### Shell Commands

```python
from provide.foundation.process import shell

# Run shell command
result = shell("ls -la | grep README")

print(result.stdout)
```

### Simple Execution

```python
from provide.foundation.process import run_simple

# Run command, raise on error
output = run_simple(["git", "status"])
print(output)  # Just the stdout
```

### Streaming Output

```python
from provide.foundation.process import stream

# Stream command output line-by-line
for line in stream(["tail", "-f", "logfile.txt"]):
    print(line)  # Process each line as it arrives
```

## Asynchronous Execution

### Async Commands

```python
from provide.foundation.process import async_run
import asyncio

async def run_command():
    result = await async_run(["ls", "-la"])
    print(result.stdout)

asyncio.run(run_command())
```

### Async Shell

```python
from provide.foundation.process import async_shell
import asyncio

async def run_shell():
    result = await async_shell("echo 'Hello'")
    print(result.stdout)

asyncio.run(run_shell())
```

### Async Streaming

```python
from provide.foundation.process import async_stream
import asyncio

async def stream_output():
    async for line in async_stream(["tail", "-f", "log.txt"]):
        print(line)

asyncio.run(stream_output())
```

## Process Management

### Managed Process

```python
from provide.foundation.process import ManagedProcess

# Create managed process
proc = ManagedProcess(["python", "server.py"])

# Start process
proc.start()

# Wait for output
output = proc.wait_for_output(pattern="Server started", timeout=10)

# Stop process
proc.stop()
```

### Wait for Output

```python
from provide.foundation.process import wait_for_process_output
import subprocess

proc = subprocess.Popen(["python", "app.py"], stdout=subprocess.PIPE)

# Wait for specific output
success = wait_for_process_output(
    proc,
    pattern="Ready to accept connections",
    timeout=30
)

if success:
    print("Application started successfully")
```

## Process Control (Linux)

**Requires**: `provide-foundation[process]` extra on Linux

```python
from provide.foundation.process import (
    set_process_title,
    set_name,
    set_death_signal,
    set_no_new_privs
)

# Set process title (visible in ps/top)
set_process_title("my-worker")

# Set process name (Linux prctl)
if is_linux():
    set_name("worker-1")

    # Set death signal (kill when parent dies)
    import signal
    set_death_signal(signal.SIGTERM)

    # Disable privilege escalation
    set_no_new_privs()
```

## Exit Utilities

```python
from provide.foundation.process import (
    exit_success,
    exit_error,
    exit_interrupted
)

# Exit with success
exit_success()  # Exit code 0

# Exit with error
exit_error("Operation failed", code=1)

# Exit on interruption (Ctrl+C)
try:
    do_work()
except KeyboardInterrupt:
    exit_interrupted()  # Exit code 130
```

## Security Features

Foundation's process execution includes:

- **Command validation**: Prevents command injection
- **Environment scrubbing**: Removes sensitive variables
- **Automatic logging**: All executions logged securely
- **Secret masking**: Secrets masked in logs

```python
from provide.foundation.process import run
from provide.foundation import logger

# Execution automatically logged with masked secrets
result = run([
    "curl",
    "-H", "Authorization: Bearer secret-token",
    "https://api.example.com"
])

# Log shows: Authorization: Bearer ***MASKED***
```

## Best Practices

### âœ… DO: Use List Arguments

```python
# âœ… Good: Safe from injection
from provide.foundation.process import run

result = run(["git", "log", "--oneline", user_input])

# âŒ Bad: Shell injection risk
result = shell(f"git log --oneline {user_input}")
```

### âœ… DO: Handle Errors

```python
# âœ… Good: Check return code
from provide.foundation.process import run, ProcessError

try:
    result = run(["command", "arg"])
    if result.returncode != 0:
        handle_error(result.stderr)
except ProcessError as e:
    logger.error("command_failed", error=str(e))
```

### âœ… DO: Use Async for Concurrent Commands

```python
# âœ… Good: Run commands concurrently
from provide.foundation.process import async_run
import asyncio

async def run_all():
    results = await asyncio.gather(
        async_run(["command1"]),
        async_run(["command2"]),
        async_run(["command3"])
    )

asyncio.run(run_all())
```

## Common Patterns

### Run with Timeout

```python
from provide.foundation.process import run

try:
    result = run(["long-running-command"], timeout=30)
except TimeoutError:
    print("Command timed out")
```

### Capture and Log Output

```python
from provide.foundation.process import run
from provide.foundation import logger

result = run(["deployment-script"])

logger.info(
    "deployment_executed",
    exit_code=result.returncode,
    output_lines=len(result.stdout.split("\n"))
)
```

## Next Steps

- **[Platform Detection](../platform/platform-detection.md)**: Platform-aware execution
- **[Security](../security/security-utilities.md)**: Secure command execution
- **[Logging](../logging/basic-logging.md)**: Log process execution

---

**Tip**: Always use list-based arguments instead of shell strings to prevent injection attacks.
>>> EOF >>>

### FILE 33: how-to-guides/production/deployment.md | checksum=721171f4bb76... | modified=2025-10-24T19:38:59 | op=+ | size=19246 | tokens=4983 | type=markdown ###
<<< BOF <<<
# Deployment Patterns

Learn best practices for deploying Foundation applications to production environments with Docker, Kubernetes, and cloud platforms.

## Overview

Deploying Foundation applications to production requires careful consideration of configuration management, secret handling, logging, health checks, and scaling. This guide provides battle-tested deployment patterns for containerized environments, cloud platforms, and orchestration systems.

**What you'll learn:**
- Build production Docker images
- Deploy to Kubernetes with best practices
- Manage secrets securely
- Configure multi-environment deployments
- Implement zero-downtime deployments
- Set up auto-scaling
- Handle graceful shutdown
- Monitor deployments

**Key Features:**
- ðŸ³ **Docker**: Optimized multi-stage builds
- â˜¸ï¸ **Kubernetes**: Production-ready manifests
- ðŸ”’ **Secret Management**: Secure secret handling
- ðŸŒ **Multi-Environment**: Dev, staging, production configs
- ðŸš€ **Zero-Downtime**: Rolling updates and health checks
- ðŸ“ˆ **Auto-Scaling**: HPA and resource management
- ðŸ›¡ï¸ **Security**: Non-root users, minimal images

## Prerequisites

```bash
# Required tools
docker --version
kubectl version --client
helm version

# Foundation with production extras
pip install provide-foundation[production]
```

## Docker Deployment

### Production Dockerfile

Build optimized Docker images:

```dockerfile
# Multi-stage build for smaller images
FROM python:3.11-slim as builder

# Install build dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    git \
    && rm -rf /var/lib/apt/lists/*

# Install UV package manager
RUN pip install uv

# Copy dependency files
WORKDIR /app
COPY pyproject.toml uv.lock ./

# Install dependencies
RUN uv sync --frozen --no-dev

# Production stage
FROM python:3.11-slim

# Install runtime dependencies only
RUN apt-get update && apt-get install -y \
    libpq5 \
    && rm -rf /var/lib/apt/lists/*

# Create non-root user
RUN useradd -m -u 1000 appuser

WORKDIR /app

# Copy virtual environment from builder
COPY --from=builder /app/.venv /app/.venv

# Copy application code
COPY src/ /app/src/
COPY examples/ /app/examples/

# Set ownership
RUN chown -R appuser:appuser /app

# Switch to non-root user
USER appuser

# Set environment variables
ENV PATH="/app/.venv/bin:$PATH" \
    PYTHONPATH="/app/src" \
    PROVIDE_LOG_LEVEL=INFO \
    PROVIDE_LOG_FORMAT=json

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health/live')"

# Run application
CMD ["python", "-m", "myapp"]
```

### Docker Compose for Local Development

```yaml
# docker-compose.yml
version: '3.8'

services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
      target: builder  # Use builder stage for development
    volumes:
      - ./src:/app/src:ro
      - ./examples:/app/examples:ro
    environment:
      PROVIDE_LOG_LEVEL: DEBUG
      PROVIDE_LOG_FORMAT: console
      PROVIDE_SERVICE_NAME: myapp-dev
      DATABASE_URL: postgresql://user:pass@db:5432/myapp
      REDIS_URL: redis://redis:6379/0
    ports:
      - "8000:8000"
    depends_on:
      - db
      - redis
    command: python -m myapp

  db:
    image: postgres:15-alpine
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: pass
      POSTGRES_DB: myapp
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

volumes:
  postgres_data:
  redis_data:
```

### Building and Publishing Images

```bash
# Build production image
docker build -t myapp:1.0.0 -t myapp:latest .

# Tag for registry
docker tag myapp:1.0.0 registry.example.com/myapp:1.0.0

# Push to registry
docker push registry.example.com/myapp:1.0.0
docker push registry.example.com/myapp:latest

# Scan for vulnerabilities
docker scan myapp:1.0.0
```

## Kubernetes Deployment

### Production Deployment Manifest

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  namespace: production
  labels:
    app: myapp
    version: v1.0.0
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0  # Zero-downtime deployments
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
        version: v1.0.0
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/metrics"
    spec:
      # Use service account with minimal permissions
      serviceAccountName: myapp

      # Run as non-root user
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000

      containers:
      - name: app
        image: registry.example.com/myapp:1.0.0
        imagePullPolicy: Always

        # Resource limits and requests
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"

        # Environment variables
        env:
        - name: PROVIDE_LOG_LEVEL
          value: "INFO"
        - name: PROVIDE_LOG_FORMAT
          value: "json"
        - name: PROVIDE_SERVICE_NAME
          value: "myapp"
        - name: ENVIRONMENT
          value: "production"
        - name: APP_VERSION
          value: "1.0.0"

        # Secrets from Kubernetes secrets
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: myapp-secrets
              key: database_url
        - name: API_KEY
          valueFrom:
            secretKeyRef:
              name: myapp-secrets
              key: api_key

        # ConfigMap values
        - name: FEATURE_FLAGS
          valueFrom:
            configMapKeyRef:
              name: myapp-config
              key: feature_flags

        # Application port
        ports:
        - name: http
          containerPort: 8000
          protocol: TCP
        - name: metrics
          containerPort: 9090
          protocol: TCP

        # Liveness probe - is the app alive?
        livenessProbe:
          httpGet:
            path: /health/live
            port: http
          initialDelaySeconds: 10
          periodSeconds: 30
          timeoutSeconds: 3
          failureThreshold: 3

        # Readiness probe - is the app ready for traffic?
        readinessProbe:
          httpGet:
            path: /health/ready
            port: http
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 3
          failureThreshold: 2

        # Graceful shutdown
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 15"]

        # Mount volumes
        volumeMounts:
        - name: config
          mountPath: /app/config
          readOnly: true
        - name: secrets
          mountPath: /app/secrets
          readOnly: true

      volumes:
      - name: config
        configMap:
          name: myapp-config
      - name: secrets
        secret:
          secretName: myapp-secrets

      # Image pull secrets
      imagePullSecrets:
      - name: registry-credentials

      # Spread pods across nodes
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - myapp
              topologyKey: kubernetes.io/hostname
```

### Service and Ingress

```yaml
# service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp
  namespace: production
  labels:
    app: myapp
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 80
    targetPort: 8000
    protocol: TCP
  - name: metrics
    port: 9090
    targetPort: 9090
    protocol: TCP
  selector:
    app: myapp

---
# ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myapp
  namespace: production
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/rate-limit: "100"
spec:
  tls:
  - hosts:
    - myapp.example.com
    secretName: myapp-tls
  rules:
  - host: myapp.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: myapp
            port:
              number: 80
```

### Horizontal Pod Autoscaler

```yaml
# hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Max
```

## Secret Management

### Kubernetes Secrets

```yaml
# secrets.yaml (encrypted with Sealed Secrets or SOPS)
apiVersion: v1
kind: Secret
metadata:
  name: myapp-secrets
  namespace: production
type: Opaque
stringData:
  database_url: "postgresql://user:password@postgres:5432/myapp"
  api_key: "sk_live_abc123xyz"
  redis_url: "redis://:password@redis:6379/0"
```

### Using file:// Prefix for Secrets

Foundation supports reading secrets from files:

```python
# Application code
import os

# Kubernetes mounts secrets as files
os.environ["DATABASE_PASSWORD"] = "file:///run/secrets/db_password"
os.environ["API_KEY"] = "file:///run/secrets/api_key"

# Foundation automatically reads from files
from provide.foundation.utils.environment import get_str

db_password = get_str("DATABASE_PASSWORD")  # Reads from /run/secrets/db_password
api_key = get_str("API_KEY")  # Reads from /run/secrets/api_key
```

### External Secrets Operator

```yaml
# external-secret.yaml
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: myapp-secrets
  namespace: production
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: aws-secretsmanager
    kind: ClusterSecretStore
  target:
    name: myapp-secrets
    creationPolicy: Owner
  data:
  - secretKey: database_url
    remoteRef:
      key: prod/myapp/database_url
  - secretKey: api_key
    remoteRef:
      key: prod/myapp/api_key
```

## Multi-Environment Configuration

### Environment-Specific Configs

```yaml
# kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: production

commonLabels:
  app: myapp
  environment: production

resources:
- deployment.yaml
- service.yaml
- ingress.yaml
- hpa.yaml

configMapGenerator:
- name: myapp-config
  literals:
  - PROVIDE_LOG_LEVEL=INFO
  - PROVIDE_LOG_FORMAT=json
  - FEATURE_FLAGS={"new_ui":true,"beta":false}

secretGenerator:
- name: myapp-secrets
  files:
  - database_url=secrets/prod/database_url
  - api_key=secrets/prod/api_key

images:
- name: registry.example.com/myapp
  newTag: 1.0.0

replicas:
- name: myapp
  count: 5
```

### Overlays for Environments

```bash
# Directory structure
k8s/
â”œâ”€â”€ base/
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ service.yaml
â”‚   â””â”€â”€ kustomization.yaml
â””â”€â”€ overlays/
    â”œâ”€â”€ dev/
    â”‚   â””â”€â”€ kustomization.yaml
    â”œâ”€â”€ staging/
    â”‚   â””â”€â”€ kustomization.yaml
    â””â”€â”€ production/
        â””â”€â”€ kustomization.yaml

# Deploy to staging
kubectl apply -k k8s/overlays/staging

# Deploy to production
kubectl apply -k k8s/overlays/production
```

## Zero-Downtime Deployments

### Rolling Update Strategy

```python
# In your application
from provide.foundation import get_hub, logger
import signal
import sys

hub = get_hub()

def graceful_shutdown(signum, frame):
    """Handle graceful shutdown."""
    logger.info("shutdown_signal_received", signal=signum)

    # Stop accepting new requests
    logger.info("stopping_http_server")
    http_server.stop()

    # Wait for in-flight requests to complete
    logger.info("waiting_for_requests", timeout=30)
    http_server.wait_for_completion(timeout=30)

    # Cleanup resources
    logger.info("cleaning_up_resources")
    hub.shutdown()

    logger.info("shutdown_complete")
    sys.exit(0)

# Register signal handlers
signal.signal(signal.SIGTERM, graceful_shutdown)
signal.signal(signal.SIGINT, graceful_shutdown)

logger.info("application_started", version=os.getenv("APP_VERSION"))
```

### PreStop Hook

```yaml
# In deployment.yaml
lifecycle:
  preStop:
    exec:
      # Sleep to allow load balancer to remove pod
      command: ["/bin/sh", "-c", "sleep 15"]
```

## Helm Charts

### Chart Structure

```yaml
# Chart.yaml
apiVersion: v2
name: myapp
description: My Foundation Application
version: 1.0.0
appVersion: "1.0.0"

---
# values.yaml
replicaCount: 3

image:
  repository: registry.example.com/myapp
  tag: "1.0.0"
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: true
  className: nginx
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
  hosts:
    - host: myapp.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: myapp-tls
      hosts:
        - myapp.example.com

resources:
  requests:
    memory: "256Mi"
    cpu: "250m"
  limits:
    memory: "512Mi"
    cpu: "500m"

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

env:
  PROVIDE_LOG_LEVEL: INFO
  PROVIDE_LOG_FORMAT: json
  PROVIDE_SERVICE_NAME: myapp
```

### Installing with Helm

```bash
# Install
helm install myapp ./myapp-chart \
  --namespace production \
  --create-namespace \
  --values values-prod.yaml

# Upgrade
helm upgrade myapp ./myapp-chart \
  --namespace production \
  --values values-prod.yaml

# Rollback if needed
helm rollback myapp 1 --namespace production
```

## CI/CD Integration

### GitHub Actions Workflow

```yaml
# .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    tags:
      - 'v*'

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Login to Registry
        uses: docker/login-action@v2
        with:
          registry: registry.example.com
          username: ${{ secrets.REGISTRY_USERNAME }}
          password: ${{ secrets.REGISTRY_PASSWORD }}

      - name: Extract version
        id: version
        run: echo "VERSION=${GITHUB_REF#refs/tags/v}" >> $GITHUB_OUTPUT

      - name: Build and push
        uses: docker/build-push-action@v4
        with:
          context: .
          push: true
          tags: |
            registry.example.com/myapp:${{ steps.version.outputs.VERSION }}
            registry.example.com/myapp:latest
          cache-from: type=registry,ref=registry.example.com/myapp:buildcache
          cache-to: type=registry,ref=registry.example.com/myapp:buildcache,mode=max

      - name: Deploy to Kubernetes
        uses: azure/k8s-deploy@v4
        with:
          manifests: |
            k8s/production/deployment.yaml
            k8s/production/service.yaml
          images: registry.example.com/myapp:${{ steps.version.outputs.VERSION }}
          kubectl-version: latest
```

## Best Practices

### âœ… DO: Use Non-Root Users

```dockerfile
# âœ… GOOD: Run as non-root
RUN useradd -m -u 1000 appuser
USER appuser
```

### âŒ DON'T: Run as Root

```dockerfile
# âŒ BAD: Security risk
USER root  # Don't run containers as root!
```

### âœ… DO: Set Resource Limits

```yaml
# âœ… GOOD: Define resource limits
resources:
  requests:
    memory: "256Mi"
    cpu: "250m"
  limits:
    memory: "512Mi"
    cpu: "500m"
```

### âŒ DON'T: Omit Resource Limits

```yaml
# âŒ BAD: No limits can cause node instability
resources: {}  # Missing limits!
```

### âœ… DO: Implement Health Checks

```yaml
# âœ… GOOD: Both liveness and readiness
livenessProbe:
  httpGet:
    path: /health/live
    port: 8000
readinessProbe:
  httpGet:
    path: /health/ready
    port: 8000
```

### âŒ DON'T: Skip Health Checks

```yaml
# âŒ BAD: No way to detect unhealthy pods
# Missing probes!
```

### âœ… DO: Use Multi-Stage Builds

```dockerfile
# âœ… GOOD: Smaller final image
FROM python:3.11-slim as builder
# ... build steps ...

FROM python:3.11-slim
COPY --from=builder /app/.venv /app/.venv
```

### âŒ DON'T: Include Build Tools in Production

```dockerfile
# âŒ BAD: Unnecessarily large image
FROM python:3.11
RUN apt-get install build-essential  # Not needed in production!
```

### âœ… DO: Version Your Images

```bash
# âœ… GOOD: Semantic versioning
docker tag myapp:1.2.3 registry.example.com/myapp:1.2.3
```

### âŒ DON'T: Use Only 'latest'

```bash
# âŒ BAD: Can't rollback or track versions
docker tag myapp registry.example.com/myapp:latest  # Only using latest!
```

### âœ… DO: Manage Secrets Securely

```yaml
# âœ… GOOD: Use Kubernetes secrets
env:
- name: API_KEY
  valueFrom:
    secretKeyRef:
      name: app-secrets
      key: api_key
```

### âŒ DON'T: Hardcode Secrets

```yaml
# âŒ BAD: Secrets in plain text
env:
- name: API_KEY
  value: "sk_live_abc123"  # NEVER do this!
```

### âœ… DO: Configure Graceful Shutdown

```python
# âœ… GOOD: Handle SIGTERM gracefully
def graceful_shutdown(signum, frame):
    logger.info("shutting_down")
    server.stop()
    cleanup_resources()
    sys.exit(0)

signal.signal(signal.SIGTERM, graceful_shutdown)
```

### âŒ DON'T: Ignore Shutdown Signals

```python
# âŒ BAD: Abrupt shutdown can lose data
# No signal handling = killed immediately
```

## Next Steps

### Related Guides
- **[Monitoring & Observability](monitoring.md)**: Monitor production deployments
- **[Basic Logging](../logging/basic-logging.md)**: Production logging setup
- **[Configuration](../configuration/env-variables.md)**: Environment configuration

### Examples
- See `examples/deployment/` for deployment templates
- See `examples/production/10_graceful_shutdown.py` for shutdown patterns

### API Reference
- **[Hub API](../../reference/provide/foundation/hub/index.md)**: Application lifecycle
- **[Config API](../../reference/provide/foundation/config/index.md)**: Configuration management

---

**Tip**: Start with Docker Compose for local development, then move to Kubernetes for production. Always use health checks, resource limits, and graceful shutdown. Implement rolling updates for zero-downtime deployments. Keep secrets in secure stores, never in code or config files.
>>> EOF >>>

### FILE 34: how-to-guides/production/monitoring.md | checksum=123478e1e80b... | modified=2025-10-24T19:37:29 | op=+ | size=20441 | tokens=4513 | type=markdown ###
<<< BOF <<<
# Monitoring & Observability

Learn how to monitor Foundation applications in production with structured logging, metrics, tracing, and alerting.

## Overview

Production monitoring is essential for maintaining reliable services. Foundation provides built-in support for structured logging, metrics collection, distributed tracing, and integration with observability platforms. This guide shows you how to implement comprehensive monitoring for your applications.

**What you'll learn:**
- Configure structured JSON logging for log aggregation
- Collect and export application metrics
- Implement distributed tracing
- Set up health checks and readiness probes
- Integrate with observability platforms (Datadog, Grafana, etc.)
- Monitor performance and resource usage
- Create alerts and dashboards
- Troubleshoot production issues

**Key Features:**
- ðŸ“Š **Structured Logging**: JSON logs with semantic events
- ðŸ“ˆ **Metrics Collection**: Built-in counters, gauges, and histograms
- ðŸ” **Distributed Tracing**: OpenTelemetry integration
- â¤ï¸ **Health Checks**: Liveness and readiness endpoints
- ðŸš¨ **Alerting**: Integration with PagerDuty, Slack, etc.
- ðŸ“‰ **Dashboards**: Pre-built Grafana dashboards

## Prerequisites

```bash
# Core observability
pip install provide-foundation

# OpenTelemetry support
pip install provide-foundation[otel]

# Prometheus metrics export
pip install provide-foundation[prometheus]
```

## Structured Logging for Observability

### JSON Log Format

Enable JSON logging for log aggregation systems:

```python
from provide.foundation import get_hub, logger
from provide.foundation.logger.config import TelemetryConfig, LoggingConfig

# Configure JSON logging
hub = get_hub()
hub.initialize_foundation(
    TelemetryConfig(
        service_name="my-service",
        logging=LoggingConfig(
            log_format="json",
            log_level="INFO"
        )
    )
)

# All logs are now JSON
logger.info(
    "user_login",
    user_id="user_123",
    source="web_app",
    duration_ms=45.2
)
```

**Output:**
```json
{
  "event": "user_login",
  "level": "info",
  "timestamp": "2025-10-24T17:30:00.000Z",
  "user_id": "user_123",
  "source": "web_app",
  "duration_ms": 45.2,
  "service": "my-service"
}
```

### Log Aggregation Integration

Configure for popular log aggregation platforms:

```python
import os
from provide.foundation import logger

# Environment-based configuration
environment = os.getenv("ENVIRONMENT", "dev")
service_name = os.getenv("SERVICE_NAME", "app")
version = os.getenv("APP_VERSION", "unknown")

# Add standard fields to all logs
def add_standard_fields(logger_instance, method_name, event_dict):
    """Add standard observability fields."""
    event_dict.update({
        "service": service_name,
        "version": version,
        "environment": environment,
        "host": os.getenv("HOSTNAME", "unknown")
    })
    return event_dict

hub = get_hub()
hub.initialize_foundation(
    TelemetryConfig(
        service_name=service_name,
        logging=LoggingConfig(
            processors=[add_standard_fields],
            log_format="json"
        )
    )
)
```

### Semantic Event Logging

Use meaningful event names for better searchability:

```python
from provide.foundation import logger

# âœ… GOOD: Semantic event names (Domain-Action-Status pattern)
logger.info("http_request_completed", method="GET", path="/api/users", status=200, duration_ms=123)
logger.error("database_query_failed", query="SELECT * FROM users", error="connection timeout")
logger.info("cache_hit", key="user:123", ttl_seconds=300)

# These events are easily searchable in your log aggregation system:
# - Filter by event type: event:http_request_completed
# - Find all errors: level:error
# - Track performance: event:http_request_completed AND duration_ms:>1000
```

## Metrics Collection

### Counter Metrics

Track event counts and rates:

```python
from provide.foundation.metrics import Counter
from provide.foundation import logger

# Create counters
http_requests_total = Counter(
    name="http_requests_total",
    description="Total HTTP requests",
    labels=["method", "path", "status"]
)

api_errors_total = Counter(
    name="api_errors_total",
    description="Total API errors",
    labels=["error_type"]
)

# Increment counters
def handle_request(method: str, path: str):
    """Handle HTTP request."""
    try:
        result = process_request(method, path)
        http_requests_total.inc(labels={"method": method, "path": path, "status": "200"})
        return result
    except ValueError as e:
        api_errors_total.inc(labels={"error_type": "validation_error"})
        http_requests_total.inc(labels={"method": method, "path": path, "status": "400"})
        raise
    except Exception as e:
        api_errors_total.inc(labels={"error_type": "internal_error"})
        http_requests_total.inc(labels={"method": method, "path": path, "status": "500"})
        raise
```

### Gauge Metrics

Track current values that can go up and down:

```python
from provide.foundation.metrics import Gauge

# Create gauges
active_connections = Gauge(
    name="active_connections",
    description="Number of active database connections"
)

queue_size = Gauge(
    name="queue_size",
    description="Current queue depth",
    labels=["queue_name"]
)

memory_usage_bytes = Gauge(
    name="memory_usage_bytes",
    description="Current memory usage in bytes"
)

# Update gauges
def update_connection_gauge():
    """Update connection pool gauge."""
    pool = get_connection_pool()
    active_connections.set(pool.active_count)

def update_queue_gauge(queue_name: str, size: int):
    """Update queue size gauge."""
    queue_size.set(size, labels={"queue_name": queue_name})
```

### Histogram Metrics

Track distributions of values (latency, size, etc.):

```python
from provide.foundation.metrics import Histogram
import time

# Create histograms
http_request_duration_seconds = Histogram(
    name="http_request_duration_seconds",
    description="HTTP request latency",
    labels=["method", "path"],
    buckets=[0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0]
)

response_size_bytes = Histogram(
    name="response_size_bytes",
    description="HTTP response size",
    buckets=[100, 1000, 10000, 100000, 1000000]
)

# Measure durations
def handle_api_request(method: str, path: str):
    """Handle API request with metrics."""
    start_time = time.time()

    try:
        response = process_request(method, path)

        # Record duration
        duration = time.time() - start_time
        http_request_duration_seconds.observe(
            duration,
            labels={"method": method, "path": path}
        )

        # Record response size
        response_size_bytes.observe(len(response))

        return response

    except Exception as e:
        # Still record duration for failed requests
        duration = time.time() - start_time
        http_request_duration_seconds.observe(
            duration,
            labels={"method": method, "path": path}
        )
        raise
```

## Distributed Tracing

### OpenTelemetry Integration

Configure distributed tracing with OpenTelemetry:

```python
from provide.foundation import get_hub, logger
from provide.foundation.logger.config import TelemetryConfig
from provide.foundation.tracer import create_tracer

# Configure with OpenTelemetry
hub = get_hub()
hub.initialize_foundation(
    TelemetryConfig(
        service_name="my-service",
        enable_tracing=True,
        otlp_endpoint="http://otel-collector:4317"
    )
)

# Create tracer
tracer = create_tracer("my-service")

# Trace operations
def process_order(order_id: str):
    """Process an order with tracing."""
    with tracer.start_as_current_span("process_order") as span:
        span.set_attribute("order.id", order_id)

        # Log within trace context
        logger.info("order_processing_started", order_id=order_id)

        # Child span for database query
        with tracer.start_as_current_span("fetch_order_data") as db_span:
            db_span.set_attribute("db.system", "postgresql")
            order_data = fetch_from_db(order_id)

        # Child span for payment processing
        with tracer.start_as_current_span("process_payment") as payment_span:
            payment_span.set_attribute("payment.amount", order_data["total"])
            process_payment(order_data)

        logger.info("order_processing_completed", order_id=order_id)
```

### Trace Context Propagation

Propagate trace context across service boundaries:

```python
from provide.foundation.tracer import inject_trace_context, extract_trace_context
import requests

def call_downstream_service(url: str, data: dict):
    """Call downstream service with trace context."""
    # Inject trace context into headers
    headers = {}
    inject_trace_context(headers)

    # Make request with propagated context
    response = requests.post(
        url,
        json=data,
        headers=headers,
        timeout=30
    )

    return response.json()

def handle_incoming_request(headers: dict, body: dict):
    """Handle request and extract trace context."""
    # Extract trace context from headers
    trace_context = extract_trace_context(headers)

    with tracer.start_as_current_span("handle_request", context=trace_context) as span:
        # Process request with correct trace parent
        result = process_request(body)
        return result
```

## Health Checks

### Liveness and Readiness Probes

Implement health check endpoints:

```python
from provide.foundation import get_hub, logger
from dataclasses import dataclass
from datetime import datetime

@dataclass
class HealthCheck:
    """Health check result."""
    healthy: bool
    components: dict[str, bool]
    version: str
    uptime_seconds: float

class HealthMonitor:
    """Monitor application health."""

    def __init__(self):
        self.start_time = datetime.now()
        self.hub = get_hub()

    async def liveness_probe(self) -> bool:
        """Check if application is alive (basic check)."""
        # Application is alive if it can respond
        return True

    async def readiness_probe(self) -> HealthCheck:
        """Check if application is ready to serve traffic."""
        components = {}

        # Check database connectivity
        try:
            db_pool = self.hub.get_component("database")
            await db_pool.execute("SELECT 1")
            components["database"] = True
        except Exception as e:
            logger.error("database_health_check_failed", error=str(e))
            components["database"] = False

        # Check cache connectivity
        try:
            cache = self.hub.get_component("cache")
            await cache.ping()
            components["cache"] = True
        except Exception as e:
            logger.error("cache_health_check_failed", error=str(e))
            components["cache"] = False

        # Check external API
        try:
            api_client = self.hub.get_component("api_client")
            await api_client.health_check()
            components["external_api"] = True
        except Exception as e:
            logger.warning("external_api_health_check_failed", error=str(e))
            components["external_api"] = False

        # Overall health
        healthy = all(components.values())
        uptime = (datetime.now() - self.start_time).total_seconds()

        return HealthCheck(
            healthy=healthy,
            components=components,
            version=os.getenv("APP_VERSION", "unknown"),
            uptime_seconds=uptime
        )

# Flask/FastAPI integration
from fastapi import FastAPI, Response

app = FastAPI()
health_monitor = HealthMonitor()

@app.get("/health/live")
async def liveness():
    """Liveness probe endpoint."""
    is_alive = await health_monitor.liveness_probe()
    if is_alive:
        return {"status": "alive"}
    return Response(status_code=503)

@app.get("/health/ready")
async def readiness():
    """Readiness probe endpoint."""
    health = await health_monitor.readiness_probe()

    if health.healthy:
        return health.__dict__
    return Response(
        content=json.dumps(health.__dict__),
        status_code=503,
        media_type="application/json"
    )
```

## Alerting

### Error Rate Alerts

Monitor error rates and alert on anomalies:

```python
from provide.foundation import logger
from provide.foundation.metrics import Counter
from datetime import datetime, timedelta
import asyncio

error_counter = Counter("errors_total", labels=["severity"])

class ErrorRateMonitor:
    """Monitor and alert on error rates."""

    def __init__(self, threshold: float = 0.05):
        self.threshold = threshold  # 5% error rate
        self.window_size = timedelta(minutes=5)
        self.error_count = 0
        self.total_count = 0
        self.window_start = datetime.now()

    def record_request(self, success: bool):
        """Record a request outcome."""
        self.total_count += 1

        if not success:
            self.error_count += 1
            error_counter.inc(labels={"severity": "error"})

        # Reset window if needed
        if datetime.now() - self.window_start > self.window_size:
            if self.total_count > 0:
                error_rate = self.error_count / self.total_count

                if error_rate > self.threshold:
                    self.trigger_alert(error_rate)

            # Reset counters
            self.error_count = 0
            self.total_count = 0
            self.window_start = datetime.now()

    def trigger_alert(self, error_rate: float):
        """Trigger alert for high error rate."""
        logger.error(
            "high_error_rate_detected",
            error_rate=error_rate,
            threshold=self.threshold,
            window_minutes=self.window_size.total_seconds() / 60
        )

        # Send to alerting platform
        send_pagerduty_alert(
            title=f"High error rate: {error_rate:.2%}",
            severity="error",
            details={
                "error_rate": error_rate,
                "threshold": self.threshold
            }
        )
```

### Performance Degradation Alerts

Alert on latency spikes:

```python
from provide.foundation.metrics import Histogram
import statistics

latency_histogram = Histogram("request_latency_seconds")

class LatencyMonitor:
    """Monitor request latency."""

    def __init__(self, p95_threshold: float = 1.0):
        self.p95_threshold = p95_threshold
        self.recent_latencies: list[float] = []
        self.max_samples = 1000

    def record_latency(self, latency: float):
        """Record a request latency."""
        latency_histogram.observe(latency)
        self.recent_latencies.append(latency)

        # Keep only recent samples
        if len(self.recent_latencies) > self.max_samples:
            self.recent_latencies.pop(0)

        # Check p95
        if len(self.recent_latencies) >= 100:
            p95 = statistics.quantiles(self.recent_latencies, n=20)[18]  # 95th percentile

            if p95 > self.p95_threshold:
                logger.warning(
                    "high_latency_detected",
                    p95_latency=p95,
                    threshold=self.p95_threshold
                )
```

## Dashboard Integration

### Prometheus Metrics Export

Export metrics in Prometheus format:

```python
from prometheus_client import start_http_server, Counter, Histogram, Gauge
from provide.foundation import logger

# Start Prometheus metrics server
start_http_server(9090)
logger.info("prometheus_metrics_enabled", port=9090)

# Metrics are now available at http://localhost:9090/metrics
```

### Grafana Dashboard JSON

Example Grafana dashboard configuration:

```json
{
  "dashboard": {
    "title": "My Service Dashboard",
    "panels": [
      {
        "title": "Request Rate",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])"
          }
        ]
      },
      {
        "title": "Error Rate",
        "targets": [
          {
            "expr": "rate(api_errors_total[5m]) / rate(http_requests_total[5m])"
          }
        ]
      },
      {
        "title": "Latency p95",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, http_request_duration_seconds)"
          }
        ]
      }
    ]
  }
}
```

## Best Practices

### âœ… DO: Use Structured Logging

```python
# âœ… GOOD: Structured events with context
logger.info(
    "payment_processed",
    payment_id="pay_123",
    amount_cents=1000,
    currency="USD",
    duration_ms=123
)
```

### âŒ DON'T: Log Unstructured Strings

```python
# âŒ BAD: Hard to parse and search
logger.info(f"Processed payment pay_123 for $10.00 in 123ms")
```

### âœ… DO: Track Key Metrics

```python
# âœ… GOOD: Track important business metrics
transactions_total.inc(labels={"type": "purchase", "status": "success"})
revenue_total.inc(amount_cents)
```

### âŒ DON'T: Ignore Performance Metrics

```python
# âŒ BAD: No observability into performance
def process_order(order):
    result = slow_operation()  # How slow? No idea!
    return result
```

### âœ… DO: Implement Health Checks

```python
# âœ… GOOD: Comprehensive health checks
@app.get("/health/ready")
async def readiness():
    checks = {
        "database": await check_database(),
        "cache": await check_cache(),
        "api": await check_external_api()
    }
    healthy = all(checks.values())
    return {"healthy": healthy, "checks": checks}
```

### âŒ DON'T: Skip Health Endpoints

```python
# âŒ BAD: No way for orchestrator to check health
# Missing /health endpoints means blind deployments
```

### âœ… DO: Use Consistent Event Names

```python
# âœ… GOOD: Domain-Action-Status pattern
logger.info("database_query_started", table="users")
logger.info("database_query_completed", table="users", duration_ms=45)
logger.error("database_query_failed", table="users", error="timeout")
```

### âŒ DON'T: Use Random Event Names

```python
# âŒ BAD: Inconsistent naming
logger.info("query started")
logger.info("completed db query")
logger.error("DB ERROR!")
```

### âœ… DO: Monitor Error Budgets

```python
# âœ… GOOD: Track SLIs and error budgets
class SLIMonitor:
    """Track Service Level Indicators."""

    def __init__(self, error_budget: float = 0.001):  # 99.9% SLO
        self.error_budget = error_budget
        self.success_count = 0
        self.total_count = 0

    def record(self, success: bool):
        """Record request outcome."""
        self.total_count += 1
        if success:
            self.success_count += 1

    def current_sli(self) -> float:
        """Get current SLI."""
        if self.total_count == 0:
            return 1.0
        return self.success_count / self.total_count

    def budget_remaining(self) -> float:
        """Get remaining error budget."""
        sli = self.current_sli()
        target = 1.0 - self.error_budget
        return max(0, sli - target)
```

### âœ… DO: Set Up Alerts for Critical Issues

```python
# âœ… GOOD: Alert on critical conditions
if error_rate > 0.05:  # 5% errors
    send_pagerduty_alert("High error rate", severity="critical")

if p95_latency > 1.0:  # 1 second p95
    send_slack_alert("Latency spike detected", channel="#ops")
```

### âŒ DON'T: Alert on Everything

```python
# âŒ BAD: Too many alerts cause alert fatigue
for log_line in logs:
    if "error" in log_line.lower():
        send_pagerduty_alert(log_line)  # Way too noisy!
```

## Next Steps

### Related Guides
- **[Deployment Patterns](deployment.md)**: Deploy to production
- **[Structured Events](../logging/structured-events.md)**: Event naming conventions
- **[Basic Logging](../logging/basic-logging.md)**: Logging fundamentals

### Examples
- See `examples/production/08_monitoring.py` for monitoring examples
- See `examples/production/09_health_checks.py` for health check patterns

### API Reference
- **[Metrics API](../../reference/provide/foundation/metrics/index.md)**: Metrics collection
- **[Tracer API](../../reference/provide/foundation/tracer/index.md)**: Distributed tracing

---

**Tip**: Start with structured JSON logging and basic metrics. Add distributed tracing as your system grows. Focus on monitoring what matters: error rates, latency, and business metrics. Use health checks to enable zero-downtime deployments.
>>> EOF >>>

### FILE 35: how-to-guides/profiling/performance-profiling.md | checksum=e69d41795492... | modified=2025-10-24T20:13:26 | op=+ | size=2009 | tokens=452 | type=markdown ###
<<< BOF <<<
# How to Use Performance Profiling

Foundation provides lightweight performance profiling for monitoring logging and telemetry performance.

## Quick Start

```python
from provide.foundation.profiling import register_profiling
from provide.foundation import get_hub

# Register profiling component
hub = get_hub()
register_profiling(hub)

# Get profiler
profiler = hub.get_component("profiler")

# Enable profiling
profiler.enable()

# Your application code here
# ...

# Get metrics
metrics = profiler.get_metrics()
print(f"Processing {metrics.messages_per_second:.0f} msg/sec")
print(f"Average latency: {metrics.avg_latency_ms:.2f}ms")
```

## ProfileMetrics

Access performance metrics:

```python
from provide.foundation.profiling import ProfileMetrics

metrics = profiler.get_metrics()

# Message throughput
print(f"Messages/sec: {metrics.messages_per_second}")
print(f"Total messages: {metrics.total_messages}")

# Latency stats
print(f"Avg latency: {metrics.avg_latency_ms}ms")
print(f"P95 latency: {metrics.p95_latency_ms}ms")
print(f"Max latency: {metrics.max_latency_ms}ms")

# Error rate
print(f"Error rate: {metrics.error_rate}%")
```

## Best Practices

### âœ… DO: Enable in Development

```python
# âœ… Good: Profile in development
import os
from provide.foundation.profiling import register_profiling

if os.getenv("ENVIRONMENT") == "development":
    register_profiling(hub)
    profiler = hub.get_component("profiler")
    profiler.enable()
```

### âŒ DON'T: Leave Enabled in Production

Profiling adds overhead - use only when needed:

```python
# âŒ Bad: Always enabled
profiler.enable()  # Impacts performance!

# âœ… Good: Conditional enabling
if need_profiling:
    profiler.enable()
```

## Next Steps

- **[Logging](../logging/basic-logging.md)**: Structured logging
- **[Metrics](../observability/metrics.md)**: Application metrics

---

**Note**: Profiling is designed for Foundation internals. For application-level profiling, use Python's built-in profilers or external tools.
>>> EOF >>>

### FILE 36: how-to-guides/resilience/circuit-breaker.md | checksum=df2453298e56... | modified=2025-10-24T18:20:08 | op=+ | size=16124 | tokens=3536 | type=markdown ###
<<< BOF <<<
# Circuit Breakers

Learn how to use circuit breakers to prevent cascading failures and protect your system from overload.

## Overview

Circuit breakers automatically stop calling failing services to prevent resource exhaustion and cascading failures. When a service fails repeatedly, the circuit "opens" and immediately rejects calls without attempting the operation, giving the failing service time to recover.

This pattern is essential for building resilient distributed systems where one failing service shouldn't bring down the entire application.

## Circuit States

The circuit breaker has three states:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLOSED  â”‚ â—„â”€â”€â”€ Normal operation
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      All requests pass through
     â”‚           Count failures
     â”‚
     â”‚ Failure threshold exceeded
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OPEN   â”‚ â—„â”€â”€â”€ Circuit tripped
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      Requests fail immediately
     â”‚           No calls to service
     â”‚
     â”‚ Timeout expires
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚HALF-OPEN â”‚ â—„â”€â”€â”€ Testing recovery
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     Allow one test request
     â”‚
     â”œâ”€â”€â”€ Success: â†’ CLOSED
     â””â”€â”€â”€ Failure: â†’ OPEN
```

### Closed State
- **Normal operation** - All requests pass through
- **Monitoring** - Counts failures
- **Transition** - Opens when failure threshold is exceeded

### Open State
- **Protection mode** - Requests fail immediately with `CircuitBreakerOpen` exception
- **No service calls** - Gives failing service time to recover
- **Transition** - Moves to half-open after timeout period

### Half-Open State
- **Recovery testing** - Allows limited test requests
- **Evaluation** - Monitors if service has recovered
- **Transition** - Returns to closed if tests succeed, back to open if they fail

## Basic Circuit Breaker

```python
from provide.foundation.resilience import circuit_breaker

@circuit_breaker(failure_threshold=5, timeout=60)
def call_external_api():
    """Call an external API with circuit breaker protection."""
    response = requests.get("https://api.example.com/data")
    return response.json()
```

**How it works:**
- After **5 consecutive failures**, the circuit opens
- Circuit stays open for **60 seconds**
- After timeout, allows one test request (half-open)
- If test succeeds, circuit closes; if it fails, stays open for another 60 seconds

## Configuration Options

### Failure Threshold

Number of consecutive failures before opening the circuit:

```python
@circuit_breaker(
    failure_threshold=3,  # Open after 3 failures
    timeout=30,
)
def unreliable_service():
    """Service with low failure tolerance."""
    pass
```

### Timeout Duration

How long the circuit stays open before testing recovery:

```python
@circuit_breaker(
    failure_threshold=5,
    timeout=120,  # Stay open for 2 minutes
)
def slow_recovery_service():
    """Service that needs time to recover."""
    pass
```

### Success Threshold (Half-Open)

Number of successful requests needed to close circuit from half-open:

```python
@circuit_breaker(
    failure_threshold=5,
    timeout=60,
    success_threshold=3,  # Need 3 successes to fully close
)
def cautious_recovery():
    """Require multiple successes before trusting service again."""
    pass
```

### Failure Window

Track failures within a time window instead of consecutively:

```python
@circuit_breaker(
    failure_threshold=10,
    failure_window=60,  # 10 failures within 60 seconds opens circuit
    timeout=120,
)
def rate_based_protection():
    """Open circuit based on failure rate, not consecutive failures."""
    pass
```

## Custom Failure Predicates

Define custom logic to determine what constitutes a "failure":

```python
def is_retriable_error(exception):
    """Only count certain errors as circuit breaker failures."""
    # Don't open circuit for client errors (4xx)
    if isinstance(exception, HTTPError):
        return exception.status_code >= 500  # Only server errors
    # Count network errors
    return isinstance(exception, (ConnectionError, TimeoutError))

@circuit_breaker(
    failure_threshold=5,
    timeout=60,
    failure_predicate=is_retriable_error,
)
def smart_http_call(url):
    """Circuit breaker that ignores client errors."""
    response = requests.get(url)
    if response.status_code >= 400:
        raise HTTPError(response.status_code)
    return response.json()
```

## Monitoring Circuit State

Check the circuit state programmatically:

```python
from provide.foundation.resilience import CircuitBreaker

# Create a reusable circuit breaker
api_circuit = CircuitBreaker(
    failure_threshold=5,
    timeout=60,
    name="external_api_circuit",
)

@api_circuit.protect
def call_api():
    """Protected by named circuit breaker."""
    return requests.get("https://api.example.com/data").json()

# Check circuit state
if api_circuit.state == CircuitState.OPEN:
    logger.warning("Circuit is open, API calls are being rejected")
elif api_circuit.state == CircuitState.HALF_OPEN:
    logger.info("Circuit is testing recovery")
else:
    logger.info("Circuit is closed, operating normally")

# Get circuit metrics
logger.info(
    "Circuit metrics",
    failure_count=api_circuit.failure_count,
    success_count=api_circuit.success_count,
    last_failure_time=api_circuit.last_failure_time,
)
```

## Handling Circuit Open Exceptions

Handle circuit breaker open exceptions gracefully:

```python
from provide.foundation.resilience import circuit_breaker, CircuitBreakerOpen
from provide.foundation import logger

@circuit_breaker(failure_threshold=3, timeout=30)
def fetch_user_data(user_id):
    """Fetch user data with circuit protection."""
    return api_client.get(f"/users/{user_id}")

def get_user_with_fallback(user_id):
    """Get user data with fallback when circuit is open."""
    try:
        return fetch_user_data(user_id)
    except CircuitBreakerOpen:
        logger.warning(
            "Circuit breaker open, using cached data",
            user_id=user_id,
        )
        # Return cached data or default
        return get_cached_user_data(user_id)
    except Exception as e:
        logger.error("Failed to fetch user data", error=str(e))
        raise
```

## Common Patterns

### Database Connection Protection

```python
@circuit_breaker(
    failure_threshold=3,
    timeout=60,
    success_threshold=2,
)
def execute_database_query(query, params):
    """Execute query with circuit breaker protection."""
    logger.debug("Executing database query", query=query[:100])

    connection = get_database_connection()
    cursor = connection.execute(query, params)
    result = cursor.fetchall()

    logger.debug("Query completed", row_count=len(result))
    return result
```

### Microservice Call Protection

```python
from provide.foundation.resilience import circuit_breaker, CircuitBreakerOpen

class UserServiceClient:
    """Client for user microservice with circuit protection."""

    def __init__(self):
        self.circuit = CircuitBreaker(
            failure_threshold=5,
            timeout=120,
            name="user_service",
        )

    @circuit.protect
    def get_user(self, user_id):
        """Get user from service."""
        response = requests.get(
            f"{self.service_url}/users/{user_id}",
            timeout=5,
        )
        response.raise_for_status()
        return response.json()

    def get_user_safe(self, user_id):
        """Get user with fallback."""
        try:
            return self.get_user(user_id)
        except CircuitBreakerOpen:
            logger.warning("User service circuit open")
            return {"id": user_id, "name": "Unknown"}
```

### External API Protection

```python
@circuit_breaker(
    failure_threshold=10,
    failure_window=60,  # 10 failures in 60 seconds
    timeout=300,  # Stay open for 5 minutes
)
def call_payment_gateway(transaction_data):
    """Call payment gateway with circuit protection."""
    logger.info(
        "Processing payment",
        amount=transaction_data["amount"],
        currency=transaction_data["currency"],
    )

    response = requests.post(
        "https://payment-gateway.example.com/charge",
        json=transaction_data,
        timeout=10,
    )

    if response.status_code >= 500:
        raise ServiceUnavailable("Payment gateway error")

    response.raise_for_status()
    return response.json()
```

## Circuit Breaker with Caching

Combine circuit breaker with caching for maximum resilience:

```python
from functools import lru_cache
from datetime import datetime, timedelta

class CachedAPIClient:
    """API client with circuit breaker and caching."""

    def __init__(self):
        self.circuit = CircuitBreaker(
            failure_threshold=5,
            timeout=60,
        )
        self.cache = {}
        self.cache_ttl = timedelta(minutes=5)

    def get_data(self, key):
        """Get data with circuit breaker and cache fallback."""
        # Check cache first
        cached = self._get_from_cache(key)
        if cached:
            return cached

        # Try to fetch from API
        try:
            data = self._fetch_from_api(key)
            self._store_in_cache(key, data)
            return data
        except CircuitBreakerOpen:
            logger.warning(
                "Circuit open, using stale cache",
                key=key,
            )
            # Return stale cache if available
            return self._get_stale_cache(key)

    @circuit.protect
    def _fetch_from_api(self, key):
        """Fetch from API (protected by circuit breaker)."""
        response = requests.get(f"https://api.example.com/data/{key}")
        response.raise_for_status()
        return response.json()

    def _get_from_cache(self, key):
        """Get fresh data from cache."""
        if key in self.cache:
            data, timestamp = self.cache[key]
            if datetime.now() - timestamp < self.cache_ttl:
                return data
        return None

    def _get_stale_cache(self, key):
        """Get stale data from cache as fallback."""
        if key in self.cache:
            data, _ = self.cache[key]
            return data
        return None

    def _store_in_cache(self, key, data):
        """Store data in cache."""
        self.cache[key] = (data, datetime.now())
```

## Best Practices

### âœ… DO: Set Appropriate Failure Thresholds

```python
# âœ… Good: Threshold based on expected error rate
@circuit_breaker(
    failure_threshold=5,  # Reasonable for most APIs
    timeout=60,
)
def api_call():
    pass

# âŒ Bad: Threshold too low (overly sensitive)
@circuit_breaker(
    failure_threshold=1,  # Opens on first failure!
    timeout=60,
)
def too_sensitive():
    pass
```

### âœ… DO: Use Different Circuits for Different Services

```python
# âœ… Good: Separate circuits for different services
payment_circuit = CircuitBreaker(failure_threshold=3, timeout=120)
user_circuit = CircuitBreaker(failure_threshold=5, timeout=60)

@payment_circuit.protect
def process_payment():
    pass

@user_circuit.protect
def get_user():
    pass
```

### âœ… DO: Implement Fallbacks

```python
# âœ… Good: Graceful degradation when circuit opens
def get_recommendations(user_id):
    try:
        return fetch_recommendations(user_id)
    except CircuitBreakerOpen:
        # Return default recommendations
        return get_default_recommendations()
```

### âœ… DO: Monitor Circuit State

```python
# âœ… Good: Log circuit state changes
from provide.foundation.resilience import CircuitState

def on_state_change(circuit, old_state, new_state):
    """Called when circuit state changes."""
    logger.warning(
        "Circuit state changed",
        circuit=circuit.name,
        old_state=old_state.name,
        new_state=new_state.name,
    )

api_circuit = CircuitBreaker(
    failure_threshold=5,
    timeout=60,
    on_state_change=on_state_change,
)
```

### âŒ DON'T: Share Circuits Across Unrelated Services

```python
# âŒ Bad: Single circuit for multiple services
@circuit_breaker(failure_threshold=5, timeout=60)
def call_any_service(service_url):
    # One service failure affects all services!
    return requests.get(service_url).json()

# âœ… Good: Separate circuits per service
@user_service_circuit.protect
def call_user_service():
    pass

@payment_service_circuit.protect
def call_payment_service():
    pass
```

### âŒ DON'T: Use Very Short Timeouts

```python
# âŒ Bad: Circuit opens and closes too quickly
@circuit_breaker(
    failure_threshold=3,
    timeout=1,  # Only 1 second - too short!
)
def flaky_call():
    pass

# âœ… Good: Give service time to recover
@circuit_breaker(
    failure_threshold=3,
    timeout=60,  # 1 minute minimum
)
def stable_call():
    pass
```

## Combining Circuit Breaker with Retry

Use both patterns together for maximum resilience:

```python
from provide.foundation.resilience import retry, circuit_breaker

# Circuit breaker on the outside, retry on the inside
@circuit_breaker(failure_threshold=5, timeout=60)
@retry(
    (NetworkError, TimeoutError),
    max_attempts=3,
    base_delay=1.0,
)
def resilient_api_call():
    """
    Protected by both retry and circuit breaker.

    - Retry handles transient failures (3 attempts)
    - Circuit breaker prevents cascading failures
    - If retries repeatedly fail, circuit opens
    """
    return requests.get("https://api.example.com/data").json()
```

**Order matters:**
- **Circuit breaker outside, retry inside** (recommended)
  - Circuit tracks overall failures including retries
  - If service is down, circuit opens and stops retry attempts

- **Retry outside, circuit breaker inside** (not recommended)
  - Retry will attempt even when circuit is open
  - Wastes resources on calls that will fail immediately

## Testing Circuit Breakers

Test circuit behavior in your tests:

```python
import pytest
from provide.foundation.resilience import CircuitBreaker, CircuitBreakerOpen

def test_circuit_opens_after_failures():
    """Test circuit opens after threshold failures."""
    circuit = CircuitBreaker(failure_threshold=3, timeout=60)

    @circuit.protect
    def failing_operation():
        raise RuntimeError("Service unavailable")

    # First 3 calls fail and circuit opens
    for i in range(3):
        with pytest.raises(RuntimeError):
            failing_operation()

    # 4th call rejected by open circuit
    with pytest.raises(CircuitBreakerOpen):
        failing_operation()

def test_circuit_half_open_recovery():
    """Test circuit recovery through half-open state."""
    circuit = CircuitBreaker(
        failure_threshold=2,
        timeout=0.1,  # Short timeout for testing
        success_threshold=2,
    )

    call_count = 0

    @circuit.protect
    def sometimes_failing():
        nonlocal call_count
        call_count += 1
        if call_count <= 2:
            raise RuntimeError("Failing")
        return "Success"

    # Open circuit with failures
    for _ in range(2):
        with pytest.raises(RuntimeError):
            sometimes_failing()

    # Wait for timeout
    time.sleep(0.2)

    # Circuit moves to half-open, test succeeds
    assert sometimes_failing() == "Success"
    assert sometimes_failing() == "Success"

    # Circuit should now be closed
    assert circuit.state == CircuitState.CLOSED
```

## Next Steps

### Related Resilience Patterns
- **[Retry Patterns](retry.md)**: Automatically retry failed operations
- **[Production Monitoring](../production/monitoring.md)**: Monitor circuit breaker metrics

### Examples
- See `examples/production/02_error_handling.py` for circuit breaker examples
- See `examples/transport/01_http_client.py` for HTTP circuit protection

### API Reference
- **[API Reference: Resilience](../../reference/provide/foundation/resilience/index.md)**: Complete API documentation

---

**Tip**: Start with conservative thresholds (5-10 failures) and adjust based on your service's behavior. Always implement fallbacks for when circuits open.
>>> EOF >>>

### FILE 37: how-to-guides/resilience/retry.md | checksum=09589939c87c... | modified=2025-10-24T18:17:07 | op=+ | size=9831 | tokens=2357 | type=markdown ###
<<< BOF <<<
# How to Automatically Retry Operations

Use the `@retry` decorator to make functions resilient to transient failures.

## Overview

The retry pattern automatically re-executes operations that fail due to temporary issues like network timeouts, rate limiting, or service unavailability. Foundation's `@retry` decorator provides flexible retry strategies with exponential backoff, jitter, and custom failure predicates.

## Basic Retry

The decorator will re-execute the function if it raises one of the specified exceptions.

```python
# From: examples/production/02_error_handling.py
from provide.foundation.resilience import retry, BackoffStrategy
from provide.foundation.errors import NetworkError

attempt_count = 0

@retry(
    NetworkError,
    max_attempts=3,
    base_delay=0.1,
    backoff=BackoffStrategy.EXPONENTIAL,
)
def unreliable_api_call():
    """Simulate an unreliable API call."""
    global attempt_count
    attempt_count += 1
    logger.info(f"API call attempt {attempt_count}")
    if attempt_count < 3:
        raise NetworkError(f"API temporarily unavailable (attempt {attempt_count})")
    return {"status": "success"}

try:
    result = unreliable_api_call()
    logger.info("API call succeeded", result=result)
except NetworkError as e:
    logger.error("API call failed after all retries", error=str(e))
```

## Backoff Strategies

Foundation provides several backoff strategies for controlling retry timing:

### Exponential Backoff

Delay doubles with each retry attempt:

```python
from provide.foundation.resilience import retry, BackoffStrategy

@retry(
    ConnectionError,
    max_attempts=5,
    base_delay=1.0,
    backoff=BackoffStrategy.EXPONENTIAL,  # 1s, 2s, 4s, 8s, 16s
)
def connect_to_database():
    """Connect with exponential backoff."""
    pass
```

### Linear Backoff

Delay increases linearly:

```python
@retry(
    TimeoutError,
    max_attempts=5,
    base_delay=2.0,
    backoff=BackoffStrategy.LINEAR,  # 2s, 4s, 6s, 8s, 10s
)
def fetch_data():
    """Fetch data with linear backoff."""
    pass
```

### Fixed Delay

Constant delay between retries:

```python
@retry(
    ValueError,
    max_attempts=3,
    base_delay=1.0,
    backoff=BackoffStrategy.FIXED,  # 1s, 1s, 1s
)
def validate_input(data):
    """Validate with fixed delay."""
    pass
```

## Jitter Configuration

Add randomness to prevent thundering herd problems:

```python
from provide.foundation.resilience import retry, JitterStrategy

@retry(
    NetworkError,
    max_attempts=5,
    base_delay=1.0,
    backoff=BackoffStrategy.EXPONENTIAL,
    jitter=JitterStrategy.FULL,  # Randomize delay: [0, calculated_delay]
)
def distributed_api_call():
    """API call with jitter to prevent simultaneous retries."""
    pass

# Jitter strategies:
# - JitterStrategy.NONE: No randomization
# - JitterStrategy.FULL: Random delay in [0, calculated_delay]
# - JitterStrategy.EQUAL: Random delay in [calculated_delay/2, calculated_delay]
```

## Maximum Delay Limits

Cap the maximum retry delay:

```python
@retry(
    TimeoutError,
    max_attempts=10,
    base_delay=1.0,
    max_delay=30.0,  # Never wait more than 30 seconds
    backoff=BackoffStrategy.EXPONENTIAL,
)
def slow_operation():
    """Operation with capped retry delay."""
    pass
```

## Multiple Exception Types

Retry on any of several exception types:

```python
from provide.foundation.errors import NetworkError, TimeoutError, RateLimitError

@retry(
    (NetworkError, TimeoutError, RateLimitError),  # Tuple of exceptions
    max_attempts=3,
    base_delay=1.0,
)
def multi_failure_operation():
    """Retry on multiple error types."""
    pass
```

## Conditional Retry with Predicates

Use custom logic to determine if a retry should happen:

```python
def should_retry_on_status(exception):
    """Only retry on specific HTTP status codes."""
    if isinstance(exception, HTTPError):
        return exception.status_code in [429, 502, 503, 504]
    return False

@retry(
    HTTPError,
    max_attempts=5,
    base_delay=2.0,
    retry_on=should_retry_on_status,  # Custom predicate
)
def http_request(url):
    """Retry only on specific HTTP errors."""
    pass
```

## Retry with Callbacks

Execute code before/after retry attempts:

```python
from provide.foundation import logger

def before_retry(attempt, exception, delay):
    """Called before each retry."""
    logger.warning(
        "Retrying operation",
        attempt=attempt,
        exception=type(exception).__name__,
        delay_seconds=delay,
    )

def after_retry(attempt, result):
    """Called after successful retry."""
    logger.info("Operation succeeded", attempt=attempt, result=result)

@retry(
    NetworkError,
    max_attempts=3,
    base_delay=1.0,
    on_retry=before_retry,
    on_success=after_retry,
)
def monitored_operation():
    """Operation with retry callbacks for monitoring."""
    pass
```

## Async Function Retries

Retry asynchronous functions:

```python
import asyncio
from provide.foundation.resilience import retry

@retry(
    asyncio.TimeoutError,
    max_attempts=3,
    base_delay=1.0,
)
async def async_api_call():
    """Async operation with automatic retry."""
    async with httpx.AsyncClient() as client:
        response = await client.get("https://api.example.com/data")
        return response.json()
```

## Common Patterns

### Database Connection Retry

```python
@retry(
    (ConnectionError, TimeoutError),
    max_attempts=5,
    base_delay=1.0,
    max_delay=30.0,
    backoff=BackoffStrategy.EXPONENTIAL,
    jitter=JitterStrategy.FULL,
)
def connect_to_database(connection_string):
    """Connect to database with retry logic."""
    logger.info("Attempting database connection")
    db = Database(connection_string)
    db.connect()
    logger.info("Database connection established")
    return db
```

### HTTP API Retry

```python
from provide.foundation.errors import NetworkError, TimeoutError

@retry(
    (NetworkError, TimeoutError, RateLimitError),
    max_attempts=3,
    base_delay=2.0,
    backoff=BackoffStrategy.EXPONENTIAL,
)
def call_external_api(endpoint, payload):
    """Call external API with retry on transient failures."""
    logger.info("API request", endpoint=endpoint)

    response = requests.post(endpoint, json=payload, timeout=10)

    if response.status_code == 429:  # Rate limited
        raise RateLimitError("API rate limit exceeded")
    elif response.status_code >= 500:  # Server error
        raise NetworkError(f"Server error: {response.status_code}")

    response.raise_for_status()
    return response.json()
```

### File Upload Retry

```python
@retry(
    (IOError, NetworkError),
    max_attempts=3,
    base_delay=1.0,
    backoff=BackoffStrategy.EXPONENTIAL,
)
def upload_file(file_path, destination_url):
    """Upload file with retry on network failures."""
    with open(file_path, 'rb') as f:
        files = {'file': f}
        response = requests.post(destination_url, files=files)
        response.raise_for_status()

    logger.info("File uploaded successfully", path=file_path)
```

## Best Practices

### âœ… DO: Use Exponential Backoff with Jitter

```python
# âœ… Good: Prevents thundering herd
@retry(
    NetworkError,
    max_attempts=5,
    base_delay=1.0,
    backoff=BackoffStrategy.EXPONENTIAL,
    jitter=JitterStrategy.FULL,
)
def distributed_call():
    pass
```

### âœ… DO: Set Maximum Delay Limits

```python
# âœ… Good: Prevents indefinite waits
@retry(
    TimeoutError,
    max_attempts=10,
    base_delay=1.0,
    max_delay=60.0,  # Cap at 1 minute
)
def bounded_retry():
    pass
```

### âœ… DO: Retry Only Transient Errors

```python
# âœ… Good: Only retry recoverable errors
@retry(
    (NetworkError, TimeoutError, RateLimitError),
    max_attempts=3,
)
def transient_failures_only():
    pass

# âŒ Bad: Don't retry permanent failures
@retry(
    (ValueError, KeyError),  # These won't fix themselves!
    max_attempts=3,
)
def permanent_failures():
    pass
```

### âœ… DO: Log Retry Attempts

```python
# âœ… Good: Track retry behavior
def log_retry(attempt, exception, delay):
    logger.warning(
        "Retry attempt",
        attempt=attempt,
        exception=str(exception),
        next_delay=delay,
    )

@retry(
    NetworkError,
    max_attempts=3,
    on_retry=log_retry,
)
def monitored_operation():
    pass
```

### âŒ DON'T: Use Excessive Max Attempts

```python
# âŒ Bad: Too many retries
@retry(
    NetworkError,
    max_attempts=100,  # Excessive!
)
def over_retry():
    pass

# âœ… Good: Reasonable retry count
@retry(
    NetworkError,
    max_attempts=3,  # Or 5 at most
)
def reasonable_retry():
    pass
```

## Combining Retry with Circuit Breaker

For maximum resilience, combine retry with circuit breakers:

```python
from provide.foundation.resilience import retry, circuit_breaker

@circuit_breaker(failure_threshold=5, timeout=60)
@retry(
    NetworkError,
    max_attempts=3,
    base_delay=1.0,
)
def resilient_service_call():
    """Protected by both retry and circuit breaker."""
    # Circuit breaker prevents calls if service is down
    # Retry handles transient failures within the circuit
    pass
```

## Next Steps

### Related Resilience Patterns
- **[Circuit Breakers](circuit-breaker.md)**: Prevent cascading failures
- **[Production Monitoring](../production/monitoring.md)**: Monitor retry behavior

### Examples
- See `examples/production/02_error_handling.py` for comprehensive retry examples
- See `examples/transport/01_http_client.py` for HTTP retry patterns

### API Reference
- **[API Reference: Resilience](../../reference/provide/foundation/resilience/index.md)**: Complete API documentation

---

**Tip**: Start with simple fixed retries and add exponential backoff with jitter for production use. Always set reasonable `max_attempts` and `max_delay` limits.
>>> EOF >>>

### FILE 38: how-to-guides/security/security-utilities.md | checksum=25e6344190c9... | modified=2025-10-24T20:38:47 | op=+ | size=8275 | tokens=1919 | type=markdown ###
<<< BOF <<<
# How to Use Security Utilities

Foundation provides security utilities for protecting sensitive data in logs, command output, and API requests.

## Overview

The security module provides two main categories:

- **Secret Masking**: Automatically hide secrets in logs and command output
- **Data Sanitization**: Remove sensitive data from HTTP headers, URIs, and dictionaries

## Secret Masking

### Automatic Secret Detection

Foundation automatically masks common secret patterns:

```python
from provide.foundation.security import mask_secrets

# Masks API keys, tokens, passwords
text = "API_KEY=sk-1234567890abcdef DATABASE_PASSWORD=secret123"
masked = mask_secrets(text)
print(masked)
# "API_KEY=***MASKED*** DATABASE_PASSWORD=***MASKED***"
```

**Default patterns masked:**
- API keys (api_key, apikey)
- Tokens (token, auth_token, access_token)
- Passwords (password, passwd, pwd)
- Secrets (secret, client_secret)
- Credentials (credentials)
- Private keys (private_key, priv_key)

### Custom Secret Patterns

Add your own secret patterns:

```python
from provide.foundation.security import mask_secrets, DEFAULT_SECRET_PATTERNS

# Add custom pattern
custom_patterns = DEFAULT_SECRET_PATTERNS + [
    r"CUSTOM_SECRET=[^\s]+",
    r"SESSION_ID=[^\s]+"
]

text = "CUSTOM_SECRET=abc123 SESSION_ID=xyz789"
masked = mask_secrets(text, patterns=custom_patterns)
```

### Mask Command Output

Protect secrets in shell command output:

```python
from provide.foundation.security import mask_command

# Mask secrets in command strings
command = "curl -H 'Authorization: Bearer secret-token' https://api.example.com"
masked = mask_command(command)
print(masked)
# "curl -H 'Authorization: Bearer ***MASKED***' https://api.example.com"
```

### Check If Should Mask

Test if a string contains secrets:

```python
from provide.foundation.security import should_mask

if should_mask("password=secret123"):
    print("Contains secrets - mask before logging")
```

## Data Sanitization

### Sanitize HTTP Headers

Remove sensitive headers from HTTP requests/responses:

```python
from provide.foundation.security import sanitize_headers

headers = {
    "Authorization": "Bearer secret-token",
    "X-API-Key": "api-key-123",
    "Content-Type": "application/json",
    "User-Agent": "MyApp/1.0"
}

safe_headers = sanitize_headers(headers)
print(safe_headers)
# {
#     "Authorization": "***REDACTED***",
#     "X-API-Key": "***REDACTED***",
#     "Content-Type": "application/json",
#     "User-Agent": "MyApp/1.0"
# }
```

**Default sensitive headers:**
- Authorization
- X-API-Key, API-Key
- Cookie, Set-Cookie
- X-Auth-Token, X-Access-Token
- Proxy-Authorization

### Sanitize URIs

Remove sensitive query parameters:

```python
from provide.foundation.security import sanitize_uri

uri = "https://api.example.com/users?api_key=secret123&user_id=456"
safe_uri = sanitize_uri(uri)
print(safe_uri)
# "https://api.example.com/users?api_key=***REDACTED***&user_id=456"
```

**Default sensitive parameters:**
- api_key, apikey
- token, access_token, auth_token
- password, passwd
- secret, client_secret
- key

### Sanitize Dictionaries

Recursively sanitize dictionary data:

```python
from provide.foundation.security import sanitize_dict

data = {
    "user_id": "user_123",
    "api_key": "secret-key",
    "settings": {
        "password": "secret-password",
        "theme": "dark"
    }
}

safe_data = sanitize_dict(data)
print(safe_data)
# {
#     "user_id": "user_123",
#     "api_key": "***REDACTED***",
#     "settings": {
#         "password": "***REDACTED***",
#         "theme": "dark"
#     }
# }
```

## Integration with Logging

Combine with Foundation logger for secure logging:

```python
from provide.foundation import logger
from provide.foundation.security import sanitize_dict, mask_secrets

def log_api_request(url: str, headers: dict, body: dict):
    """Log API request with sanitized data."""
    from provide.foundation.security import sanitize_headers

    logger.info(
        "api_request_sent",
        url=url,
        headers=sanitize_headers(headers),
        body=sanitize_dict(body)
    )

# Usage
log_api_request(
    url="https://api.example.com/users",
    headers={"Authorization": "Bearer secret-token"},
    body={"password": "secret123", "email": "user@example.com"}
)
# Logs with sensitive data masked
```

## Configuration

### Automatic Sanitization

Foundation can automatically sanitize logs:

```python
from provide.foundation import get_hub, LoggingConfig, TelemetryConfig

config = TelemetryConfig(
    logging=LoggingConfig(
        sanitization_enabled=True,  # Enable auto-sanitization
        sanitization_sanitize_dicts=True,  # Sanitize dict values
        sanitization_mask_patterns=[  # Custom patterns
            r"CUSTOM_KEY=[^\s]+",
        ]
    )
)

hub = get_hub()
hub.initialize_foundation(config)
```

### Environment Variables

```bash
# Enable sanitization
export PROVIDE_LOG_SANITIZATION_ENABLED=true

# Sanitize dictionaries
export PROVIDE_LOG_SANITIZATION_SANITIZE_DICTS=true

# Add custom patterns (comma-separated)
export PROVIDE_LOG_SANITIZATION_MASK_PATTERNS="CUSTOM_KEY=[^\\s]+,SESSION=[^\\s]+"
```

## Best Practices

### âœ… DO: Sanitize Before Logging

```python
# âœ… Good: Sanitize sensitive data
from provide.foundation import logger
from provide.foundation.security import sanitize_dict

logger.info("user_data", data=sanitize_dict(user_dict))

# âŒ Bad: Log raw sensitive data
logger.info("user_data", data=user_dict)  # May contain passwords!
```

### âœ… DO: Mask Command Output

```python
# âœ… Good: Mask commands before logging
from provide.foundation import logger
from provide.foundation.security import mask_command

cmd = "curl -H 'X-API-Key: secret' https://api.example.com"
logger.debug("executing_command", command=mask_command(cmd))

# âŒ Bad: Log commands directly
logger.debug("executing_command", command=cmd)  # Exposes API key!
```

### âœ… DO: Use Custom Patterns for Domain-Specific Secrets

```python
# âœ… Good: Add domain-specific patterns
from provide.foundation.security import mask_secrets, DEFAULT_SECRET_PATTERNS

COMPANY_PATTERNS = DEFAULT_SECRET_PATTERNS + [
    r"INTERNAL_TOKEN=[^\s]+",
    r"VENDOR_KEY=[^\s]+"
]

text = "INTERNAL_TOKEN=abc123 VENDOR_KEY=xyz789"
safe = mask_secrets(text, patterns=COMPANY_PATTERNS)

# âŒ Bad: Rely only on default patterns
safe = mask_secrets(text)  # May miss company-specific secrets
```

### âœ… DO: Sanitize User Input

```python
# âœ… Good: Sanitize user-provided data
from provide.foundation.security import sanitize_dict

def process_user_input(data: dict):
    safe_data = sanitize_dict(data)
    store_in_database(safe_data)

# âŒ Bad: Store raw user input
def process_user_input_bad(data: dict):
    store_in_database(data)  # May contain passwords/tokens!
```

## Common Patterns

### Secure HTTP Client

```python
from provide.foundation import logger
from provide.foundation.security import sanitize_headers, sanitize_uri
import httpx

def secure_http_request(method: str, url: str, **kwargs):
    """Make HTTP request with secure logging."""
    headers = kwargs.get("headers", {})

    logger.info(
        "http_request_started",
        method=method,
        url=sanitize_uri(url),
        headers=sanitize_headers(headers)
    )

    response = httpx.request(method, url, **kwargs)

    logger.info(
        "http_request_completed",
        status_code=response.status_code,
        headers=sanitize_headers(dict(response.headers))
    )

    return response
```

### Secure Configuration Logging

```python
from provide.foundation import logger
from provide.foundation.security import sanitize_dict

def log_application_config(config: dict):
    """Log application configuration securely."""
    # Sanitize before logging
    safe_config = sanitize_dict(config)

    logger.info(
        "application_configured",
        config=safe_config
    )
```

## Next Steps

- **[Logging](../logging/basic-logging.md)**: Structured logging with security
- **[Configuration](../configuration/env-variables.md)**: Secure configuration management
- **[Process Execution](../process/subprocess.md)**: Secure command execution

---

**Tip**: Always sanitize data before logging to prevent accidental secret exposure.
>>> EOF >>>

### FILE 39: how-to-guides/state/state-management.md | checksum=8e6f96ede3bc... | modified=2025-10-24T20:12:16 | op=+ | size=11720 | tokens=2466 | type=markdown ###
<<< BOF <<<
# How to Use State Management

Foundation provides thread-safe, immutable state management utilities for building robust applications with predictable state transitions.

## Overview

The state module provides:

- **ImmutableState**: Thread-safe immutable state containers
- **StateMachine**: Finite state machine implementation
- **StateManager**: Centralized state management
- **ConfigManager**: Configuration versioning with change tracking
- **VersionedConfig**: Configuration with audit trails

## ImmutableState

Create thread-safe, immutable state containers:

```python
from provide.foundation.state import ImmutableState

# Create initial state
state = ImmutableState(
    user_id="user_123",
    status="active",
    count=0
)

# Read values
print(state.user_id)  # "user_123"
print(state.count)    # 0

# Create new state with updates (original unchanged)
new_state = state.update(count=1, status="processing")

print(state.count)      # 0 (unchanged)
print(new_state.count)  # 1 (new state)
```

**Key features:**
- Thread-safe access
- Immutable by design
- Type-safe attribute access
- Efficient copy-on-write semantics

### Use Cases

**1. Request Context:**
```python
from provide.foundation.state import ImmutableState

def handle_request(request_id: str):
    # Create immutable request context
    context = ImmutableState(
        request_id=request_id,
        user_id=None,
        authenticated=False
    )

    # Authenticate user (creates new state)
    context = context.update(
        user_id="user_123",
        authenticated=True
    )

    # Process with guaranteed immutability
    process_request(context)
```

**2. Transaction State:**
```python
from provide.foundation.state import ImmutableState

def process_transaction(transaction_id: str):
    state = ImmutableState(
        transaction_id=transaction_id,
        status="pending",
        amount=0,
        timestamp=None
    )

    try:
        state = state.update(status="processing")
        result = execute_transaction()
        state = state.update(
            status="completed",
            amount=result.amount,
            timestamp=result.timestamp
        )
        return state

    except Exception as e:
        state = state.update(status="failed", error=str(e))
        raise
```

## StateMachine

Implement finite state machines with explicit transitions:

```python
from provide.foundation.state import StateMachine

# Define states and allowed transitions
machine = StateMachine(
    initial_state="draft",
    transitions={
        "draft": ["review", "archived"],
        "review": ["approved", "rejected", "draft"],
        "approved": ["published", "archived"],
        "rejected": ["draft", "archived"],
        "published": ["archived"],
        "archived": []  # Terminal state
    }
)

# Check current state
print(machine.current_state)  # "draft"

# Transition to new state
machine.transition("review")
print(machine.current_state)  # "review"

# Invalid transitions raise errors
try:
    machine.transition("published")  # Not allowed from "review"
except ValueError as e:
    print(f"Invalid transition: {e}")

# Check if transition is valid
if machine.can_transition("approved"):
    machine.transition("approved")
```

### Document Workflow Example

```python
from provide.foundation.state import StateMachine
from provide.foundation import logger

class Document:
    def __init__(self, doc_id: str):
        self.doc_id = doc_id
        self.machine = StateMachine(
            initial_state="draft",
            transitions={
                "draft": ["submitted"],
                "submitted": ["reviewing"],
                "reviewing": ["approved", "rejected"],
                "approved": ["published"],
                "rejected": ["draft"],
                "published": []
            }
        )

    def submit(self):
        """Submit document for review."""
        if self.machine.can_transition("submitted"):
            self.machine.transition("submitted")
            logger.info("document_submitted", doc_id=self.doc_id)
        else:
            raise ValueError("Cannot submit from current state")

    def review(self):
        """Start review process."""
        self.machine.transition("reviewing")
        logger.info("document_review_started", doc_id=self.doc_id)

    def approve(self):
        """Approve document."""
        self.machine.transition("approved")
        logger.info("document_approved", doc_id=self.doc_id)

    def publish(self):
        """Publish approved document."""
        self.machine.transition("published")
        logger.info("document_published", doc_id=self.doc_id)

# Usage
doc = Document("doc_001")
doc.submit()
doc.review()
doc.approve()
doc.publish()
```

## StateManager

Centralized state management with observers:

```python
from provide.foundation.state import StateManager

# Create manager with initial state
manager = StateManager(initial_state={
    "connection_count": 0,
    "status": "idle"
})

# Register state change observers
def on_state_change(old_state, new_state):
    print(f"State changed: {old_state} -> {new_state}")

manager.add_observer(on_state_change)

# Update state
manager.update_state({"connection_count": 1, "status": "active"})

# Get current state
current = manager.get_state()
print(current["connection_count"])  # 1
```

### Application State Example

```python
from provide.foundation.state import StateManager
from provide.foundation import logger

class ApplicationState:
    def __init__(self):
        self.manager = StateManager(initial_state={
            "initialized": False,
            "connections": 0,
            "errors": 0,
            "status": "starting"
        })

        # Log state changes
        self.manager.add_observer(self._log_state_change)

    def _log_state_change(self, old_state, new_state):
        """Log all state changes."""
        logger.info("app_state_changed", old=old_state, new=new_state)

    def mark_initialized(self):
        """Mark application as initialized."""
        self.manager.update_state({"initialized": True, "status": "ready"})

    def increment_connections(self):
        """Increment connection count."""
        current = self.manager.get_state()
        self.manager.update_state({
            "connections": current["connections"] + 1
        })

    def record_error(self):
        """Record an error."""
        current = self.manager.get_state()
        self.manager.update_state({
            "errors": current["errors"] + 1
        })

    def get_health(self) -> dict:
        """Get application health status."""
        state = self.manager.get_state()
        return {
            "status": state["status"],
            "initialized": state["initialized"],
            "connections": state["connections"],
            "errors": state["errors"]
        }
```

## ConfigManager

Manage configuration with versioning and change tracking:

```python
from provide.foundation.state import ConfigManager

# Create manager with initial config
config = ConfigManager(initial_config={
    "log_level": "INFO",
    "timeout": 30,
    "enabled_features": ["auth", "api"]
})

# Update configuration
config.update_config({
    "log_level": "DEBUG",
    "timeout": 60
})

# Get current config
current = config.get_config()
print(current["log_level"])  # "DEBUG"

# Get config version
version = config.get_version()
print(f"Config version: {version}")

# Get change history
history = config.get_history()
for change in history:
    print(f"Version {change.version}: {change.changes}")
```

### Runtime Configuration Updates

```python
from provide.foundation.state import ConfigManager
from provide.foundation import logger, get_hub

class DynamicConfig:
    def __init__(self):
        self.manager = ConfigManager(initial_config={
            "log_level": "INFO",
            "max_connections": 100,
            "cache_ttl": 300,
            "features": {
                "auth_enabled": True,
                "metrics_enabled": False
            }
        })

    def update_log_level(self, level: str):
        """Update log level at runtime."""
        self.manager.update_config({"log_level": level})

        # Apply to Foundation logger
        hub = get_hub()
        logger.info("log_level_changed", new_level=level)

    def enable_feature(self, feature: str):
        """Enable a feature flag."""
        config = self.manager.get_config()
        features = config["features"].copy()
        features[f"{feature}_enabled"] = True

        self.manager.update_config({"features": features})
        logger.info("feature_enabled", feature=feature)

    def rollback_to_version(self, version: int):
        """Rollback to previous configuration version."""
        history = self.manager.get_history()
        if version < len(history):
            old_config = history[version].config
            self.manager.update_config(old_config)
            logger.info("config_rolled_back", version=version)

# Usage
config = DynamicConfig()
config.update_log_level("DEBUG")
config.enable_feature("metrics")
```

## VersionedConfig

Track configuration changes with full audit trails:

```python
from provide.foundation.state import VersionedConfig

# Create versioned config
config = VersionedConfig(
    version=1,
    config={
        "database_url": "postgresql://localhost/mydb",
        "cache_enabled": True
    },
    metadata={"author": "admin", "reason": "Initial configuration"}
)

# Create new version
config_v2 = VersionedConfig(
    version=2,
    config={
        "database_url": "postgresql://prod-db/mydb",
        "cache_enabled": True,
        "read_replicas": 3
    },
    metadata={"author": "admin", "reason": "Production migration"}
)

# Compare versions
print(f"Version {config.version} -> Version {config_v2.version}")
print(f"Changes: {config_v2.metadata['reason']}")
```

## Best Practices

### âœ… DO: Use Immutable State for Concurrency

```python
# âœ… Good: Thread-safe immutable state
from provide.foundation.state import ImmutableState

shared_state = ImmutableState(count=0)

def increment():
    global shared_state
    shared_state = shared_state.update(count=shared_state.count + 1)

# âŒ Bad: Mutable shared state
shared_dict = {"count": 0}

def increment_bad():
    shared_dict["count"] += 1  # Race condition!
```

### âœ… DO: Use StateMachine for Complex Workflows

```python
# âœ… Good: Explicit state machine
from provide.foundation.state import StateMachine

order = StateMachine(
    initial_state="pending",
    transitions={
        "pending": ["processing"],
        "processing": ["shipped", "cancelled"],
        "shipped": ["delivered"],
        "cancelled": [],
        "delivered": []
    }
)

# âŒ Bad: Manual state tracking
order_status = "pending"
if order_status == "pending":
    order_status = "shipped"  # Skipped "processing"!
```

### âœ… DO: Track Configuration Changes

```python
# âœ… Good: Version configuration changes
from provide.foundation.state import ConfigManager

config = ConfigManager(initial_config={"timeout": 30})
config.update_config({"timeout": 60})

history = config.get_history()
# Can see who changed what and when

# âŒ Bad: Direct updates without tracking
global_config = {"timeout": 30}
global_config["timeout"] = 60  # No audit trail
```

## Next Steps

- **[Configuration](../configuration/env-variables.md)**: Environment-based configuration
- **[Architecture](../../explanation/architecture.md)**: Understanding Foundation's design
- **[Logging](../logging/basic-logging.md)**: Combine with structured logging

---

**Tip**: Use ImmutableState for thread-safe data, StateMachine for workflows, and ConfigManager for dynamic configuration.
>>> EOF >>>

### FILE 40: how-to-guides/testing/cli-tests.md | checksum=b74eeef2f211... | modified=2025-10-24T18:52:16 | op=+ | size=18031 | tokens=4139 | type=markdown ###
<<< BOF <<<
# Testing CLI Commands

Learn how to test CLI applications built with Foundation using Click's testing utilities and provide-testkit.

## Overview

Testing CLI applications requires special tools to simulate command execution, capture output, and verify behavior. Foundation provides comprehensive testing support for CLI commands through integration with Click's test runner and provide-testkit.

**What you'll learn:**
- Basic CLI command testing
- Testing with arguments and options
- Capturing and verifying output
- Testing interactive prompts
- Error handling and exit codes
- Testing file I/O operations
- Mocking dependencies

## Prerequisites

Install testing dependencies:
```bash
pip install provide-testkit pytest
```

## Basic CLI Testing

### Simple Command Test

Test a basic CLI command:

```python
import pytest
from click.testing import CliRunner
from provide.testkit import reset_foundation_setup_for_testing

@pytest.fixture(autouse=True)
def reset_foundation():
    """Reset Foundation state before each test."""
    reset_foundation_setup_for_testing()

def test_hello_command():
    """Test basic hello command."""
    from myapp.cli import cli

    runner = CliRunner()
    result = runner.invoke(cli, ["hello"])

    assert result.exit_code == 0
    assert "Hello, World!" in result.output
```

### Test with Arguments

Test commands that accept arguments:

```python
def test_greet_with_name():
    """Test greeting with name argument."""
    runner = CliRunner()
    result = runner.invoke(cli, ["greet", "Alice"])

    assert result.exit_code == 0
    assert "Hello, Alice!" in result.output

def test_greet_multiple_names():
    """Test greeting multiple names."""
    runner = CliRunner()
    result = runner.invoke(cli, ["greet", "Alice", "Bob", "Charlie"])

    assert result.exit_code == 0
    assert "Alice" in result.output
    assert "Bob" in result.output
    assert "Charlie" in result.output
```

### Test with Options

Test commands with flags and options:

```python
def test_greet_with_options():
    """Test command with options."""
    runner = CliRunner()
    result = runner.invoke(cli, [
        "greet",
        "Alice",
        "--greeting", "Hi",
        "--uppercase"
    ])

    assert result.exit_code == 0
    assert "HI, ALICE!" in result.output

def test_short_flags():
    """Test short flag options."""
    runner = CliRunner()
    result = runner.invoke(cli, ["process", "-v", "-f", "input.txt"])

    assert result.exit_code == 0
    # Verify verbose output appears
    assert "Processing" in result.output
```

## Testing Output

### Capture Standard Output

Verify command output:

```python
def test_list_command_output():
    """Test list command produces correct output."""
    runner = CliRunner()
    result = runner.invoke(cli, ["list", "--format", "table"])

    # Check exit code
    assert result.exit_code == 0

    # Verify output contains expected content
    assert "ID" in result.output
    assert "Name" in result.output
    assert "Status" in result.output

    # Verify output format
    lines = result.output.split("\n")
    assert len(lines) >= 2  # Header + at least one row
```

### Capture Standard Error

Test error messages:

```python
from provide.testkit import set_log_stream_for_testing
from io import StringIO

def test_error_messages():
    """Test error output goes to stderr."""
    # Capture logs
    log_stream = StringIO()
    set_log_stream_for_testing(log_stream)

    runner = CliRunner(mix_stderr=False)
    result = runner.invoke(cli, ["invalid-command"])

    assert result.exit_code != 0
    assert "Error" in result.stderr

    # Check logs
    logs = log_stream.getvalue()
    assert "invalid-command" in logs
```

### Test JSON Output

Verify structured output:

```python
import json

def test_json_output():
    """Test command with JSON output."""
    runner = CliRunner()
    result = runner.invoke(cli, ["export", "--format", "json"])

    assert result.exit_code == 0

    # Parse and verify JSON
    data = json.loads(result.output)
    assert isinstance(data, list)
    assert len(data) > 0
    assert "id" in data[0]
    assert "name" in data[0]
```

## Testing File Operations

### Test with Temporary Files

Use Click's file isolation:

```python
def test_process_file():
    """Test file processing command."""
    runner = CliRunner()

    with runner.isolated_filesystem():
        # Create test input file
        with open("input.txt", "w") as f:
            f.write("test data\n")

        # Run command
        result = runner.invoke(cli, ["process", "input.txt"])

        assert result.exit_code == 0

        # Verify output file was created
        assert Path("output.txt").exists()

        # Verify output content
        output = Path("output.txt").read_text()
        assert "PROCESSED: test data" in output
```

### Test File Reading

Test commands that read files:

```python
def test_analyze_file():
    """Test file analysis command."""
    runner = CliRunner()

    with runner.isolated_filesystem():
        # Create test file with known content
        test_data = "line1\nline2\nline3\n"
        Path("data.txt").write_text(test_data)

        result = runner.invoke(cli, ["analyze", "data.txt"])

        assert result.exit_code == 0
        assert "3 lines" in result.output
        assert "17 bytes" in result.output
```

### Test File Writing

Verify file output:

```python
def test_export_to_file():
    """Test exporting data to file."""
    runner = CliRunner()

    with runner.isolated_filesystem():
        result = runner.invoke(cli, [
            "export",
            "--output", "export.csv",
            "--format", "csv"
        ])

        assert result.exit_code == 0

        # Verify file created
        export_file = Path("export.csv")
        assert export_file.exists()

        # Verify CSV content
        import csv
        with open(export_file) as f:
            reader = csv.DictReader(f)
            rows = list(reader)
            assert len(rows) > 0
            assert "id" in rows[0]
```

## Testing Interactive Prompts

### Test Input Prompts

Simulate user input:

```python
def test_interactive_input():
    """Test command with interactive prompts."""
    runner = CliRunner()

    # Simulate user typing "Alice" when prompted
    result = runner.invoke(cli, ["greet"], input="Alice\n")

    assert result.exit_code == 0
    assert "What is your name?" in result.output
    assert "Hello, Alice!" in result.output

def test_multiple_prompts():
    """Test multiple interactive prompts."""
    runner = CliRunner()

    # Simulate multiple inputs
    result = runner.invoke(cli, ["configure"], input="myapp\nproduction\ny\n")

    assert result.exit_code == 0
    assert "App name: myapp" in result.output
    assert "Environment: production" in result.output
```

### Test Confirmation Prompts

Test yes/no confirmations:

```python
def test_confirmation_yes():
    """Test accepting confirmation."""
    runner = CliRunner()

    result = runner.invoke(cli, ["delete", "item-123"], input="y\n")

    assert result.exit_code == 0
    assert "Deleted item-123" in result.output

def test_confirmation_no():
    """Test declining confirmation."""
    runner = CliRunner()

    result = runner.invoke(cli, ["delete", "item-123"], input="n\n")

    assert result.exit_code == 0
    assert "Cancelled" in result.output
    assert "Deleted" not in result.output
```

### Test Password Input

Test secure password prompts:

```python
def test_password_prompt():
    """Test password input (hidden)."""
    runner = CliRunner()

    result = runner.invoke(cli, ["login"], input="alice\nsecret123\n")

    assert result.exit_code == 0
    assert "Username:" in result.output
    assert "Password:" in result.output
    # Password should not appear in output
    assert "secret123" not in result.output
    assert "Logged in as alice" in result.output
```

## Testing Error Handling

### Test Invalid Arguments

Verify error handling for bad input:

```python
def test_invalid_argument_type():
    """Test invalid argument type."""
    runner = CliRunner()

    # Pass string where integer expected
    result = runner.invoke(cli, ["process", "--count", "invalid"])

    assert result.exit_code != 0
    assert "Invalid value for '--count'" in result.output

def test_missing_required_argument():
    """Test missing required argument."""
    runner = CliRunner()

    result = runner.invoke(cli, ["greet"])

    assert result.exit_code != 0
    assert "Missing argument" in result.output
```

### Test File Not Found

Test file error handling:

```python
def test_file_not_found():
    """Test handling of missing file."""
    runner = CliRunner()

    with runner.isolated_filesystem():
        result = runner.invoke(cli, ["process", "nonexistent.txt"])

        assert result.exit_code != 0
        assert "File not found" in result.output or "does not exist" in result.output
```

### Test Validation Errors

Test custom validation:

```python
def test_email_validation():
    """Test email format validation."""
    runner = CliRunner()

    # Invalid email
    result = runner.invoke(cli, ["register", "--email", "invalid-email"])

    assert result.exit_code != 0
    assert "Invalid email" in result.output

    # Valid email
    result = runner.invoke(cli, ["register", "--email", "user@example.com"])

    assert result.exit_code == 0
```

## Testing Exit Codes

### Verify Success

Test successful execution:

```python
def test_success_exit_code():
    """Test successful command returns 0."""
    runner = CliRunner()

    result = runner.invoke(cli, ["status"])

    assert result.exit_code == 0

def test_all_commands_success():
    """Test all commands can succeed."""
    runner = CliRunner()

    commands = ["status", "version", "help"]

    for cmd in commands:
        result = runner.invoke(cli, [cmd])
        assert result.exit_code == 0, f"Command '{cmd}' failed"
```

### Verify Failure Exit Codes

Test different error conditions:

```python
def test_error_exit_codes():
    """Test appropriate exit codes for errors."""
    runner = CliRunner()

    with runner.isolated_filesystem():
        # File not found
        result = runner.invoke(cli, ["process", "missing.txt"])
        assert result.exit_code == 1

        # Invalid input
        result = runner.invoke(cli, ["convert", "--format", "invalid"])
        assert result.exit_code == 2

        # Permission denied (simulated)
        Path("readonly.txt").touch()
        Path("readonly.txt").chmod(0o444)
        result = runner.invoke(cli, ["delete", "readonly.txt"])
        assert result.exit_code != 0
```

## Mocking and Fixtures

### Mock External Dependencies

Mock API calls and external services:

```python
from unittest.mock import patch, MagicMock

def test_api_command_with_mock():
    """Test command that calls external API."""
    runner = CliRunner()

    with patch('myapp.api.client.get_users') as mock_get_users:
        # Setup mock response
        mock_get_users.return_value = [
            {"id": 1, "name": "Alice"},
            {"id": 2, "name": "Bob"}
        ]

        result = runner.invoke(cli, ["list-users"])

        assert result.exit_code == 0
        assert "Alice" in result.output
        assert "Bob" in result.output

        # Verify API was called
        mock_get_users.assert_called_once()
```

### Use Pytest Fixtures

Share test setup:

```python
import pytest
from click.testing import CliRunner

@pytest.fixture
def cli_runner():
    """Provide CLI runner."""
    return CliRunner()

@pytest.fixture
def sample_data_file(cli_runner):
    """Create sample data file."""
    with cli_runner.isolated_filesystem():
        data = "id,name,status\n1,Alice,active\n2,Bob,inactive\n"
        Path("data.csv").write_text(data)
        yield "data.csv"

def test_with_fixture(cli_runner, sample_data_file):
    """Test using fixtures."""
    result = cli_runner.invoke(cli, ["import", sample_data_file])

    assert result.exit_code == 0
    assert "2 records imported" in result.output
```

### Parameterized Tests

Test multiple scenarios:

```python
@pytest.mark.parametrize("input_value,expected", [
    ("5", "Result: 25"),
    ("10", "Result: 100"),
    ("0", "Result: 0"),
])
def test_square_command(input_value, expected):
    """Test square command with various inputs."""
    runner = CliRunner()

    result = runner.invoke(cli, ["square", input_value])

    assert result.exit_code == 0
    assert expected in result.output

@pytest.mark.parametrize("format,extension", [
    ("json", ".json"),
    ("csv", ".csv"),
    ("xml", ".xml"),
])
def test_export_formats(format, extension):
    """Test different export formats."""
    runner = CliRunner()

    with runner.isolated_filesystem():
        result = runner.invoke(cli, ["export", "--format", format])

        assert result.exit_code == 0

        # Find created file with correct extension
        files = list(Path(".").glob(f"*{extension}"))
        assert len(files) == 1
```

## Testing Async Commands

### Test Async CLI Commands

Test commands using async operations:

```python
import pytest

@pytest.mark.asyncio
async def test_async_command():
    """Test async CLI command."""
    reset_foundation_setup_for_testing()

    from myapp.cli import cli

    runner = CliRunner()
    result = runner.invoke(cli, ["fetch", "https://api.example.com/data"])

    assert result.exit_code == 0
    assert "Data fetched" in result.output
```

## Testing Command Groups

### Test Subcommands

Test commands organized in groups:

```python
def test_user_subcommands():
    """Test user management subcommands."""
    runner = CliRunner()

    # Test create
    result = runner.invoke(cli, ["user", "create", "--name", "Alice"])
    assert result.exit_code == 0
    assert "User created" in result.output

    # Test list
    result = runner.invoke(cli, ["user", "list"])
    assert result.exit_code == 0
    assert "Alice" in result.output

    # Test delete
    result = runner.invoke(cli, ["user", "delete", "Alice"])
    assert result.exit_code == 0
    assert "User deleted" in result.output
```

### Test Help Output

Verify help text:

```python
def test_help_output():
    """Test help text is displayed."""
    runner = CliRunner()

    result = runner.invoke(cli, ["--help"])

    assert result.exit_code == 0
    assert "Usage:" in result.output
    assert "Options:" in result.output
    assert "Commands:" in result.output

def test_command_specific_help():
    """Test command-specific help."""
    runner = CliRunner()

    result = runner.invoke(cli, ["process", "--help"])

    assert result.exit_code == 0
    assert "Usage: cli process" in result.output
    assert "Process files" in result.output  # Command description
```

## Best Practices

### âœ… DO: Reset Foundation State

```python
# âœ… Good: Clean state for each test
@pytest.fixture(autouse=True)
def reset_foundation():
    reset_foundation_setup_for_testing()

def test_command():
    runner = CliRunner()
    result = runner.invoke(cli, ["command"])
    assert result.exit_code == 0
```

### âœ… DO: Use Isolated Filesystem

```python
# âœ… Good: Isolate file operations
def test_file_command():
    runner = CliRunner()
    with runner.isolated_filesystem():
        Path("test.txt").write_text("data")
        result = runner.invoke(cli, ["process", "test.txt"])
        assert result.exit_code == 0
```

### âœ… DO: Test Both Success and Failure

```python
# âœ… Good: Test happy and error paths
def test_valid_input():
    result = runner.invoke(cli, ["greet", "Alice"])
    assert result.exit_code == 0

def test_invalid_input():
    result = runner.invoke(cli, ["greet"])  # Missing name
    assert result.exit_code != 0
```

### âœ… DO: Verify Output Content

```python
# âœ… Good: Check actual output
def test_output_content():
    result = runner.invoke(cli, ["list"])
    assert result.exit_code == 0
    assert "Total: 5 items" in result.output

# âŒ Bad: Only check exit code
def test_only_exit_code():
    result = runner.invoke(cli, ["list"])
    assert result.exit_code == 0  # Could still have wrong output!
```

### âŒ DON'T: Forget to Test Edge Cases

```python
# âœ… Good: Test edge cases
def test_empty_list():
    result = runner.invoke(cli, ["list"])
    assert "No items found" in result.output

def test_special_characters():
    result = runner.invoke(cli, ["greet", "Alice & Bob"])
    assert result.exit_code == 0
```

## Integration Testing

### Test Full Workflows

Test complete user workflows:

```python
def test_full_workflow():
    """Test complete user workflow."""
    runner = CliRunner()

    with runner.isolated_filesystem():
        # Step 1: Initialize
        result = runner.invoke(cli, ["init"])
        assert result.exit_code == 0
        assert Path("config.yml").exists()

        # Step 2: Add data
        result = runner.invoke(cli, ["add", "--name", "Item1"])
        assert result.exit_code == 0

        # Step 3: List data
        result = runner.invoke(cli, ["list"])
        assert result.exit_code == 0
        assert "Item1" in result.output

        # Step 4: Export
        result = runner.invoke(cli, ["export", "--format", "json"])
        assert result.exit_code == 0
        assert Path("export.json").exists()
```

## Next Steps

### Related Guides
- **[Unit Testing](unit-tests.md)**: General unit testing with provide-testkit
- **[Building Commands](../cli/commands.md)**: Create CLI commands
- **[CLI Arguments](../cli/arguments.md)**: Advanced argument handling

### Examples
- See `examples/cli/` for CLI application examples
- See `tests/cli/` in the repository for more test patterns

### API Reference
- **[API Reference: CLI](../../reference/provide/foundation/cli/index.md)**: Complete CLI API documentation

---

**Tip**: Always use `CliRunner` in isolated filesystem mode for file operations to avoid test pollution. Use `reset_foundation_setup_for_testing()` to ensure clean state between tests.
>>> EOF >>>

### FILE 41: how-to-guides/testing/unit-tests.md | checksum=bc698a3e7575... | modified=2025-10-24T19:28:42 | op=+ | size=16415 | tokens=3688 | type=markdown ###
<<< BOF <<<
# Unit Testing

Learn how to write comprehensive unit tests for Foundation applications using provide-testkit.

## Overview

Foundation provides `provide-testkit`, a comprehensive testing toolkit that ensures clean state between tests, proper resource cleanup, and easy log capture. It integrates seamlessly with pytest for a powerful testing experience.

**Key features:**
- **State reset** - Clean Foundation state for each test
- **Log capture** - Capture and verify log output
- **FoundationTestCase** - Base test class with setup/teardown
- **Async support** - Test async code easily
- **Fixture support** - Pytest fixtures for common patterns

## Prerequisites

Install testing dependencies:
```bash
pip install provide-testkit pytest pytest-asyncio
```

## Basic Test Setup

### Minimal Test

```python
import pytest
from provide.testkit import reset_foundation_setup_for_testing

@pytest.fixture(autouse=True)
def reset_foundation():
    """Reset Foundation state before each test."""
    reset_foundation_setup_for_testing()

def test_simple_operation():
    """Test basic operation."""
    result = 2 + 2
    assert result == 4
```

### Testing with Logging

```python
from provide.foundation import logger

def test_logging():
    """Test logging works correctly."""
    reset_foundation_setup_for_testing()

    logger.info("test_event", value=123)
    # Logger is initialized and working
```

## Using FoundationTestCase

The base test class provides setup/teardown and utilities:

```python
from provide.testkit import FoundationTestCase

class TestMyFeature(FoundationTestCase):
    """Test suite for my feature."""

    def setup_method(self):
        """Set up test environment."""
        super().setup_method()  # IMPORTANT: Call parent
        self.test_data = {"key": "value"}
        self.counter = 0

    def test_basic_functionality(self):
        """Test basic feature."""
        assert self.test_data["key"] == "value"
        self.counter += 1
        assert self.counter == 1

    def test_another_feature(self):
        """Test another aspect."""
        # counter is 0 here - fresh setup for each test
        assert self.counter == 0

    def teardown_method(self):
        """Clean up after test."""
        # Cleanup code here
        super().teardown_method()  # IMPORTANT: Call parent
```

## Capturing Logs

### Basic Log Capture

```python
from provide.testkit import set_log_stream_for_testing
from io import StringIO

def test_log_output():
    """Test log output content."""
    reset_foundation_setup_for_testing()

    stream = StringIO()
    set_log_stream_for_testing(stream)

    from provide.foundation import logger
    logger.info("test_message", value=42)

    output = stream.getvalue()
    assert "test_message" in output
    assert "42" in output
```

### Structured Log Verification

```python
import json

def test_structured_logging():
    """Test structured log fields."""
    reset_foundation_setup_for_testing()

    stream = StringIO()
    set_log_stream_for_testing(stream)

    from provide.foundation import logger
    logger.info("user_login", user_id="user_123", success=True)

    # Parse JSON log output
    output = stream.getvalue()
    for line in output.strip().split("\n"):
        if "user_login" in line:
            log_entry = json.loads(line)
            assert log_entry["event"] == "user_login"
            assert log_entry["user_id"] == "user_123"
            assert log_entry["success"] is True
```

### Log Level Testing

```python
def test_log_levels():
    """Test different log levels."""
    reset_foundation_setup_for_testing()

    stream = StringIO()
    set_log_stream_for_testing(stream)

    from provide.foundation import logger
    logger.debug("debug_message")
    logger.info("info_message")
    logger.warning("warning_message")
    logger.error("error_message")

    output = stream.getvalue()
    assert "debug_message" in output
    assert "info_message" in output
    assert "warning_message" in output
    assert "error_message" in output
```

## Mocking and Patching

### Mock External Dependencies

```python
from unittest.mock import Mock, patch

def test_with_mock_database():
    """Test with mocked database."""
    reset_foundation_setup_for_testing()

    # Create mock
    mock_db = Mock()
    mock_db.query.return_value = [{"id": 1, "name": "Alice"}]

    # Inject mock
    service = UserService(database=mock_db)

    # Test
    users = service.get_all_users()
    assert len(users) == 1
    assert users[0]["name"] == "Alice"

    # Verify mock was called
    mock_db.query.assert_called_once_with("SELECT * FROM users")
```

### Patch Functions

```python
@patch('myapp.api.client.get_users')
def test_api_call(mock_get_users):
    """Test with patched API call."""
    reset_foundation_setup_for_testing()

    # Setup mock response
    mock_get_users.return_value = [
        {"id": 1, "name": "Alice"},
        {"id": 2, "name": "Bob"}
    ]

    # Test
    from myapp.service import UserService
    service = UserService()
    users = service.fetch_users()

    assert len(users) == 2
    assert users[0]["name"] == "Alice"
```

### Context Manager Mocking

```python
from unittest.mock import MagicMock

def test_file_operations():
    """Test file operations with mock."""
    reset_foundation_setup_for_testing()

    mock_file = MagicMock()
    mock_file.__enter__.return_value.read.return_value = "test content"

    with patch('builtins.open', return_value=mock_file):
        content = read_config_file("config.txt")
        assert content == "test content"
```

## Testing Async Code

### Basic Async Test

```python
import pytest

@pytest.mark.asyncio
async def test_async_function():
    """Test async function."""
    reset_foundation_setup_for_testing()

    result = await async_operation()
    assert result == "success"
```

### Async with Mocks

```python
from unittest.mock import AsyncMock

@pytest.mark.asyncio
async def test_async_api_call():
    """Test async API call."""
    reset_foundation_setup_for_testing()

    mock_client = AsyncMock()
    mock_client.get.return_value = {"status": "ok"}

    service = APIService(client=mock_client)
    result = await service.fetch_data()

    assert result["status"] == "ok"
    mock_client.get.assert_awaited_once()
```

## Parameterized Tests

### Basic Parametrization

```python
@pytest.mark.parametrize("input,expected", [
    (2, 4),
    (3, 9),
    (4, 16),
    (5, 25),
])
def test_square(input, expected):
    """Test square function with multiple inputs."""
    result = square(input)
    assert result == expected
```

### Multiple Parameters

```python
@pytest.mark.parametrize("username,password,should_succeed", [
    ("alice", "correct_password", True),
    ("alice", "wrong_password", False),
    ("bob", "correct_password", True),
    ("invalid_user", "any_password", False),
])
def test_authentication(username, password, should_succeed):
    """Test authentication with various credentials."""
    reset_foundation_setup_for_testing()

    result = authenticate(username, password)
    assert result == should_succeed
```

### Parameterized Fixtures

```python
@pytest.fixture(params=["sqlite", "postgres", "mysql"])
def database(request):
    """Provide different database backends."""
    db = Database(backend=request.param)
    db.connect()
    yield db
    db.disconnect()

def test_database_operations(database):
    """Test operations work on all database backends."""
    database.execute("CREATE TABLE test (id INT)")
    database.execute("INSERT INTO test VALUES (1)")
    result = database.query("SELECT * FROM test")
    assert len(result) == 1
```

## Pytest Fixtures

### Shared Fixtures

```python
@pytest.fixture
def sample_user():
    """Provide sample user for tests."""
    return {
        "id": "user_123",
        "name": "Alice",
        "email": "alice@example.com"
    }

@pytest.fixture
def user_repository():
    """Provide user repository."""
    reset_foundation_setup_for_testing()
    from myapp.repositories import UserRepository
    return UserRepository(database=":memory:")

def test_create_user(user_repository, sample_user):
    """Test user creation."""
    user_repository.create(sample_user)
    retrieved = user_repository.get(sample_user["id"])
    assert retrieved["name"] == "Alice"
```

### Fixture Scope

```python
@pytest.fixture(scope="module")
def database_connection():
    """Single database connection for all tests in module."""
    db = Database(":memory:")
    db.connect()
    db.initialize_schema()
    yield db
    db.disconnect()

@pytest.fixture(scope="function")
def clean_database(database_connection):
    """Clean database before each test."""
    database_connection.execute("DELETE FROM users")
    return database_connection
```

### Fixture Factories

```python
@pytest.fixture
def user_factory():
    """Factory for creating test users."""
    def _create_user(name="TestUser", email=None):
        return {
            "id": f"user_{name.lower()}",
            "name": name,
            "email": email or f"{name.lower()}@test.com"
        }
    return _create_user

def test_multiple_users(user_factory):
    """Test with multiple users."""
    alice = user_factory(name="Alice")
    bob = user_factory(name="Bob")

    assert alice["name"] == "Alice"
    assert bob["email"] == "bob@test.com"
```

## Testing Exceptions

### Basic Exception Testing

```python
def test_raises_exception():
    """Test function raises expected exception."""
    with pytest.raises(ValueError):
        validate_email("invalid-email")

def test_exception_message():
    """Test exception message."""
    with pytest.raises(ValueError, match="Invalid email format"):
        validate_email("invalid-email")
```

### Exception Context

```python
def test_exception_details():
    """Test exception with detailed verification."""
    with pytest.raises(DatabaseError) as exc_info:
        connect_to_database("invalid://url")

    assert "connection failed" in str(exc_info.value)
    assert exc_info.value.error_code == "DB001"
```

## Test Organization

### Class-Based Organization

```python
class TestUserService(FoundationTestCase):
    """Test suite for UserService."""

    def setup_method(self):
        """Set up test dependencies."""
        super().setup_method()
        self.mock_repo = Mock()
        self.service = UserService(repository=self.mock_repo)

    def test_get_user(self):
        """Test getting user by ID."""
        self.mock_repo.get.return_value = {"id": "123", "name": "Alice"}

        user = self.service.get_user("123")
        assert user["name"] == "Alice"

    def test_create_user(self):
        """Test user creation."""
        user_data = {"name": "Bob", "email": "bob@example.com"}

        self.service.create_user(user_data)
        self.mock_repo.save.assert_called_once()

    def teardown_method(self):
        """Clean up."""
        super().teardown_method()
```

### Module-Level Organization

```python
# test_authentication.py

def test_login_success():
    """Test successful login."""
    pass

def test_login_failure():
    """Test failed login."""
    pass

def test_logout():
    """Test logout."""
    pass

# test_authorization.py

def test_has_permission():
    """Test permission check."""
    pass

def test_lacks_permission():
    """Test denied permission."""
    pass
```

## Testing Best Practices

### âœ… DO: Reset State Between Tests

```python
# âœ… Good: Clean state for each test
@pytest.fixture(autouse=True)
def reset_foundation():
    reset_foundation_setup_for_testing()

def test_operation_a():
    # Clean state
    pass

def test_operation_b():
    # Clean state, not affected by test_operation_a
    pass
```

### âœ… DO: Use Descriptive Test Names

```python
# âœ… Good: Clear test names
def test_user_login_with_valid_credentials_succeeds():
    pass

def test_user_login_with_invalid_password_fails():
    pass

# âŒ Bad: Unclear names
def test_login():
    pass

def test_login2():
    pass
```

### âœ… DO: Test One Thing Per Test

```python
# âœ… Good: Focused tests
def test_user_creation():
    user = create_user("Alice")
    assert user.name == "Alice"

def test_user_email_validation():
    with pytest.raises(ValueError):
        create_user("Alice", email="invalid")

# âŒ Bad: Testing multiple things
def test_user_everything():
    user = create_user("Alice")
    assert user.name == "Alice"
    with pytest.raises(ValueError):
        create_user("Bob", email="invalid")
    # Too much in one test
```

### âœ… DO: Use Fixtures for Setup

```python
# âœ… Good: Reusable fixtures
@pytest.fixture
def authenticated_user():
    return authenticate("alice", "password")

def test_api_call(authenticated_user):
    response = api.call(authenticated_user)
    assert response.status == 200

# âŒ Bad: Setup in each test
def test_api_call():
    user = authenticate("alice", "password")  # Repeated
    response = api.call(user)
    assert response.status == 200
```

### âŒ DON'T: Test Implementation Details

```python
# âŒ Bad: Testing internal implementation
def test_internal_cache():
    service = UserService()
    service.get_user("123")
    assert service._cache["123"] is not None  # Internal detail

# âœ… Good: Test behavior
def test_user_retrieval():
    service = UserService()
    user = service.get_user("123")
    assert user["id"] == "123"
```

### âŒ DON'T: Share State Between Tests

```python
# âŒ Bad: Shared mutable state
SHARED_DATA = []

def test_append():
    SHARED_DATA.append(1)
    assert len(SHARED_DATA) == 1  # Fails if test runs twice

# âœ… Good: Isolated state
def test_append():
    data = []
    data.append(1)
    assert len(data) == 1
```

## Coverage

### Running with Coverage

```bash
# Run tests with coverage
pytest --cov=myapp --cov-report=html

# Run with coverage threshold
pytest --cov=myapp --cov-fail-under=80
```

### Coverage Configuration

```ini
# pyproject.toml
[tool.coverage.run]
source = ["src"]
omit = ["*/tests/*", "*/migrations/*"]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise AssertionError",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
]
```

### Testing Untested Code

```python
def test_previously_untested_function():
    """Add test for uncovered function."""
    result = previously_untested_function(input_data)
    assert result == expected_output
```

## Common Patterns

### Testing Database Operations

```python
@pytest.fixture
def test_database():
    """Provide test database."""
    db = Database(":memory:")
    db.execute("CREATE TABLE users (id INT, name TEXT)")
    yield db
    db.close()

def test_save_user(test_database):
    """Test saving user to database."""
    test_database.execute("INSERT INTO users VALUES (1, 'Alice')")
    result = test_database.query("SELECT * FROM users WHERE id = 1")
    assert result[0]["name"] == "Alice"
```

### Testing HTTP Clients

```python
from unittest.mock import Mock

def test_http_get():
    """Test HTTP GET request."""
    mock_client = Mock()
    mock_client.get.return_value = Mock(
        status_code=200,
        json=lambda: {"data": "test"}
    )

    service = APIService(client=mock_client)
    response = service.fetch_data()

    assert response["data"] == "test"
    mock_client.get.assert_called_once_with("/api/data")
```

### Testing Time-Dependent Code

```python
from unittest.mock import patch
from datetime import datetime

@patch('myapp.utils.datetime')
def test_time_dependent(mock_datetime):
    """Test code that depends on current time."""
    # Fix time to specific value
    mock_datetime.now.return_value = datetime(2025, 10, 24, 10, 0, 0)

    result = get_greeting()
    assert result == "Good morning"  # Predictable result
```

## Next Steps

### Related Guides
- **[Testing CLI Commands](cli-tests.md)**: Test CLI applications
- **[Basic Logging](../logging/basic-logging.md)**: Understand logging for tests
- **[Dependency Injection](../../explanation/dependency-injection.md)**: DI makes testing easier

### Examples
- See `tests/` directory in the repository for comprehensive test examples
- See `examples/testing/` for testing patterns

---

**Tip**: Always reset Foundation state with `reset_foundation_setup_for_testing()` before each test. Use `FoundationTestCase` as your base class for automatic setup/teardown.
>>> EOF >>>

### FILE 42: how-to-guides/tracing/distributed-tracing.md | checksum=a1c72923289b... | modified=2025-10-24T20:13:26 | op=+ | size=2898 | tokens=667 | type=markdown ###
<<< BOF <<<
# How to Use Distributed Tracing

Foundation provides distributed tracing with optional OpenTelemetry integration.

## Quick Start

```python
from provide.foundation.tracer import with_span, get_current_trace_id
from provide.foundation import logger

@with_span("process_order")
def process_order(order_id: str):
    """Process an order with automatic tracing."""
    trace_id = get_current_trace_id()

    logger.info("processing_order", order_id=order_id, trace_id=trace_id)

    # Your logic here
    validate_order(order_id)
    charge_customer(order_id)
    ship_order(order_id)

    logger.info("order_processed", order_id=order_id, trace_id=trace_id)
```

## Manual Span Management

```python
from provide.foundation.tracer import Span, set_current_span, get_current_span

# Create span
span = Span(name="database_query", attributes={"table": "users"})
set_current_span(span)

try:
    # Your code
    result = execute_query()
    span.set_attribute("rows_returned", len(result))
except Exception as e:
    span.set_status(error=True, description=str(e))
    raise
finally:
    span.end()
```

## Trace Context

Access current trace context:

```python
from provide.foundation.tracer import get_trace_context, get_current_trace_id

# Get full trace context
context = get_trace_context()
print(f"Trace ID: {context.trace_id}")
print(f"Span ID: {context.span_id}")

# Or just get trace ID
trace_id = get_current_trace_id()
```

## OpenTelemetry Integration

Enable OpenTelemetry for full distributed tracing:

```python
from provide.foundation import get_hub, TelemetryConfig

config = TelemetryConfig(
    service_name="my-service",
    tracing_enabled=True,
    otlp_endpoint="http://localhost:4317"
)

hub = get_hub()
hub.initialize_foundation(config)

# Traces automatically exported to OTLP endpoint
```

## Environment Configuration

```bash
# Enable tracing
export PROVIDE_TRACING_ENABLED=true

# Set OTLP endpoint
export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

# Set service name
export OTEL_SERVICE_NAME=my-service

# Set trace sample rate
export PROVIDE_TRACE_SAMPLE_RATE=1.0  # 100% sampling
```

## Best Practices

### âœ… DO: Add Meaningful Attributes

```python
# âœ… Good: Rich span attributes
@with_span("process_payment", attributes={
    "payment_method": "card",
    "currency": "USD"
})
def process_payment(amount: float):
    pass
```

### âœ… DO: Propagate Trace Context

```python
# âœ… Good: Include trace_id in logs
from provide.foundation.tracer import get_current_trace_id
from provide.foundation import logger

trace_id = get_current_trace_id()
logger.info("operation_completed", trace_id=trace_id)
```

## Next Steps

- **[Logging](../logging/basic-logging.md)**: Combine tracing with logging
- **[Metrics](../observability/metrics.md)**: Complete observability stack

---

**Tip**: See `examples/tracing/` for complete examples of distributed tracing.
>>> EOF >>>

### FILE 43: how-to-guides/troubleshooting.md | checksum=59ddb0d8ad75... | modified=2025-10-24T18:23:02 | op=+ | size=16038 | tokens=4002 | type=markdown ###
<<< BOF <<<
# Troubleshooting Guide

Common issues and solutions when using provide.foundation.

## Table of Contents

- [Import Errors](#import-errors)
- [Logging Issues](#logging-issues)
- [Configuration Problems](#configuration-problems)
- [Performance Issues](#performance-issues)
- [CLI Problems](#cli-problems)
- [Environment Variable Issues](#environment-variable-issues)
- [Testing Issues](#testing-issues)
- [Integration Problems](#integration-problems)

## Import Errors

### ModuleNotFoundError: No module named 'provide.foundation'

**Symptoms:**
```python
ImportError: No module named 'provide.foundation'
```

**Solutions:**

1. **Install the package:**
   ```bash
   pip install provide-foundation
   ```

2. **Check virtual environment:**
   ```bash
   which python  # Should point to your venv
   pip list | grep provide-foundation
   ```

3. **Verify Python version:**
   ```bash
   python --version  # Must be 3.11 or higher
   ```

---

### ImportError: cannot import name 'X' from 'provide.foundation'

**Symptoms:**
```python
ImportError: cannot import name 'pout' from 'provide.foundation'
```

**Solutions:**

1. **Check if you need an optional dependency:**
   ```bash
   # CLI features require [cli] extra
   pip install "provide-foundation[cli]"

   # Crypto features require [crypto] extra
   pip install "provide-foundation[crypto]"

   # Or install everything
   pip install "provide-foundation[all]"
   ```

2. **Verify import path:**
   ```python
   # âœ… Correct
   from provide.foundation import logger, pout, perr

   # âŒ Wrong
   from provide import foundation  # Don't do this
   ```

---

### ModuleNotFoundError: No module named 'click'

**Symptoms:**
```python
ModuleNotFoundError: No module named 'click'
```

**Solution:**

Install CLI extras:
```bash
pip install "provide-foundation[cli]"
```

---

## Logging Issues

### Logs Not Appearing

**Symptoms:**
- No log output to console
- Expected logs missing

**Solutions:**

1. **Check log level:**
   ```bash
   # Set log level to see more logs
   export PROVIDE_LOG_LEVEL=DEBUG
   python your_script.py
   ```

2. **Verify logger is being used:**
   ```python
   from provide.foundation import logger

   # âœ… Correct
   logger.info("This will log")

   # âŒ Wrong - using print instead
   print("This won't go to logs")
   ```

3. **Check if logs are going to the right stream:**
   ```python
   # Logs go to stderr by default
   python your_script.py 2>&1 | less
   ```

---

### Logs Missing Context Fields

**Symptoms:**
```
Expected: {"event": "user_login", "user_id": "123"}
Got: {"event": "user_login"}
```

**Solutions:**

1. **Use keyword arguments:**
   ```python
   # âœ… Correct
   logger.info("user_login", user_id="123")

   # âŒ Wrong - missing context
   logger.info("user_login 123")
   ```

2. **Check field naming:**
   ```python
   # Field names must be valid Python identifiers
   logger.info("event", valid_field="value")
   logger.info("event", **{"valid-field": "value"})  # Use ** for non-identifier names
   ```

---

### JSON Logs Not Formatted

**Symptoms:**
- Logs appear as plain text instead of JSON
- Log aggregation tools can't parse logs

**Solution:**

Enable JSON format:
```bash
export PROVIDE_LOG_FORMAT=json
python your_script.py
```

Output:
```json
{"event": "user_login", "level": "info", "timestamp": "2025-10-24T10:00:00Z"}
```

---

###  Emojis Not Showing in Logs

**Symptoms:**
- Emojis appear as `?` or boxes
- No emoji prefixes on log messages

**Solutions:**

1. **Check terminal encoding:**
   ```bash
   # Set UTF-8 encoding
   export LANG=en_US.UTF-8
   export LC_ALL=en_US.UTF-8
   ```

2. **Use console format (not JSON):**
   ```bash
   export PROVIDE_LOG_FORMAT=console
   ```

3. **Disable emojis if needed:**
   ```bash
   export PROVIDE_DISABLE_EMOJIS=true
   ```

---

### Third-Party Library Logs Too Verbose

**Symptoms:**
```
DEBUG:urllib3:Starting new HTTPS connection
DEBUG:asyncio:Using selector: KqueueSelector
```

**Solution:**

Control module log levels:
```bash
# Suppress urllib3 and asyncio debug logs
export PROVIDE_LOG_MODULE_LEVELS="urllib3:WARNING,asyncio:WARNING"
```

Or programmatically:
```python
from provide.foundation.logger import set_module_log_level

set_module_log_level("urllib3", "WARNING")
set_module_log_level("asyncio", "WARNING")
```

---

## Configuration Problems

### Environment Variables Not Loading

**Symptoms:**
- Environment variables set but not being read
- Config shows default values instead of env values

**Solutions:**

1. **Verify variable names:**
   ```bash
   # Foundation looks for PROVIDE_* prefix
   export PROVIDE_LOG_LEVEL=DEBUG  # âœ… Correct
   export LOG_LEVEL=DEBUG  # âŒ Wrong prefix
   ```

2. **Check variable is exported:**
   ```bash
   # âœ… Export variable
   export PROVIDE_LOG_LEVEL=DEBUG

   # âŒ Just setting doesn't export
   PROVIDE_LOG_LEVEL=DEBUG
   ```

3. **Verify in environment:**
   ```bash
   env | grep PROVIDE
   ```

---

### BaseConfig.from_env() Not Working

**Symptoms:**
```python
config = MyConfig.from_env()  # Values are None or defaults
```

**Solutions:**

1. **Use env_field() decorator:**
   ```python
   from provide.foundation.config import BaseConfig, env_field
   from attrs import define

   @define
   class MyConfig(BaseConfig):
       # âœ… Correct
       api_key: str = env_field(env_var="API_KEY")

       # âŒ Wrong - missing env_field
       db_host: str = "localhost"  # Won't load from env
   ```

2. **Check environment variable is set:**
   ```bash
   export API_KEY="my-secret-key"
   ```

---

### File-Based Secrets Not Loading

**Symptoms:**
```python
config.password = "file:///run/secrets/password"  # Should be file contents
```

**Solutions:**

1. **Verify file exists:**
   ```bash
   ls -la /run/secrets/password
   cat /run/secrets/password
   ```

2. **Check file permissions:**
   ```bash
   chmod 400 /run/secrets/password  # Read-only for owner
   ```

3. **Use correct file:// syntax:**
   ```bash
   # âœ… Correct
   export DB_PASSWORD="file:///run/secrets/db_password"

   # âŒ Wrong - missing file:// prefix
   export DB_PASSWORD="/run/secrets/db_password"
   ```

---

## Performance Issues

### Slow Logging Performance

**Symptoms:**
- Application slows down significantly with logging
- High CPU usage

**Solutions:**

1. **Reduce log level:**
   ```bash
   # Only log INFO and above in production
   export PROVIDE_LOG_LEVEL=INFO
   ```

2. **Avoid logging in tight loops:**
   ```python
   # âŒ Bad: Logs in loop
   for item in million_items:
       logger.debug("Processing", item=item)  # Too much!

   # âœ… Good: Log summary
   logger.info("Processing batch", count=len(million_items))
   for item in million_items:
       process(item)
   logger.info("Batch completed")
   ```

3. **Use sampling for high-frequency events:**
   ```python
   import random

   for i, item in enumerate(items):
       # Only log 1% of items
       if random.random() < 0.01:
           logger.debug("Processing sample", item=item, position=i)
   ```

---

### High Memory Usage

**Symptoms:**
- Memory grows over time
- Out of memory errors

**Solutions:**

1. **Check for log buffering:**
   ```python
   # Flush logs regularly
   import sys
   sys.stderr.flush()
   ```

2. **Avoid storing large objects in log context:**
   ```python
   # âŒ Bad: Logs entire large object
   logger.info("Processing", data=large_dataframe)

   # âœ… Good: Log summary
   logger.info("Processing", row_count=len(large_dataframe))
   ```

3. **Clear log handlers if dynamically creating loggers:**
   ```python
   import logging

   # Clear handlers to prevent accumulation
   logger_obj = logging.getLogger("my_logger")
   logger_obj.handlers.clear()
   ```

---

## CLI Problems

### Commands Not Discovered

**Symptoms:**
```bash
$ mycli mycommand
Error: No such command 'mycommand'
```

**Solutions:**

1. **Verify @register_command decorator:**
   ```python
   from provide.foundation.hub import register_command

   # âœ… Correct
   @register_command("mycommand")
   def my_command():
       pass

   # âŒ Wrong - missing decorator
   def my_command():
       pass
   ```

2. **Ensure module is imported:**
   ```python
   # In your main file
   import my_commands  # Must import to register

   hub = get_hub()
   cli = hub.create_cli()
   cli()
   ```

3. **Check command name:**
   ```python
   # Command name must be lowercase, use hyphens
   @register_command("process-data")  # âœ… Good
   @register_command("processData")   # âŒ Not idiomatic
   ```

---

### CLI Help Text Not Showing

**Symptoms:**
- `--help` shows empty or generic help
- Command descriptions missing

**Solution:**

Add docstrings:
```python
@register_command("process")
def process_data(input_file: str):
    """Process data from input file.

    Args:
        input_file: Path to the input data file
    """
    pass
```

---

## Environment Variable Issues

### Boolean Variables Not Parsing

**Symptoms:**
```python
debug = get_bool("DEBUG")  # Returns None even when DEBUG=false
```

**Solutions:**

1. **Use accepted boolean values:**
   ```bash
   # âœ… Truthy values
   export DEBUG=true
   export DEBUG=1
   export DEBUG=yes
   export DEBUG=on

   # âœ… Falsy values
   export DEBUG=false
   export DEBUG=0
   export DEBUG=no
   export DEBUG=off
   export DEBUG=""  # Empty string is false

   # âŒ Wrong
   export DEBUG=False  # Case-sensitive, should be lowercase
   ```

2. **Check for required vs optional:**
   ```python
   # Returns None if not set
   debug = get_bool("DEBUG")

   # Raises error if not set
   debug = get_bool("DEBUG", required=True)

   # Uses default if not set
   debug = get_bool("DEBUG", default=False)
   ```

---

### List Variables Not Parsing

**Symptoms:**
```python
hosts = get_list("HOSTS")  # Expected list, got string
```

**Solutions:**

1. **Use comma separation:**
   ```bash
   # âœ… Correct
   export HOSTS="host1,host2,host3"

   # âŒ Wrong - spaces without commas
   export HOSTS="host1 host2 host3"
   ```

2. **Custom separator:**
   ```python
   # For colon-separated values
   paths = get_list("PATH", separator=":")
   ```

---

## Testing Issues

### Tests Failing Due to Shared State

**Symptoms:**
- Tests pass individually but fail when run together
- Random test failures

**Solutions:**

1. **Use provide-testkit:**
   ```bash
   pip install provide-testkit
   ```

2. **Reset Foundation state:**
   ```python
   import pytest
   from provide.testkit import reset_foundation_setup_for_testing

   @pytest.fixture(autouse=True)
   def reset_foundation():
       """Reset Foundation state before each test."""
       reset_foundation_setup_for_testing()
   ```

3. **Isolate test configuration:**
   ```python
   def test_with_custom_config():
       """Test with isolated configuration."""
       reset_foundation_setup_for_testing()

       # Set test-specific config
       from provide.foundation import get_hub
       hub = get_hub()
       hub.initialize_foundation(test_config)

       # Run test
       assert something()
   ```

---

### Log Output Polluting Test Output

**Symptoms:**
- Test output cluttered with log messages
- Hard to read test results

**Solutions:**

1. **Capture logs in tests:**
   ```python
   from provide.testkit import set_log_stream_for_testing
   from io import StringIO

   def test_with_captured_logs():
       log_stream = StringIO()
       set_log_stream_for_testing(log_stream)

       # Run code that logs
       my_function()

       # Check logs
       logs = log_stream.getvalue()
       assert "expected_message" in logs
   ```

2. **Suppress logs in tests:**
   ```bash
   # Run tests with minimal logging
   PROVIDE_LOG_LEVEL=ERROR pytest
   ```

---

## Integration Problems

### FastAPI/Flask Integration Issues

**Symptoms:**
- Request context not preserved in logs
- Logs from different requests mixed together

**Solution:**

Use correlation IDs:
```python
from fastapi import FastAPI, Request
from provide.foundation import logger
import uuid

app = FastAPI()

@app.middleware("http")
async def add_correlation_id(request: Request, call_next):
    """Add correlation ID to each request."""
    correlation_id = request.headers.get("X-Correlation-ID") or str(uuid.uuid4())

    # Add to all logs in this request
    with logger.bind(correlation_id=correlation_id):
        response = await call_next(request)
        response.headers["X-Correlation-ID"] = correlation_id
        return response
```

---

### Celery Integration Issues

**Symptoms:**
- Logs from different Celery tasks mixed together
- Can't track which logs belong to which task

**Solution:**

Bind task context:
```python
from celery import Task
from provide.foundation import logger

class FoundationTask(Task):
    """Celery task with Foundation logging."""

    def __call__(self, *args, **kwargs):
        with logger.bind(task_id=self.request.id, task_name=self.name):
            return super().__call__(*args, **kwargs)

@celery.task(base=FoundationTask)
def my_task(user_id):
    logger.info("Processing task", user_id=user_id)
    # All logs will include task_id and task_name
```

---

## Getting More Help

### Enable Debug Logging

Get maximum visibility:
```bash
export PROVIDE_LOG_LEVEL=DEBUG
export PROVIDE_LOG_FORMAT=console
python your_script.py
```

### Check Versions

Verify you're using compatible versions:
```bash
pip list | grep provide
python --version
```

### Reproduce in Minimal Example

Create a minimal reproduction:
```python
from provide.foundation import logger

logger.info("Testing basic logging")
# Add minimal code to reproduce your issue
```

### Report Issues

If you've found a bug:

1. **Check existing issues:** [GitHub Issues](https://github.com/provide-io/provide-foundation/issues)
2. **Create a new issue** with:
   - Python version
   - Foundation version
   - Minimal reproduction code
   - Expected vs actual behavior
   - Full error traceback

---

## Common Error Messages

### RuntimeError: Foundation not initialized

**Message:**
```
RuntimeError: Foundation not initialized. Call get_hub().initialize_foundation() first.
```

**Solution:**
```python
from provide.foundation import get_hub

# Initialize before using Foundation features
hub = get_hub()
hub.initialize_foundation()

# Now you can use Foundation
from provide.foundation import logger
logger.info("Ready to go")
```

---

### AttributeError: module 'provide.foundation' has no attribute 'X'

**Message:**
```
AttributeError: module 'provide.foundation' has no attribute 'CircuitBreaker'
```

**Solution:**

Import from the correct submodule:
```python
# âœ… Correct
from provide.foundation.resilience import CircuitBreaker

# âŒ Wrong
from provide.foundation import CircuitBreaker
```

---

### CircuitBreakerOpen: Circuit breaker is open

**Message:**
```
CircuitBreakerOpen: Circuit breaker 'api_service' is open
```

**Solution:**

This is expected behavior when a circuit breaker trips. Options:

1. **Implement fallback:**
   ```python
   try:
       result = call_api()
   except CircuitBreakerOpen:
       result = get_cached_result()
   ```

2. **Wait for circuit to close:**
   - Circuit automatically resets after timeout period
   - Check `circuit.state` to monitor recovery

3. **Adjust thresholds:**
   ```python
   # Make circuit less sensitive
   @circuit_breaker(
       failure_threshold=10,  # More failures before opening
       timeout=30,  # Shorter recovery time
   )
   def call_api():
       pass
   ```

---

## Still Having Issues?

1. **Review the documentation:** [docs.provide.io](https://foundry.provide.io/foundation/)
2. **Check examples:** Look at `examples/` in the repository
3. **Ask for help:** Open a [GitHub Discussion](https://github.com/provide-io/provide-foundation/discussions)
4. **Report bugs:** Create an [Issue](https://github.com/provide-io/provide-foundation/issues)

---

**Tip**: When troubleshooting, start with `PROVIDE_LOG_LEVEL=DEBUG` to see what's happening internally. Most issues become clear with debug logging enabled.
>>> EOF >>>

### FILE 44: index.md | checksum=423390e476cf... | modified=2025-10-24T17:42:43 | op=+ | size=3494 | tokens=855 | type=markdown ###
<<< BOF <<<
# Welcome to provide.foundation

**provide.foundation** is a comprehensive Python 3.11+ library for building robust, operationally excellent applications. It provides a cohesive, "batteries-included" toolkit that addresses common challenges in modern application development.

Built on industry-standard libraries like `structlog`, `click`, and `attrs`, `provide.foundation` offers a superior developer experience with beautiful console output, powerful error handling, and cross-platform system utilities.

## Why provide.foundation?

| For Developers | For Teams |
| :--- | :--- |
| âœ… **Zero Configuration**: Works beautifully out of the box. | ðŸ¤ **Consistent**: Standardized patterns across all services. |
| âœ… **Type Safe**: Full type hints and runtime validation. | ðŸ”­ **Observable**: Structured logs ready for analysis. |
| âœ… **Fast**: Optimized for production (>14,000 msg/sec). | ðŸ› ï¸ **Maintainable**: Clean, well-documented APIs. |
| âœ… **Testable**: Built-in testing utilities and patterns. | ðŸ§© **Extensible**: Plugin system for customization. |

## Learning Path

This documentation is structured to help you learn effectively, whether you're a beginner or an expert.

<div class="feature-grid">
  <div class="feature-card">
    <h3>ðŸŽ“ Tutorials</h3>
    <p>Step-by-step lessons to get you started. Perfect for new users.</p>
    <p><a href="getting-started/quick-start/">Start Learning â†’</a></p>
  </div>
  <div class="feature-card">
    <h3>ðŸ“– How-To Guides</h3>
    <p>Practical, goal-oriented recipes to solve specific problems.</p>
    <p><a href="how-to-guides/logging/basic-logging/">Solve a Problem â†’</a></p>
  </div>
  <div class="feature-card">
    <h3>ðŸ§  Concepts</h3>
    <p>Deep dives into the concepts and architecture behind the framework.</p>
    <p><a href="explanation/architecture/">Understand the "Why" â†’</a></p>
  </div>
  <div class="feature-card">
    <h3>ðŸ“– Reference</h3>
    <p>Complete API documentation for all modules, classes, and functions.</p>
    <p><a href="reference/">Browse API Docs â†’</a></p>
  </div>
</div>

## Quick Example

```python
from provide.foundation import logger, pout, get_hub
from provide.foundation.hub import register_command
from provide.foundation.resilience import retry
from provide.foundation.errors import NetworkError

# Initialize the framework (optional - logger auto-initializes on first use)
# For advanced configuration:
# get_hub().initialize_foundation()

# Structured logging with event enrichment
logger.info("application_startup", version="1.0.0", emoji="ðŸš€")

# User-facing console output
pout("âœ… Configuration loaded successfully.", color="green")

# Resilient functions
@retry(max_attempts=3, exceptions=(NetworkError,))
def fetch_data_from_api():
    logger.info("api_call_start", endpoint="/data", emoji="ðŸ“¡")
    # ... API call logic that might fail ...
    # if failed:
    #     raise NetworkError("API is unavailable")
    logger.info("api_call_complete", status=200, emoji="âœ…")

# Declarative CLI commands
@register_command("process")
def process_data(file: str, force: bool = False):
    """Process the given data file."""
    pout(f"Processing {file} with force={force}...")
    fetch_data_from_api()
```

## System Requirements

-   Python 3.11 or higher
-   Works on Linux, macOS, and Windows
-   Minimal core dependencies (`structlog`, `attrs`, `click`)

---

Ready to get started? Head to the **[Quick Start Tutorial](getting-started/quick-start.md)**.
>>> EOF >>>

### FILE 45: information/changelog.md | checksum=0f726ead7364... | modified=2025-10-21T08:08:03 | op=+ | size=4732 | tokens=1008 | type=markdown ###
<<< BOF <<<
# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Removed
- Removed workenv/wrknv dependency - project now uses standard Python virtual environment setup
- Removed all references to `.wrknv` directories - now uses `.provide-foundation` for tool installations

### Changed
- Tools module now uses `.provide-foundation` directory instead of `.wrknv` for tool cache and installations
- Registry now uses `provide.foundation.tools` entry point group instead of `wrknv.tools`
- Simplified development setup to use standard `uv venv` and `.venv` directory

## [0.1.0-beta.2] - 2025-01-14

### Removed (Breaking Changes)
- **BREAKING**: Removed deprecated `setup_foundation()` function - use `get_hub().initialize_foundation(config)` instead
- **BREAKING**: Removed deprecated `setup_telemetry()` function - use `get_hub().initialize_foundation(config)` instead
- **BREAKING**: Removed deprecated `setup_logging()` function - Foundation now auto-initializes on first use
- **BREAKING**: Removed deprecated `emoji` and `emoji_hierarchy` parameters from `get_logger()` - replaced by event sets

### Fixed
- Fixed RecursionError in Foundation `__getattr__` method that caused infinite loops when importing CLI modules
- Fixed `provide.foundation.hub` module access through lazy loading
- Resolved security vulnerabilities identified by bandit scan (tarfile extraction, shell injection, SQL injection)

### Improved
- **Code Quality**: Comprehensive "dogfooding" - Foundation now uses its own utilities consistently throughout codebase
- **Security**: All high/medium severity security findings addressed
- **API Surface**: Simplified API by removing all deprecated functions and parameters
- **Documentation**: Updated example files to use modern Hub-based initialization

### Changed
- All setup functions now route through Hub-based initialization system
- Examples updated to demonstrate Hub-based configuration instead of deprecated setup functions

## [0.1.0-beta.1] - 2025-01-13

### Added
- Initial beta release of provide.foundation
- Structured logging with emoji-enhanced visual parsing
- CLI framework with command registration and comprehensive subcommands
- Configuration management system with environment variable support
- Cryptographic utilities (hash, signatures, certificates)
- File operations with atomic write support
- Console I/O with color support and proper output handling
- Platform detection and system utilities
- Process execution with streaming support
- Error handling with retry logic and error boundaries
- Registry pattern for component management
- OpenTelemetry integration for distributed tracing
- OpenObserve integration for log aggregation
- Rate limiting with token bucket algorithm
- Comprehensive example suite

### Improved
- **Code Quality**: Reduced cyclomatic complexity violations from 41 to 39 (C901)
- **Error Handling**: Improved exception chaining patterns (B904)
- **Type Safety**: Enhanced type annotations and resolved type errors
- **Test Coverage**: Increased from 79.84% to 82.50% (exceeding 80% target)
- **Import Organization**: Fixed all 45 import sorting issues (I001)
- **Code Organization**: Fixed all 22 unsorted __all__ declarations

### Fixed
- Complex function refactoring in CLI commands (query, send, generate)
- Type annotation errors in configuration parsers
- Missing imports for NoReturn, Callable, and other typing constructs
- Exception chaining in error handling
- Click decorator placement in CLI commands

### Added Tests
- Comprehensive config error handling tests (33 new tests, 100% coverage)
- CLI logs generate command tests (30 new tests, coverage improved from 18.48% to 61.96%)
- Config defaults tests (39 new tests, 100% coverage)

### Technical Improvements
- Moved from lambda factories to proper named functions in config defaults
- Enhanced type safety with proper cast() usage for string literals
- Improved function complexity through strategic helper function extraction
- Better separation of concerns in CLI command implementations

### Dependencies
- Python 3.11+ required
- Built on structlog, attrs, and modern Python typing
- Optional dependencies for CLI (click), crypto (cryptography), transport (httpx), and OpenTelemetry

### Documentation
- Complete API documentation
- Getting started guide
- Practical examples for all major features
- Environment variable configuration reference

---

**Note**: This is a pre-release version. APIs may change before 1.0.0.

---

[Return to Documentation Home](../index.md)
>>> EOF >>>

### FILE 46: information/features.md | checksum=0e061c912edd... | modified=2025-10-24T18:06:19 | op=+ | size=5530 | tokens=1098 | type=markdown ###
<<< BOF <<<
# Features

**provide.foundation** offers a comprehensive toolkit for building robust applications with excellent quality standards.

## Quality Standards

provide.foundation maintains high standards for code quality, testing, and reliability:

- **High Test Coverage (>80%)** with 1000+ comprehensive tests
- **Extensive 100% coverage** of core components and critical modules
- **Comprehensive Security Testing** with path traversal, symlink validation, and input sanitization
- **Performance Benchmarked** logging, transport, and archive operations
- **Type-Safe Codebase** with comprehensive type annotations
- **Automated Quality Checks** with ruff, mypy, and bandit

## Core Components

### Structured Logging
Beautiful, performant logging built on `structlog` with event-enriched structured logging and zero configuration required.

**Key Features:**
- Zero configuration - works out of the box
- Structured event logging with key-value pairs
- Performance optimized (>14,000 msg/sec)
- Emoji-enhanced visual parsing for quick scanning
- Domain-Action-Status pattern support
- Automatic exception logging with tracebacks

**Learn More:** [Basic Logging Guide](../how-to-guides/logging/basic-logging.md)

### Metrics
Lightweight and extensible metrics collection with optional OpenTelemetry integration.

**Key Features:**
- Simple counter, gauge, and histogram metrics
- OpenTelemetry integration for distributed systems
- Low overhead design
- Automatic service identification

### CLI Framework
Build command-line interfaces with automatic help generation and component registration (requires `[cli]` extra).

**Key Features:**
- Declarative command registration with `@register_command`
- Automatic help text generation
- Built on Click for robust argument parsing
- Component-based architecture with the Hub system
- Rich console output with colors and formatting

**Learn More:** [CLI Commands Guide](../how-to-guides/cli/commands.md)

### Configuration Management
Flexible configuration system supporting environment variables, files, and runtime updates.

**Key Features:**
- Environment variable support with type coercion
- File-based configuration (YAML, JSON, TOML)
- Secret file support with `file://` prefix
- Type-safe configuration classes with attrs
- Runtime configuration updates

### Error Handling
Comprehensive error handling with retry logic and error boundaries.

**Key Features:**
- Rich error types for different failure modes
- Automatic error logging with context
- Integration with structured logging
- Clear error messages for debugging

**Learn More:** [Exception Logging Guide](../how-to-guides/logging/exception-logging.md)

### Resilience Patterns
Suite of decorators for building reliable applications (retry, circuit breaker, bulkhead).

**Key Features:**
- `@retry` decorator with exponential backoff
- Configurable retry strategies
- Exception filtering
- Automatic logging of retry attempts

**Learn More:** [Retry Patterns Guide](../how-to-guides/resilience/retry.md)

### Concurrency Utilities
High-level utilities for managing asynchronous tasks and thread-safe operations.

**Key Features:**
- Thread-safe registry and component management
- Async/await compatible logging
- Safe subprocess execution with streaming
- Async helper utilities

### Cryptographic Utilities
Comprehensive cryptographic operations with modern algorithms and secure defaults (requires `[crypto]` extra).

**Key Features:**
- Ed25519 and RSA key generation and signing
- X.509 certificate generation and management
- Secure hash functions (SHA-256, BLAKE2b)
- Checksum validation
- Prefixed key encoding for easy identification

### File Operations
Atomic file operations with format support and safety guarantees.

**Key Features:**
- Atomic writes with temporary files and rename
- File watching and change detection
- Safe path validation (prevents path traversal)
- Symlink attack protection
- Format detection and serialization

### Archive Operations
Create and extract archives with support for TAR, ZIP, GZIP, and BZIP2 formats.

**Key Features:**
- Deterministic archive creation (reproducible builds)
- Secure extraction with path validation
- Compression support
- Metadata preservation

### Serialization
Safe and consistent JSON serialization and deserialization.

**Key Features:**
- Type-safe JSON encoding/decoding
- Support for common Python types (datetime, Path, etc.)
- Pretty printing support
- Consistent formatting

### Console I/O
Enhanced console input/output with color support, JSON mode, and interactive prompts.

**Key Features:**
- Color-coded output with `pout()` and `perr()`
- Interactive prompts with validation
- JSON mode for machine-readable output
- Progress indicators

### Hub and Registry
Central system for managing application components, commands, and resources.

**Key Features:**
- Multi-dimensional component registry
- Automatic command discovery
- Dependency injection patterns
- Component lifecycle management
- Context propagation

## Additional Utilities

### Formatting Utilities
Collection of helpers for formatting text, numbers, and data structures.

### Platform Utilities
Cross-platform detection and system information gathering.

### Process Execution
Safe subprocess execution with streaming and async support.

---

**Next Steps:**
- See [Use Cases](use-cases.md) for practical applications
- Check [Architecture](../explanation/architecture.md) for design philosophy
- Start with the [Quick Start Tutorial](../getting-started/quick-start.md)
>>> EOF >>>

### FILE 47: information/use-cases.md | checksum=f01e1e88d985... | modified=2025-10-24T19:55:25 | op=+ | size=7597 | tokens=1573 | type=markdown ###
<<< BOF <<<
# Use Cases

provide.foundation is designed as a foundation layer for building production-ready Python applications. This page explains when to use it and what kinds of applications it's best suited for.

## When to Use provide.foundation

### Excellent Fit

These application types are ideally suited for provide.foundation:

#### CLI Applications and Developer Tools
Perfect for command-line tools that need structured logging, configuration management, and beautiful console output.

**Why it fits:**
- Declarative CLI framework with `@register_command`
- Rich console output with colors and formatting
- Zero-configuration logging that just works
- Component registry for managing resources

**Examples:**
- Development tools (code generators, project scaffolders)
- DevOps utilities (deployment scripts, infrastructure tools)
- Data processing command-line tools
- Build and automation tools

**See:** [CLI Application Tutorial](../getting-started/first-app.md)

#### Microservices with Structured Logging
Services that need production-ready logging and observability.

**Why it fits:**
- Structured logging ready for log aggregation
- OpenTelemetry integration for distributed tracing
- Metrics collection and reporting
- Environment-based configuration

**Examples:**
- REST API backend services (with FastAPI/Flask)
- gRPC services
- Message queue consumers
- Background job processors

#### Data Processing Pipelines
Batch and streaming data processing applications.

**Why it fits:**
- Resilience patterns (retry, circuit breaker)
- Progress tracking and logging
- File operations with safety guarantees
- Archive and serialization utilities

**Examples:**
- ETL pipelines
- Data transformation jobs
- Log processing systems
- Report generators

#### Background Task Processors
Long-running workers that process async tasks.

**Why it fits:**
- Structured logging for debugging
- Error handling with automatic retries
- Metrics and health monitoring
- Clean shutdown handling

**Examples:**
- Celery workers
- RQ job processors
- Scheduled task runners
- Email/notification senders

### Good Fit (With Awareness)

These use cases work well but require understanding of the architecture:

#### Web APIs
Use for logging, configuration, and resilience - NOT as a web framework.

**Integration Pattern:**
```python
# Use FastAPI/Flask for HTTP, Foundation for logging
from fastapi import FastAPI
from provide.foundation import logger, get_hub
from provide.foundation.logger.config import TelemetryConfig, LoggingConfig

app = FastAPI()

# Initialize Foundation
config = TelemetryConfig(
    service_name="my-api",
    logging=LoggingConfig(default_level="INFO")
)
get_hub().initialize_foundation(config)

@app.get("/users")
async def get_users():
    logger.info("users_fetch_started")
    # ... your logic ...
    logger.info("users_fetch_completed", count=len(users))
    return users
```

#### Task Queue Systems
Great for worker logging, less ideal if using async-heavy task queues.

**Consideration:** Registry uses threading locks, not async locks. For ultra-high-throughput async workers (>10k tasks/sec), consider this trade-off.

#### Libraries Needing Structured Logging
Libraries can use Foundation's logging, but should allow users to configure it.

**Pattern:**
```python
# In your library
from provide.foundation import logger

def process_data(data):
    logger.debug("library_processing", data_size=len(data))
    # ... processing ...
```

### Consider Alternatives

These scenarios might be better served by other tools:

#### Ultra-Low Latency Systems
If you need <100Î¼s latencies, Foundation's structured logging overhead may be too high.

**Alternative:** Use Python's standard `logging` module with minimal formatting, or consider lower-level languages.

#### Full-Stack Framework Needs
If you need batteries-included web framework with ORM, auth, and templates.

**Alternative:** Use Django or Rails instead. Foundation is explicitly NOT a full-stack framework.

#### Tool Stack Incompatibility
If your project is standardized on Pydantic-only or loguru-only stacks.

**Trade-off:** Foundation uses attrs and structlog for consistency. Mixing tool stacks adds complexity.

## Real-World Application Examples

### Example 1: Developer CLI Tool

A CLI tool for managing cloud infrastructure:

```python
from provide.foundation import logger, pout, get_hub
from provide.foundation.hub import register_command
from provide.foundation.resilience import retry

@register_command("deploy")
def deploy(environment: str, version: str):
    """Deploy application to cloud environment."""
    logger.info("deployment_started", env=environment, version=version)
    pout(f"ðŸš€ Deploying version {version} to {environment}...")

    deploy_to_cloud(environment, version)

    logger.info("deployment_completed", env=environment)
    pout("âœ… Deployment successful!", color="green")

if __name__ == "__main__":
    cli = get_hub().create_cli(name="cloud-deploy")
    cli()
```

### Example 2: Microservice with FastAPI

An API service with structured logging:

```python
from fastapi import FastAPI
from provide.foundation import logger, get_hub
from provide.foundation.logger.config import TelemetryConfig, LoggingConfig

# Initialize Foundation
get_hub().initialize_foundation(
    TelemetryConfig(
        service_name="user-api",
        logging=LoggingConfig(
            default_level="INFO",
            console_formatter="json",  # JSON for production
        ),
    )
)

app = FastAPI()

@app.get("/health")
async def health():
    logger.debug("health_check")
    return {"status": "healthy"}
```

### Example 3: Data Pipeline

A data processing pipeline with resilience:

```python
from provide.foundation import logger
from provide.foundation.resilience import retry
from provide.foundation.errors import NetworkError

@retry(max_attempts=3, exceptions=(NetworkError,))
def fetch_data_from_api(endpoint: str):
    """Fetch data with automatic retry on network errors."""
    logger.info("api_fetch_started", endpoint=endpoint)

    try:
        data = call_api(endpoint)
        logger.info("api_fetch_completed", records=len(data))
        return data
    except Exception as e:
        logger.error("api_fetch_failed", endpoint=endpoint, error=str(e))
        raise NetworkError(f"Failed to fetch from {endpoint}")

def process_pipeline():
    data = fetch_data_from_api("/v1/users")
    # ... process data ...
    logger.info("pipeline_completed", records_processed=len(data))
```

## Architecture Considerations

### Threading vs Async
Foundation's registry uses threading locks (`threading.RLock`), not async locks.

**Impact:**
- **Negligible** for CLI apps, initialization-time registration
- **Low** for read-heavy workloads (command lookup)
- **Consider** for high-throughput async services with runtime registration in hot paths

### Global State Pattern
Foundation uses singleton patterns (`get_hub()`, `logger`) for ergonomic APIs.

**Mitigation:** Use `provide-testkit`'s `reset_foundation_setup_for_testing()` for clean test isolation.

### Intentional Scope
Foundation provides logging, CLI, configuration. It does NOT provide:
- Web frameworks (use FastAPI/Flask/Django)
- Databases or ORMs (use SQLAlchemy/Django ORM)
- Authentication systems (use libraries specific to your framework)
- Template engines (use Jinja2/etc.)

---

**Next Steps:**
- Review [Features](features.md) for complete capabilities
- Check [Architecture](../explanation/architecture.md) for design decisions
- Start building with [Quick Start](../getting-started/quick-start.md)
>>> EOF >>>

### FILE 48: javascripts/mermaid-init.js | checksum=b77bd02bac4a... | modified=2025-10-21T08:08:03 | op=+ | size=375 | tokens=100 | type=javascript ###
<<< BOF <<<
// Initialize Mermaid for diagram rendering
document$.subscribe(function() {
  mermaid.initialize({
    startOnLoad: true,
    theme: 'default',
    themeVariables: {
      primaryColor: '#6366f1',
      primaryTextColor: '#fff',
      primaryBorderColor: '#4f46e5',
      lineColor: '#6366f1',
      secondaryColor: '#818cf8',
      tertiaryColor: '#c7d2fe'
    }
  });
});
>>> EOF >>>

### FILE 49: reference/gen_ref_pages.py | checksum=d8928cc3b6c6... | modified=2025-10-24T17:10:26 | op=+ | size=1426 | tokens=351 | type=x-python ###
<<< BOF <<<
"""Generate the API reference pages for mkdocs."""

from pathlib import Path

import mkdocs_gen_files

nav = mkdocs_gen_files.Nav()
# Assuming your source code is in a 'src' directory at the project root
src_root = Path("src")

for path in sorted(src_root.rglob("*.py")):
    if "__pycache__" in str(path):
        continue

    module_path = path.relative_to(src_root).with_suffix("")
    doc_path = Path("reference") / module_path.with_suffix(".md")
    full_doc_path = Path("reference") / module_path.with_suffix(".md")

    parts = tuple(module_path.parts)
    if any(part.startswith("_") and part != "__init__" for part in parts):
        continue

    if parts[-1] == "__init__":
        parts = parts[:-1]
        doc_path = doc_path.with_name("index.md")
        full_doc_path = full_doc_path.with_name("index.md")

    if not parts:
        continue

    # Strip "reference/" prefix from nav paths since SUMMARY.md is already in reference/
    nav_path = str(doc_path)
    if nav_path.startswith("reference/"):
        nav_path = nav_path[10:]  # Remove "reference/" prefix
    nav[parts] = nav_path

    with mkdocs_gen_files.open(full_doc_path, "w") as fd:
        identifier = ".".join(parts)
        print(f"::: {identifier}", file=fd)

    mkdocs_gen_files.set_edit_path(full_doc_path, path)

with mkdocs_gen_files.open("reference/SUMMARY.md", "w") as nav_file:
    nav_file.writelines(nav.build_literate_nav())
>>> EOF >>>

### FILE 50: reference/index.md | checksum=a70784a27300... | modified=2025-10-24T19:57:35 | op=+ | size=5876 | tokens=1257 | type=markdown ###
<<< BOF <<<
# API Reference

Complete API documentation for `provide.foundation` â€” auto-generated from source code docstrings and type hints.

## ðŸš€ Quick Access

Most commonly used functions and classes:

### Essential Functions

```python
from provide.foundation import logger, pout, perr, get_hub
from provide.foundation.hub import register_command
from provide.foundation.resilience import retry
```

| Import | Purpose | Documentation |
|--------|---------|---------------|
| `logger` | Global logger instance for structured logging | [logger docs](provide/foundation/logger/index.md) |
| `pout()` | User-facing output to stdout (with colors) | [console docs](provide/foundation/console/index.md) |
| `perr()` | User-facing errors to stderr (with colors) | [console docs](provide/foundation/console/index.md) |
| `get_hub()` | Access the central component registry | [hub docs](provide/foundation/hub/index.md) |
| `@register_command` | Register CLI commands | [CLI docs](provide/foundation/cli/index.md) |
| `@retry` | Retry decorator with exponential backoff | [resilience docs](provide/foundation/resilience/index.md) |

### Configuration Classes

```python
from provide.foundation.config import BaseConfig, env_field
from provide.foundation.logger.config import TelemetryConfig, LoggingConfig
```

| Class | Purpose | Documentation |
|-------|---------|---------------|
| `BaseConfig` | Base class for configuration objects | [config docs](provide/foundation/config/index.md) |
| `TelemetryConfig` | Configure logging and telemetry | [logger config docs](provide/foundation/logger/config/index.md) |
| `LoggingConfig` | Detailed logging configuration | [logger config docs](provide/foundation/logger/config/index.md) |

### Utilities

```python
from provide.foundation.utils.environment import get_bool, get_int, get_str
from provide.foundation.file import atomic_write
from provide.foundation.serialization import provide_dumps, provide_loads
from provide.foundation.eventsets.display import show_event_matrix
from provide.foundation import shutdown_foundation
```

| Function | Purpose | Documentation |
|----------|---------|---------------|
| `get_bool()`, `get_int()`, `get_str()` | Environment variable helpers | [environment docs](provide/foundation/utils/environment/index.md) |
| `atomic_write()` | Safe atomic file writes | [file docs](provide/foundation/file/index.md) |
| `provide_dumps()`, `provide_loads()` | JSON serialization | [serialization docs](provide/foundation/serialization/index.md) |
| `show_event_matrix()` | Display event set emoji matrix | [eventsets docs](provide/foundation/eventsets/index.md) |
| `shutdown_foundation()` | Graceful shutdown and cleanup | [setup docs](provide/foundation/setup/index.md) |

### Advanced Features

```python
from provide.foundation.hub import injectable
from provide.foundation.hub.container import Container, create_container
from provide.foundation.eventsets.types import EventSet, EventMapping
```

| Feature | Purpose | Documentation |
|---------|---------|---------------|
| `@injectable` | Mark classes for dependency injection | [hub docs](provide/foundation/hub/index.md) |
| `Container` | Dependency injection container | [container docs](provide/foundation/hub/container/index.md) |
| `EventSet` | Define custom event sets with emojis | [eventsets docs](provide/foundation/eventsets/index.md) |
| `EventMapping` | Map events to emoji representations | [eventsets docs](provide/foundation/eventsets/index.md) |

---

## Quick Navigation

### Core Components

- **[logger](provide/foundation/logger/index.md)** - Structured logging system with emoji-enhanced output
- **[hub](provide/foundation/hub/index.md)** - Central component registry and dependency injection
- **[config](provide/foundation/config/index.md)** - Configuration management with environment variable support
- **[errors](provide/foundation/errors/index.md)** - Error handling and custom exception types

### CLI & Console

- **[cli](provide/foundation/cli/index.md)** - Command-line interface framework
- **[console](provide/foundation/console/index.md)** - Console I/O with colors and formatting

### Resilience & Observability

- **[resilience](provide/foundation/resilience/index.md)** - Retry patterns, circuit breakers, and failure handling
- **[metrics](provide/foundation/metrics/index.md)** - Metrics collection and OpenTelemetry integration
- **[tracer](provide/foundation/tracer/index.md)** - Distributed tracing support

### Data & Files

- **[file](provide/foundation/file/index.md)** - Atomic file operations and change detection
- **[archive](provide/foundation/archive/index.md)** - Archive creation and extraction (tar, zip, gzip)
- **[serialization](provide/foundation/serialization/index.md)** - JSON serialization with type safety

### Security & Crypto

- **[crypto](provide/foundation/crypto/index.md)** - Cryptographic operations (keys, signatures, certificates)
- **[security](provide/foundation/security/index.md)** - Security utilities and path validation

### Utilities

- **[utils](provide/foundation/utils/index.md)** - General utilities (async, caching, formatting, timing)
- **[process](provide/foundation/process/index.md)** - Safe subprocess execution
- **[platform](provide/foundation/platform/index.md)** - Platform detection and system information
- **[transport](provide/foundation/transport/index.md)** - HTTP client utilities
- **[parsers](provide/foundation/parsers/index.md)** - Data parsing utilities

## Browse All Modules

For a complete hierarchical view of all modules, classes, and functions:

**[ðŸ“‘ Full Module Index](SUMMARY/)** - Complete navigation tree

## Module Count

This reference documents **36 foundation modules** with **352 API pages**.

---

**Tip:** Use your browser's search (Ctrl+F / Cmd+F) within individual module pages to quickly find specific functions or classes.
>>> EOF >>>

### FILE 51: specs/index.md | checksum=0cc8ee4e7da2... | modified=2025-10-13T14:38:53 | op=+ | size=455 | tokens=81 | type=markdown ###
<<< BOF <<<
# Developer Specifications

!!! warning "Forward-Looking Information"
    The documents in this section are **design specifications** for planned or future features. They describe a potential future state of the framework and **do not** reflect the currently implemented API.

    They are provided for transparency into the project's roadmap and to gather community feedback on future development. Do not rely on these APIs for current development work.
>>> EOF >>>

### FILE 52: stylesheets/extra.css | checksum=c336cf1366c8... | modified=2025-10-21T08:08:03 | op=+ | size=8942 | tokens=2461 | type=css ###
<<< BOF <<<
/* Extra styles for provide.foundation documentation */
/* Fonts and styling to match provide.io branding */

/* Import Google Fonts matching provide.io */
@import url('https://fonts.googleapis.com/css2?family=Chakra+Petch:wght@400;500;600;700&family=IBM+Plex+Serif:ital,wght@0,400;0,600;0,700;1,400&display=swap');

/* Override Material theme fonts with provide.io fonts */
:root {
  --md-text-font: system-ui, -apple-system, "Segoe UI", Roboto, "Helvetica Neue", "Noto Sans", "Liberation Sans", Arial, sans-serif;
  --md-code-font: "SFMono-Regular", "Menlo", "Monaco", "Consolas", "Liberation Mono", "Courier New", monospace;
}

/* Body text uses system font stack like provide.io */
body, .md-typeset, .md-typeset p, .md-typeset li {
  font-family: system-ui, -apple-system, "Segoe UI", Roboto, "Helvetica Neue", "Noto Sans", "Liberation Sans", Arial, sans-serif !important;
}

/* ========================================================================== */
/* PROFESSIONAL HEADER SPACING & TYPOGRAPHY */
/* ========================================================================== */

/* Main headers (h1-h3) use Chakra Petch like provide.io */
h1, h2, h3,
.md-typeset h1, .md-typeset h2, .md-typeset h3 {
  font-family: "Chakra Petch", sans-serif !important;
  font-weight: bold !important;
}

/* H1 - Large headers with optimal spacing */
.md-typeset h1 {
  font-size: 2.25em !important;
  margin-top: 2.5rem !important;
  margin-bottom: 1.25rem !important;
  line-height: 1.2 !important;
  letter-spacing: -0.02em !important;
}

/* H2 - Section headers */
.md-typeset h2 {
  font-size: 1.75em !important;
  margin-top: 2rem !important;
  margin-bottom: 1rem !important;
  line-height: 1.3 !important;
  letter-spacing: -0.01em !important;
}

/* H3 - Subsection headers */
.md-typeset h3 {
  font-size: 1.375em !important;
  margin-top: 1.75rem !important;
  margin-bottom: 0.875rem !important;
  line-height: 1.4 !important;
}

/* First header after content - reduce top margin */
.md-typeset > h1:first-child,
.md-typeset > h2:first-child,
.md-typeset > h3:first-child {
  margin-top: 0 !important;
}

/* Smaller headers (h4-h6) use IBM Plex Serif like provide.io */
h4, h5, h6,
.md-typeset h4, .md-typeset h5, .md-typeset h6 {
  font-family: "IBM Plex Serif", serif !important;
  font-weight: 600 !important;
  margin-top: 1.5rem !important;
  margin-bottom: 0.75rem !important;
  line-height: 1.5 !important;
}

/* ========================================================================== */
/* SITE HEADER & NAVIGATION */
/* ========================================================================== */

/* Site title uses Chakra Petch with better sizing */
.md-header__title {
  font-family: "Chakra Petch", sans-serif !important;
  font-weight: 600 !important;
  font-size: 1.125rem !important;
  letter-spacing: -0.01em !important;
}

.md-nav__title {
  font-family: "Chakra Petch", sans-serif !important;
  font-weight: 600 !important;
}

/* Navigation tabs with smooth transitions */
.md-tabs__link {
  transition: background-color 200ms ease, color 200ms ease !important;
}

.md-tabs__link:hover {
  opacity: 0.9 !important;
}

/* Sidebar navigation */
.md-nav__link {
  transition: color 150ms ease !important;
}

.md-nav__link:hover {
  color: var(--md-accent-fg-color) !important;
}

/* Code blocks use monospace stack */
code, pre, kbd, samp,
.md-typeset code, .md-typeset pre {
  font-family: "SFMono-Regular", "Menlo", "Monaco", "Consolas", "Liberation Mono", "Courier New", monospace !important;
}

/* ========================================================================== */
/* INTERACTIVE HOVER STATES & TRANSITIONS */
/* ========================================================================== */

/* Smooth transitions for all interactive elements */
.md-typeset a,
.md-typeset h1 a,
.md-typeset h2 a,
.md-typeset h3 a {
  transition: color 200ms ease, opacity 200ms ease, transform 200ms ease !important;
}

/* Header link hover effects */
.md-typeset h1 a:hover,
.md-typeset h2 a:hover,
.md-typeset h3 a:hover {
  opacity: 0.8 !important;
  text-decoration: underline !important;
  text-decoration-color: var(--md-primary-fg-color) !important;
  text-underline-offset: 0.25em !important;
}

/* Permalink hover (the # icon) - fade in on hover */
.md-typeset .headerlink {
  opacity: 0 !important;
  transition: opacity 200ms ease !important;
  margin-left: 0.5rem !important;
  color: var(--md-default-fg-color--lighter) !important;
}

.md-typeset h1:hover .headerlink,
.md-typeset h2:hover .headerlink,
.md-typeset h3:hover .headerlink,
.md-typeset h4:hover .headerlink,
.md-typeset h5:hover .headerlink,
.md-typeset h6:hover .headerlink {
  opacity: 0.5 !important;
}

.md-typeset .headerlink:hover {
  opacity: 1 !important;
  color: var(--md-primary-fg-color) !important;
}

/* Regular link hovers with accent color */
.md-typeset a:not(.headerlink):hover {
  color: var(--md-accent-fg-color) !important;
}

/* ========================================================================== */
/* CONTENT SPACING & RHYTHM */
/* ========================================================================== */

/* Paragraph spacing for better readability */
.md-typeset p {
  margin-bottom: 1rem !important;
  line-height: 1.7 !important;
}

/* List spacing */
.md-typeset ul, .md-typeset ol {
  margin-top: 0.5rem !important;
  margin-bottom: 1rem !important;
}

.md-typeset li {
  margin-bottom: 0.375rem !important;
  line-height: 1.6 !important;
}

/* Code block spacing */
.md-typeset pre {
  margin-top: 1rem !important;
  margin-bottom: 1.5rem !important;
}

/* Table spacing */
.md-typeset table {
  margin-top: 1rem !important;
  margin-bottom: 1.5rem !important;
}

/* Content block spacing */
.md-content__inner {
  padding-top: 2rem !important;
}

/* Better hr spacing */
.md-typeset hr {
  margin-top: 2rem !important;
  margin-bottom: 2rem !important;
  border: none !important;
  border-top: 1px solid var(--md-default-fg-color--lightest) !important;
}

/* Feature grids for landing pages */
.feature-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
    gap: 1.5rem;
    margin: 2rem 0;
}

.feature-card {
    padding: 1.5rem;
    border: 1px solid var(--md-default-fg-color--lightest);
    border-radius: 0.5rem;
    background: var(--md-code-bg-color);
    transition: transform 200ms ease, box-shadow 200ms ease, border-color 200ms ease !important;
}

.feature-card:hover {
    transform: translateY(-4px) !important;
    box-shadow: 0 8px 16px rgba(0, 0, 0, 0.15) !important;
    border-color: var(--md-primary-fg-color) !important;
}

.feature-card h3 {
    margin-top: 0;
    color: var(--md-primary-fg-color);
}

.feature-card a {
    display: inline-block;
    margin-top: 1rem;
    font-weight: 600;
    transition: background-color 200ms ease, transform 150ms ease !important;
}

.feature-card a:hover {
    transform: scale(1.02) !important;
}

/* Getting started grid */
.getting-started-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
    gap: 1.5rem;
    margin: 2rem 0;
}

.getting-started-card {
    padding: 1.5rem;
    border: 1px solid var(--md-default-fg-color--lightest);
    border-radius: 0.5rem;
    background: var(--md-code-bg-color);
    text-align: center;
    transition: transform 200ms ease, box-shadow 200ms ease, border-color 200ms ease !important;
}

.getting-started-card:hover {
    transform: translateY(-4px) !important;
    box-shadow: 0 8px 16px rgba(0, 0, 0, 0.15) !important;
    border-color: var(--md-primary-fg-color) !important;
}

.getting-started-card h3 {
    margin-top: 0;
    font-size: 1.2rem;
}

.getting-started-card p {
    margin: 0.5rem 0 1rem;
    color: var(--md-default-fg-color--light);
}

.getting-started-card a {
    display: inline-block;
    padding: 0.5rem 1rem;
    background: var(--md-primary-fg-color);
    color: white;
    border-radius: 0.25rem;
    text-decoration: none;
    font-weight: 600;
    transition: background-color 200ms ease, transform 150ms ease !important;
}

.getting-started-card a:hover {
    opacity: 0.9;
    transform: scale(1.02) !important;
}

/* Code blocks improvements */
.md-typeset code {
    border-radius: 0.25rem;
}

/* Tables */
.md-typeset table:not([class]) {
    border: 1px solid var(--md-default-fg-color--lightest);
    border-radius: 0.25rem;
}

.md-typeset table:not([class]) th {
    background: var(--md-code-bg-color);
    font-weight: 700;
}

/* Admonitions */
.md-typeset .admonition,
.md-typeset details {
    border-radius: 0.5rem;
}

/* Better spacing for lists */
.md-typeset ul,
.md-typeset ol {
    margin-bottom: 1rem;
}

/* Improve readability */
.md-typeset p {
    line-height: 1.7;
}

/* Bold for better hierarchy */
.md-typeset strong {
    font-weight: 700;
    color: var(--md-default-fg-color);
}

/* Emoji alignment */
.md-typeset .twemoji {
    vertical-align: middle;
}
>>> EOF >>>


### BUNDLE SUMMARY ###
- Included Files: 52
- Total Size (Included): 547876 bytes
- Duplicate Files Skipped: 0
- Items Excluded by Config/Defaults: 1 (Files: 0, Dirs: 1)
- Empty Files Found: 0
- System Errors Encountered: 0
- Encoding Errors (Fallback Attempted): 0
- Estimated Bundle Token Range: 129908 - 133438
- Processing Time: 0.25 seconds
### END BUNDLE SUMMARY ###

--- END OF BFILE bf-20251029-112231.txt ---
